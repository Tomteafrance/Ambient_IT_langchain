{"cells":[{"cell_type":"markdown","metadata":{"id":"RPZmxoJ8BS2l"},"source":["# Cours LangChain TP1"]},{"cell_type":"markdown","metadata":{"id":"IYHMYRjsBS2p"},"source":["Dans ce TP vous allez d'abord apprendre à créez des Prompt Templates et comprenez comment fournir des instructions au LLM."]},{"cell_type":"markdown","metadata":{"id":"hqiuuQpQBS2p"},"source":["preinstall et export de clé"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"xMmI0ONABS2q","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1710151014365,"user_tz":-60,"elapsed":21521,"user":{"displayName":"Tom TEA","userId":"00599091751588244729"}},"outputId":"fa0437c7-6599-4577-a666-756d8c33e133"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting langchain-openai\n","  Downloading langchain_openai-0.0.8-py3-none-any.whl (32 kB)\n","Collecting langchain\n","  Downloading langchain-0.1.11-py3-none-any.whl (807 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m807.5/807.5 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting langchain-core<0.2.0,>=0.1.27 (from langchain-openai)\n","  Downloading langchain_core-0.1.30-py3-none-any.whl (256 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m256.9/256.9 kB\u001b[0m \u001b[31m22.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting openai<2.0.0,>=1.10.0 (from langchain-openai)\n","  Downloading openai-1.13.3-py3-none-any.whl (227 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.4/227.4 kB\u001b[0m \u001b[31m22.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting tiktoken<1,>=0.5.2 (from langchain-openai)\n","  Downloading tiktoken-0.6.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m35.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\n","Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.28)\n","Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.9.3)\n","Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n","Collecting dataclasses-json<0.7,>=0.5.7 (from langchain)\n","  Downloading dataclasses_json-0.6.4-py3-none-any.whl (28 kB)\n","Collecting jsonpatch<2.0,>=1.33 (from langchain)\n","  Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n","Collecting langchain-community<0.1,>=0.0.25 (from langchain)\n","  Downloading langchain_community-0.0.27-py3-none-any.whl (1.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m55.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting langchain-text-splitters<0.1,>=0.0.1 (from langchain)\n","  Downloading langchain_text_splitters-0.0.1-py3-none-any.whl (21 kB)\n","Collecting langsmith<0.2.0,>=0.1.17 (from langchain)\n","  Downloading langsmith-0.1.23-py3-none-any.whl (66 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.6/66.6 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.25.2)\n","Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.6.3)\n","Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.31.0)\n","Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.2.3)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n","Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain)\n","  Downloading marshmallow-3.21.1-py3-none-any.whl (49 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain)\n","  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n","Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain)\n","  Downloading jsonpointer-2.4-py2.py3-none-any.whl (7.8 kB)\n","Requirement already satisfied: anyio<5,>=3 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2.0,>=0.1.27->langchain-openai) (3.7.1)\n","Requirement already satisfied: packaging<24.0,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2.0,>=0.1.27->langchain-openai) (23.2)\n","Collecting orjson<4.0.0,>=3.9.14 (from langsmith<0.2.0,>=0.1.17->langchain)\n","  Downloading orjson-3.9.15-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (138 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.5/138.5 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai<2.0.0,>=1.10.0->langchain-openai) (1.7.0)\n","Collecting httpx<1,>=0.23.0 (from openai<2.0.0,>=1.10.0->langchain-openai)\n","  Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.10.0->langchain-openai) (1.3.1)\n","Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.10.0->langchain-openai) (4.66.2)\n","Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.10.0->langchain-openai) (4.10.0)\n","Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (0.6.0)\n","Requirement already satisfied: pydantic-core==2.16.3 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (2.16.3)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2024.2.2)\n","Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\n","Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken<1,>=0.5.2->langchain-openai) (2023.12.25)\n","Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3->langchain-core<0.2.0,>=0.1.27->langchain-openai) (1.2.0)\n","Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai<2.0.0,>=1.10.0->langchain-openai)\n","  Downloading httpcore-1.0.4-py3-none-any.whl (77 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.8/77.8 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.10.0->langchain-openai)\n","  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain)\n","  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n","Installing collected packages: orjson, mypy-extensions, marshmallow, jsonpointer, h11, typing-inspect, tiktoken, jsonpatch, httpcore, langsmith, httpx, dataclasses-json, openai, langchain-core, langchain-text-splitters, langchain-openai, langchain-community, langchain\n","Successfully installed dataclasses-json-0.6.4 h11-0.14.0 httpcore-1.0.4 httpx-0.27.0 jsonpatch-1.33 jsonpointer-2.4 langchain-0.1.11 langchain-community-0.0.27 langchain-core-0.1.30 langchain-openai-0.0.8 langchain-text-splitters-0.0.1 langsmith-0.1.23 marshmallow-3.21.1 mypy-extensions-1.0.0 openai-1.13.3 orjson-3.9.15 tiktoken-0.6.0 typing-inspect-0.9.0\n"]}],"source":["!pip install -U langchain-openai langchain"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"qPt6ee1UBS2r","executionInfo":{"status":"ok","timestamp":1710151597363,"user_tz":-60,"elapsed":219,"user":{"displayName":"Tom TEA","userId":"00599091751588244729"}}},"outputs":[],"source":["import os\n","os.environ['OPENAI_API_KEY'] = \"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\" # À Modifier"]},{"cell_type":"markdown","metadata":{"id":"WSPG-X_-BS2s"},"source":["Initialise le LLM"]},{"cell_type":"code","execution_count":26,"metadata":{"id":"Y_ZNTBKeBS2s","executionInfo":{"status":"ok","timestamp":1710153711279,"user_tz":-60,"elapsed":340,"user":{"displayName":"Tom TEA","userId":"00599091751588244729"}}},"outputs":[],"source":["from langchain_openai import OpenAI\n","\n","# initialize the models\n","llm = OpenAI(\n","    model_name=\"gpt-3.5-turbo-instruct\",\n",")"]},{"cell_type":"markdown","metadata":{"id":"jeESsOmbBS2t"},"source":["### Exercice 1\n","Créez un simple `PromptTemplate` pour un ChatBot qui doit guider des passagers dans un centre commercial.\n","Rajoutez en contexte que le foot-locker se trouve au 3e étage du bâtiment."]},{"cell_type":"code","execution_count":8,"metadata":{"id":"yHX8MlWiBS2t","executionInfo":{"status":"ok","timestamp":1710151606110,"user_tz":-60,"elapsed":192,"user":{"displayName":"Tom TEA","userId":"00599091751588244729"}}},"outputs":[],"source":["from langchain import PromptTemplate\n","\n","template = \"\"\"Réponds à la question grâce au contexte si dessous. S'il n'y a pas de réponses possible à la question avec les informations fournies,\n","réponds simplement \"jsp\".\n","\n","Context:Le foot-locker se trouve au 3e étage du bâtiment.\n","\n","Question: {query}\n","\n","Answer: \"\"\"\n","\n","prompt_template = PromptTemplate(\n","    input_variables=[\"query\"],\n","    template=template\n",")"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"U8Zx3KDeBS2t","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1710151608121,"user_tz":-60,"elapsed":715,"user":{"displayName":"Tom TEA","userId":"00599091751588244729"}},"outputId":"d91c87e0-b235-4f90-81df-c84fddf4a2db"},"outputs":[{"output_type":"stream","name":"stdout","text":["3e étage.\n"]}],"source":["#Call LLM avec le prompt\n","print(llm.invoke(\n","    prompt_template.format(\n","        query=\"Bonjour! Où se trouve le foot-locker?\"\n","    )\n","))"]},{"cell_type":"markdown","metadata":{"id":"hQA_HMFeBS2u"},"source":["### Exercice 2"]},{"cell_type":"code","source":["!pip install langchainhub"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"enEjtXD4-wj1","executionInfo":{"status":"ok","timestamp":1710151664113,"user_tz":-60,"elapsed":8028,"user":{"displayName":"Tom TEA","userId":"00599091751588244729"}},"outputId":"5adc83be-ad60-48c4-f6da-80949668496d"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting langchainhub\n","  Downloading langchainhub-0.1.15-py3-none-any.whl (4.6 kB)\n","Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchainhub) (2.31.0)\n","Collecting types-requests<3.0.0.0,>=2.31.0.2 (from langchainhub)\n","  Downloading types_requests-2.31.0.20240311-py3-none-any.whl (14 kB)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchainhub) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchainhub) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchainhub) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchainhub) (2024.2.2)\n","Installing collected packages: types-requests, langchainhub\n","Successfully installed langchainhub-0.1.15 types-requests-2.31.0.20240311\n"]}]},{"cell_type":"markdown","metadata":{"id":"aJUfL7xjBS2u"},"source":["Langchain vous donne accès à un grand panel de PromptTemplate à traver le [hub](https://smith.langchain.com/hub/search?q=french&organizationId=9a13fe78-6323-5784-993c-9714ceb93dd5)\n","\n","le But de cette exercice est d'analyser les variables d'un promptTemplate pour voir comment le call avec notre LLM\n","Ici le avec template [conversation-title-generator](https://smith.langchain.com/hub/takatost/conversation-title-generator?organizationId=9a13fe78-6323-5784-993c-9714ceb93dd5&tab=0)"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"iXjzthMfBS2u","executionInfo":{"status":"ok","timestamp":1710151667204,"user_tz":-60,"elapsed":218,"user":{"displayName":"Tom TEA","userId":"00599091751588244729"}}},"outputs":[],"source":["from langchain import hub\n","prompt = hub.pull(\"takatost/conversation-title-generator\")"]},{"cell_type":"markdown","metadata":{"id":"qaXlk2lrBS2v"},"source":["Quelles variables d'entrées prennent le template?"]},{"cell_type":"markdown","metadata":{"id":"0bSNjfpOBS2v"},"source":["***Répondez ici***\n","\n","Le template prend en variables d'entrées :\n","- input_variables : subject & intention\n","- template : exemple de réponse attendu"]},{"cell_type":"markdown","metadata":{"id":"h1wNVrY9BS2v"},"source":["En fonction de votre réponse faites un simple call de votre LLM avec le prompt template ci dessus"]},{"cell_type":"code","source":["prompt_template = PromptTemplate(\n","    input_variables=[\"query\"],\n","    template=template\n",")"],"metadata":{"id":"pYjg4MfgDW3I","executionInfo":{"status":"ok","timestamp":1710153012549,"user_tz":-60,"elapsed":3,"user":{"displayName":"Tom TEA","userId":"00599091751588244729"}}},"execution_count":17,"outputs":[]},{"cell_type":"code","execution_count":21,"metadata":{"id":"35jR7gg1BS2v","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1710153057554,"user_tz":-60,"elapsed":713,"user":{"displayName":"Tom TEA","userId":"00599091751588244729"}},"outputId":"fc03a9a5-be7b-4697-8cd9-54073b113145"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'Where is the foot-locker located?'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":21}],"source":["llm.invoke(\n","    prompt_template.format(\n","        query= prompt\n","    )\n",")"]},{"cell_type":"code","source":["prompt"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ixo2SV91EGrZ","executionInfo":{"status":"ok","timestamp":1710153037933,"user_tz":-60,"elapsed":7,"user":{"displayName":"Tom TEA","userId":"00599091751588244729"}},"outputId":"e068ecbc-2a97-4581-b3d3-82db7f891b3c"},"execution_count":19,"outputs":[{"output_type":"execute_result","data":{"text/plain":["ChatPromptTemplate(input_variables=['question'], messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template='You need to decompose the user\\'s input into \"subject\" and \"intention\" in order to accurately figure out what the user\\'s input language actually is. \\nNotice: the language type user use could be diverse, which can be English, Chinese, Español, Arabic, Japanese, French, and etc.\\nMAKE SURE your output is the SAME language as the user\\'s input!\\nYour output is restricted only to: (Input language) Intention + Subject(short as possible)\\nYour output MUST be a valid JSON.\\n\\nTip: When the user\\'s question is directed at you (the language model), you can add an emoji to make it more fun.\\n\\n\\nexample 1:\\nUser Input: hi, yesterday i had some burgers.\\n{{\\n  \"Language Type\": \"The user\\'s input is pure English\",\\n  \"Your Reasoning\": \"The language of my output must be pure English.\",\\n  \"Your Output\": \"sharing yesterday\\'s food\"\\n}}\\n\\nexample 2:\\nUser Input: hello\\n{{\\n  \"Language Type\": \"The user\\'s input is written in pure English\",\\n  \"Your Reasoning\": \"The language of my output must be pure English.\",\\n  \"Your Output\": \"Greeting myself☺️\"\\n}}\\n\\n\\nexample 3:\\nUser Input: why mmap file: oom\\n{{\\n  \"Language Type\": \"The user\\'s input is written in pure English\",\\n  \"Your Reasoning\": \"The language of my output must be pure English.\",\\n  \"Your Output\": \"Asking about the reason for mmap file: oom\"\\n}}\\n\\n\\nexample 4:\\nUser Input: www.convinceme.yesterday-you-ate-seafood.tv讲了什么？\\n{{\\n  \"Language Type\": \"The user\\'s input English-Chinese mixed\",\\n  \"Your Reasoning\": \"The English-part is an URL, the main intention is still written in Chinese, so the language of my output must be using Chinese.\",\\n  \"Your Output\": \"询问网站www.convinceme.yesterday-you-ate-seafood.tv\"\\n}}\\n\\nexample 5:\\nUser Input: why小红的年龄is老than小明？\\n{{\\n  \"Language Type\": \"The user\\'s input is English-Chinese mixed\",\\n  \"Your Reasoning\": \"The English parts are subjective particles, the main intention is written in Chinese, besides, Chinese occupies a greater \\\\\"actual meaning\\\\\" than English, so the language of my output must be using Chinese.\",\\n  \"Your Output\": \"询问小红和小明的年龄\"\\n}}\\n\\nexample 6:\\nUser Input: yo, 你今天咋样？\\n{{\\n  \"Language Type\": \"The user\\'s input is English-Chinese mixed\",\\n  \"Your Reasoning\": \"The English-part is a subjective particle, the main intention is written in Chinese, so the language of my output must be using Chinese.\",\\n  \"Your Output\": \"查询今日我的状态☺️\"\\n}}')), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['question'], template='{question}'))])"]},"metadata":{},"execution_count":19}]},{"cell_type":"markdown","metadata":{"id":"L5MMea0ZBS2v"},"source":["### Exercice 3\n","\n","Vous voulez utiliser un LLM pour essayer de créer des recettes à partir d'ingrédients que vous avez en reste chez vous.\n","\n","Créez un Prompt template prenant deux variables. Un variable `{type_plat}`pour savoir quel type de plat vous voulez cuisiner et une `ingredients`contenant la liste d'ingrédients que vous voulez cuisiner.\n","\n","Après avoir crée votre prompt template. Appelez votre LLM avec les entrées suivantes"]},{"cell_type":"code","execution_count":23,"metadata":{"id":"EaDHphS8BS2w","executionInfo":{"status":"ok","timestamp":1710153569615,"user_tz":-60,"elapsed":2,"user":{"displayName":"Tom TEA","userId":"00599091751588244729"}}},"outputs":[],"source":["plat=\"Dessert\"\n","\n","ingredients=\"de la semoule, de l'huile d'olive, du persil, des patates et du chocolat\""]},{"cell_type":"code","source":["prompt = PromptTemplate.from_template(\"Quel est le type de plat que vous souhaitez cuisiner {plat} et avec quels ingrédients {ingredients} ?\")"],"metadata":{"id":"Vz_9KNu2GH0L","executionInfo":{"status":"ok","timestamp":1710153737906,"user_tz":-60,"elapsed":3,"user":{"displayName":"Tom TEA","userId":"00599091751588244729"}}},"execution_count":27,"outputs":[]},{"cell_type":"code","source":["print(llm.invoke(prompt.format(plat = plat, ingredients = ingredients)))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nyTpAI8YGmQ9","executionInfo":{"status":"ok","timestamp":1710153946392,"user_tz":-60,"elapsed":3291,"user":{"displayName":"Tom TEA","userId":"00599091751588244729"}},"outputId":"5c5de073-d685-404a-c9eb-c894d08d5038"},"execution_count":29,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","\n","Je n'ai pas de recette précise en tête, mais voici une idée de dessert original avec ces ingrédients :\n","\n","Gâteau de semoule à la patate douce et au chocolat :\n","\n","Ingrédients :\n","- 250g de semoule fine\n","- 500ml d'eau\n","- 1 patate douce\n","- 50g de chocolat noir\n","- Un bouquet de persil\n","- 2 cuillères à soupe d'huile d'olive\n","- 50g de sucre\n","\n","Instructions :\n","1. Préchauffez votre four à 180°C.\n","2. Dans une casserole, faites bouillir l'eau avec l'huile d'olive et le sucre.\n","3. Ajoutez la semoule en pluie et laissez cuire à feu doux pendant 5 minutes en remuant régulièrement.\n","4. Épluchez et râpez la patate douce, puis ajoutez-la à la semoule.\n","5. Faites fondre le chocolat au bain-marie.\n","6. Ciselez le persil et ajoutez-le à la préparation\n"]}]},{"cell_type":"markdown","metadata":{"id":"dw7gD-RjBS2w"},"source":["### Exercice 4\n","\n","Few Shot Template"]},{"cell_type":"markdown","metadata":{"id":"qbP2q8kyBS2w"},"source":["L'objectif de ce prompt FewShot est de fournir des exemples liés au cinéma et de guider le LLM dans la génération de réponses basées sur des requêtes liées au cinéma."]},{"cell_type":"code","execution_count":35,"metadata":{"id":"gUBcSy5WBS2w","executionInfo":{"status":"ok","timestamp":1710154196970,"user_tz":-60,"elapsed":296,"user":{"displayName":"Tom TEA","userId":"00599091751588244729"}}},"outputs":[],"source":["example = [\n","{\"Question\" : \"Qui a réalisé le film 'Inception' ?\",\n"," \"Réponse\" : \"Le réalisateur de 'Inception' est Christopher Nolan\"\n","},{\n","\"Question\" : \"Quelle est l'intrigue du film 'The Shawshank Redemption' ?\",\n","\"Réponse\": \"L'intrigue du film 'The Shawshank Redemption' tourne autour d'un banquier condamné à la prison à vie de Shawshank pour le meurtre de sa femme et de l'amant de celle-ci.\"\n","},{\n","\"Question\" : \"Quel acteur a joué le rôle de 'Forrest Gump' dans le film 'Forrest Gump' ?\",\n","\"Réponse\" : \"Le personnage de 'Forrest Gump' a été joué par Tom Hanks.\"\n","},{\n","\"Question\" : \"Quel est le film qui a rapporté le plus d'argent de tous les temps ?\",\n","\"Réponse\" : \"Le film qui a rapporté le plus d'argent de tous les temps est 'Avatar'\"\n","}\n","]"]},{"cell_type":"markdown","metadata":{"id":"bSubBm-NBS2w"},"source":["Créer un FewShotPromptTemplate : Utilisez la bibliothèque LangChain pour créer un objet FewShotPromptTemplate qui comprend les exemples liés au cinéma et un formateur pour les exemples de quelques plans.\n","Guidez le modèle : Demandez au modèle d'utiliser ces exemples pour comprendre les requêtes liées au cinéma et y répondre efficacement."]},{"cell_type":"code","execution_count":39,"metadata":{"id":"OwsVL55-BS2w","executionInfo":{"status":"ok","timestamp":1710154294199,"user_tz":-60,"elapsed":204,"user":{"displayName":"Tom TEA","userId":"00599091751588244729"}}},"outputs":[],"source":["from langchain import FewShotPromptTemplate\n","\n","# create a example template\n","example_template = \"\"\"\n","User: {Question}\n","AI: {Réponse}\n","\"\"\"\n","\n","# create a prompt example from above template\n","example_prompt = PromptTemplate(\n","    input_variables=[\"Question\", \"Réponse\"],\n","    template=example_template\n",")\n","\n","# now break our previous prompt into a prefix and suffix\n","# the prefix is our instructions\n","prefix = \"\"\"Voici des extraits de questions réponses concernant le cinéma avec un assistant d'IA.\n","Voici quelques exemples:\n","\"\"\"\n","# and the suffix our user input and output indicator\n","suffix = \"\"\"\n","User: {Question}\n","AI: \"\"\"\n","\n","# now create the few shot prompt template\n","few_shot_prompt_template = FewShotPromptTemplate(\n","    examples=example,\n","    example_prompt=example_prompt,\n","    prefix=prefix,\n","    suffix=suffix,\n","    input_variables=[\"Question\"],\n","    example_separator=\"\\n\\n\"\n",")"]},{"cell_type":"code","source":["query = \"Qui a réalise le film 'Inception'?\"\n","\n","print(few_shot_prompt_template.format(Question=query))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zwFOzbIoIaBu","executionInfo":{"status":"ok","timestamp":1710154307468,"user_tz":-60,"elapsed":226,"user":{"displayName":"Tom TEA","userId":"00599091751588244729"}},"outputId":"d707c2b8-26a0-4bf8-8a46-fdd7118d90f2"},"execution_count":41,"outputs":[{"output_type":"stream","name":"stdout","text":["Voici des extraits de questions réponses concernant le cinéma avec un assistant d'IA. \n","Voici quelques exemples: \n","\n","\n","\n","User: Qui a réalisé le film 'Inception' ?\n","AI: Le réalisateur de 'Inception' est Christopher Nolan\n","\n","\n","\n","User: Quelle est l'intrigue du film 'The Shawshank Redemption' ?\n","AI: L'intrigue du film 'The Shawshank Redemption' tourne autour d'un banquier condamné à la prison à vie de Shawshank pour le meurtre de sa femme et de l'amant de celle-ci.\n","\n","\n","\n","User: Quel acteur a joué le rôle de 'Forrest Gump' dans le film 'Forrest Gump' ?\n","AI: Le personnage de 'Forrest Gump' a été joué par Tom Hanks.\n","\n","\n","\n","User: Quel est le film qui a rapporté le plus d'argent de tous les temps ?\n","AI: Le film qui a rapporté le plus d'argent de tous les temps est 'Avatar'\n","\n","\n","\n","User: Qui a réalise le film 'Inception'?\n","AI: \n"]}]},{"cell_type":"code","source":["query = \"Qui a réalise le film 'Forest Gump\"\n","print(llm.invoke(few_shot_prompt_template.format(Question=query)))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Q7C602qDJGOT","executionInfo":{"status":"ok","timestamp":1710154399855,"user_tz":-60,"elapsed":526,"user":{"displayName":"Tom TEA","userId":"00599091751588244729"}},"outputId":"794ea364-e1f2-4da4-ccb2-577d534b650a"},"execution_count":45,"outputs":[{"output_type":"stream","name":"stdout","text":["Le réalisateur de 'Forest Gump' est Robert Zemeckis.\n"]}]},{"cell_type":"markdown","metadata":{"id":"N-PQgIMuBS2x"},"source":["### Exercice 5\n","ChatModel et LLM"]},{"cell_type":"markdown","metadata":{"id":"qF0Ck5B-BS2x"},"source":["Reprenons le cas d'utilisation de l'exercice 1\n","Essayons de mettre ce contexte juste pour un ChatModel"]},{"cell_type":"code","execution_count":47,"metadata":{"id":"PJllibxmBS2x","executionInfo":{"status":"ok","timestamp":1710159908471,"user_tz":-60,"elapsed":461,"user":{"displayName":"Tom TEA","userId":"00599091751588244729"}}},"outputs":[],"source":["from langchain_openai import ChatOpenAI\n","\n","from langchain import PromptTemplate\n","from langchain.prompts.chat import (\n","    ChatPromptTemplate,\n","    SystemMessagePromptTemplate,\n","    AIMessagePromptTemplate,\n","    HumanMessagePromptTemplate,\n",")\n","from langchain.schema import (\n","    AIMessage,\n","    HumanMessage,\n","    SystemMessage\n",")\n","\n","chat = ChatOpenAI(temperature=0.5)"]},{"cell_type":"markdown","metadata":{"id":"lnGXMTBSBS2x"},"source":["Créez un `ChatPromptTemplate` d'un assistant IA d'un centre commercial qui aura son nom en paramètre et une question de l'utilisateurs"]},{"cell_type":"code","execution_count":51,"metadata":{"id":"wW5jyZLwBS2y","executionInfo":{"status":"ok","timestamp":1710160006632,"user_tz":-60,"elapsed":304,"user":{"displayName":"Tom TEA","userId":"00599091751588244729"}}},"outputs":[],"source":["chat_template = ChatPromptTemplate.from_messages(\n","    [\n","        (\"system\", \"You are a helpful AI bot. Your name is {name}.\"),\n","        (\"human\", \"Hello, how are you doing?\"),\n","        (\"ai\", \"I'm doing well, thanks!\"),\n","        (\"human\", \"{user_input}\"),\n","    ]\n",")"]},{"cell_type":"code","source":["messages = chat_template.format_messages(\n","    name=\"Idriss\",\n","    user_input=\"Où sommes-nous?\"\n",")"],"metadata":{"id":"RGGA9LX3ebp8","executionInfo":{"status":"ok","timestamp":1710160006944,"user_tz":-60,"elapsed":2,"user":{"displayName":"Tom TEA","userId":"00599091751588244729"}}},"execution_count":52,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QwxEDSL8BS2y"},"outputs":[],"source":["chat.invoke(messages)"]},{"cell_type":"markdown","metadata":{"id":"PQ003PZoBS2y"},"source":["# Exercice 6\n","\n","Output Parser\n","\n","À partir de la sortie du LLM ci-dessous, implémentez un `OutputParser` pour avoir la sortie du LLM en format JSON"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cAXZI3T1BS2y"},"outputs":[],"source":["from langchain_core.output_parsers import JsonOutputParser\n","\n","prompt = hub.pull(\"ripemetrics/aspect-based-sentiment-analysis\")\n","llm = OpenAI(model=\"gpt-3.5-turbo-instruct\",temperature=0)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WgJJriTZBS2y"},"outputs":[],"source":["# Create runnable\n","runnable = prompt | llm #basic chain, on verra ca en detail au prochain TP\n","\n","output = runnable.invoke({\"user_comment\":\"Aujourd'hui j'ai eu une promotion grâce au nouveau produit que j'ai déployé!\"})\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8DZww8ZNBS2y","outputId":"8d98ebe5-54e2-46d7-9c43-be31493cc1e2"},"outputs":[{"name":"stdout","output_type":"stream","text":["{'aspects': [{'text': 'promotion', 'sentiment': {'score': 0.9, 'label': 'positive'}}, {'text': 'nouveau produit', 'sentiment': {'score': 0.9, 'label': 'positive'}}, {'text': 'déployé', 'sentiment': {'score': 0.9, 'label': 'positive'}}]}\n"]}],"source":["# Implement the JSON Output Parser\n","\n","#TO DO"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bGChehgIBS2z"},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"base","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.10"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}