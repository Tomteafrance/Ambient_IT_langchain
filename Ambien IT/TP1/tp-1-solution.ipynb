{"cells":[{"cell_type":"markdown","metadata":{"id":"RPZmxoJ8BS2l"},"source":["# Cours LangChain TP1"]},{"cell_type":"markdown","metadata":{"id":"IYHMYRjsBS2p"},"source":["Dans ce TP vous allez d'abord apprendre à créez des Prompt Templates et comprenez comment fournir des instructions au LLM."]},{"cell_type":"markdown","metadata":{"id":"hqiuuQpQBS2p"},"source":["preinstall et export de clé"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xMmI0ONABS2q"},"outputs":[],"source":["!pip install -U langchain-openai langchain langchainhub"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"qPt6ee1UBS2r"},"outputs":[],"source":["import os\n","os.environ['OPENAI_API_KEY']=\"OPENAI_API_KEY\" # À Modifier"]},{"cell_type":"markdown","metadata":{"id":"WSPG-X_-BS2s"},"source":["Initialise le LLM"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"Y_ZNTBKeBS2s"},"outputs":[],"source":["from langchain_openai import OpenAI\n","\n","# initialize the models\n","llm = OpenAI(\n","    model_name=\"gpt-3.5-turbo-instruct\",\n",")"]},{"cell_type":"markdown","metadata":{"id":"jeESsOmbBS2t"},"source":["### Exercice 1\n","Créez un simple `PromptTemplate` pour un ChatBot qui doit guider des passagers dans un centre commercial.\n","Rajoutez en contexte que le foot-locker se trouve au 3e étage du bâtiment."]},{"cell_type":"code","execution_count":5,"metadata":{"id":"yHX8MlWiBS2t"},"outputs":[],"source":["from langchain import PromptTemplate\n","\n","template = \"\"\"Vous êtes un assistant de centre commercial qui doit guider les passager\n","            Context le foot locker se trouve au troisième étage du batiment\n","\n","            Question: {query}\n","\n","            Réponse:\"\"\"\n","\n","prompt_template = PromptTemplate(\n","    input_variables=[\"query\"],\n","    template=template\n",")"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"U8Zx3KDeBS2t"},"outputs":[{"name":"stdout","output_type":"stream","text":[" Bonjour! Le foot-locker se trouve au troisième étage du bâtiment. Suivez simplement les indications pour y arriver. Est-ce que vous avez besoin de plus d'aide pour trouver votre chemin?\n"]}],"source":["#Call LLM avec le prompt\n","print(llm.invoke(\n","    prompt_template.format(\n","        query=\"Bonjour! Où se trouve le foot-locker?\"\n","    )\n","))"]},{"cell_type":"markdown","metadata":{"id":"hQA_HMFeBS2u"},"source":["### Exercice 2"]},{"cell_type":"markdown","metadata":{"id":"aJUfL7xjBS2u"},"source":["Langchain vous donne accès à un grand panel de PromptTemplate à traver le [hub](https://smith.langchain.com/hub/search?q=french&organizationId=9a13fe78-6323-5784-993c-9714ceb93dd5)\n","\n","le But de cette exercice est d'analyser les variables d'un promptTemplate pour voir comment le call avec notre LLM\n","Ici le avec template [conversation-title-generator](https://smith.langchain.com/hub/takatost/conversation-title-generator?organizationId=9a13fe78-6323-5784-993c-9714ceb93dd5&tab=0)"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"iXjzthMfBS2u"},"outputs":[],"source":["from langchain import hub\n","prompt = hub.pull(\"takatost/conversation-title-generator\")"]},{"cell_type":"markdown","metadata":{"id":"qaXlk2lrBS2v"},"source":["Quelles variables d'entrées prennent le template?"]},{"cell_type":"markdown","metadata":{"id":"0bSNjfpOBS2v"},"source":["***Répondez ici***"]},{"cell_type":"markdown","metadata":{"id":"h1wNVrY9BS2v"},"source":["En fonction de votre réponse faites un simple call de votre LLM avec le prompt template ci dessus"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"35jR7gg1BS2v"},"outputs":[{"data":{"text/plain":["'\\n\\nBonjour, oui ça va merci et vous ?'"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["llm.invoke(\n","    \"Bonjour ca va?\"\n",")"]},{"cell_type":"markdown","metadata":{"id":"L5MMea0ZBS2v"},"source":["### Exercice 3\n","\n","Vous voulez utiliser un LLM pour essayer de créer des recettes à partir d'ingrédients que vous avez en reste chez vous.\n","\n","Créez un Prompt template prenant deux variables. Un variable `{type_plat}`pour savoir quel type de plat vous voulez cuisiner et une `ingredients`contenant la liste d'ingrédients que vous voulez cuisiner.\n","\n","Après avoir crée votre prompt template. Appelez votre LLM avec les entrées suivantes"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"EaDHphS8BS2w"},"outputs":[],"source":["plat=\"Dessert\"\n","\n","ingredients=\"de la semoule, de l'huile d'olive, du persil, des patates et du chocolat\""]},{"cell_type":"code","execution_count":27,"metadata":{},"outputs":[{"data":{"text/plain":["\"\\n\\nRecette de Gâteau de semoule aux patates et au chocolat :\\n\\nIngrédients :\\n- 200g de semoule fine\\n- 2 cuillères à soupe d'huile d'olive\\n- 1 bouquet de persil frais\\n- 2 patates moyennes\\n- 100g de chocolat noir\\n- 50g de sucre en poudre\\n- 1 sachet de levure chimique\\n- 1 pincée de sel\\n- 1 cuillère à soupe de cacao en poudre\\n- 50g de beurre\\n- 2 œufs\\n- 1 cuillère à soupe de lait\\n\\nPréparation :\\n\\n1. Épluchez et coupez les patates en petits cubes. Faites-les cuire dans une casserole d'eau bouillante pendant 15 minutes. Égouttez-les et écrasez-les à la fourchette pour obtenir une purée.\\n\\n2. Dans un saladier, mélangez la semoule, le sucre, la levure, le sel et le cacao en poudre\""]},"execution_count":27,"metadata":{},"output_type":"execute_result"}],"source":["prompt_template= PromptTemplate.from_template(\"Donne-moi une recette de/d' {plat} avec les ingredients suivants:{ingredients}\")\n","llm.invoke(prompt_template.format(plat=plat,ingredients=ingredients))"]},{"cell_type":"markdown","metadata":{"id":"dw7gD-RjBS2w"},"source":["### Exercice 4\n","\n","Few Shot Template"]},{"cell_type":"markdown","metadata":{"id":"qbP2q8kyBS2w"},"source":["L'objectif de ce prompt FewShot est de fournir des exemples liés au cinéma et de guider le LLM dans la génération de réponses basées sur des requêtes liées au cinéma."]},{"cell_type":"code","execution_count":36,"metadata":{"id":"gUBcSy5WBS2w"},"outputs":[],"source":["example = [\n","{\"query\" : \"Qui a réalisé le film 'Inception' ?\",\n"," \"answer\" : \"Le réalisateur de 'Inception' est Christopher Nolan\"\n","},{\n","\"query\" : \"Quelle est l'intrigue du film 'The Shawshank Redemption' ?\",\n","\"answer\": \"L'intrigue du film 'The Shawshank Redemption' tourne autour d'un banquier condamné à la prison à vie de Shawshank pour le meurtre de sa femme et de l'amant de celle-ci.\"\n","},{\n","\"query\" : \"Quel acteur a joué le rôle de 'Forrest Gump' dans le film 'Forrest Gump' ?\",\n","\"answer\" : \"Le personnage de 'Forrest Gump' a été joué par Tom Hanks.\"\n","},{\n","\"query\" : \"Quel est le film qui a rapporté le plus d'argent de tous les temps ?\",\n","\"answer\" : \"Le film qui a rapporté le plus d'argent de tous les temps est 'Avatar'\"\n","}\n","]"]},{"cell_type":"markdown","metadata":{"id":"bSubBm-NBS2w"},"source":["Créer un FewShotPromptTemplate : Utilisez la bibliothèque LangChain pour créer un objet FewShotPromptTemplate qui comprend les exemples liés au cinéma et un formateur pour les exemples de quelques plans.\n","Guidez le modèle : Demandez au modèle d'utiliser ces exemples pour comprendre les requêtes liées au cinéma et y répondre efficacement."]},{"cell_type":"code","execution_count":37,"metadata":{"id":"OwsVL55-BS2w"},"outputs":[],"source":["from langchain import FewShotPromptTemplate\n","\n","#TO DO\n","# create a example template\n","example_template = \"\"\"\n","Question: {query}\n","Réponse: {answer}\n","\"\"\"\n","\n","# create a prompt example from above template\n","example_prompt = PromptTemplate(\n","    input_variables=[\"query\", \"answer\"],\n","    template=example_template\n",")\n","\n","# now break our previous prompt into a prefix and suffix\n","# the prefix is our instructions\n","prefix = \"\"\"Voici des extraits de conversations avec un assistant d'IA.\n","L'assistant est généralement sarcastique et plein d'esprit, produisant des réponses créatives et amusantes aux questions des utilisateurs. \n","Voici quelques exemples: \n","\"\"\"\n","# and the suffix our user input and output indicator\n","suffix = \"\"\"\n","User: {query}\n","AI: \"\"\"\n","\n","# now create the few shot prompt template\n","few_shot_prompt_template = FewShotPromptTemplate(\n","    examples=example,\n","    example_prompt=example_prompt,\n","    prefix=prefix,\n","    suffix=suffix,\n","    input_variables=[\"query\"],\n","    example_separator=\"\\n\\n\"\n",")"]},{"cell_type":"code","execution_count":38,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Eh bien, il y a ce petit acteur nommé Leonardo DiCaprio et une certaine Kate Winslet. Peut-être les avez-vous déjà entendus parler ?\n"]}],"source":["query = \"Qui joue dans Titanic?\"\n","\n","print(llm.invoke(few_shot_prompt_template.format(query=query)))"]},{"cell_type":"markdown","metadata":{"id":"N-PQgIMuBS2x"},"source":["### Exercice 5\n","ChatModel et LLM"]},{"cell_type":"markdown","metadata":{"id":"qF0Ck5B-BS2x"},"source":["Reprenons le cas d'utilisation de l'exercice 1\n","Essayons de mettre ce contexte juste pour un ChatModel"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"PJllibxmBS2x"},"outputs":[],"source":["from langchain_openai import ChatOpenAI\n","\n","from langchain import PromptTemplate\n","from langchain.prompts.chat import (\n","    ChatPromptTemplate,\n","    SystemMessagePromptTemplate,\n","    AIMessagePromptTemplate,\n","    HumanMessagePromptTemplate,\n",")\n","from langchain.schema import (\n","    AIMessage,\n","    HumanMessage,\n","    SystemMessage\n",")\n","\n","chat = ChatOpenAI(temperature=0.5)"]},{"cell_type":"markdown","metadata":{"id":"lnGXMTBSBS2x"},"source":["Créez un `ChatPromptTemplate` d'un assistant IA d'un centre commercial qui aura son nom en paramètre et une question de l'utilisateurs"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"wW5jyZLwBS2y"},"outputs":[],"source":["template = ChatPromptTemplate.from_template(\"Tu es un assistant IA qui s'appelle {name} et qui réponds aux question de l'utilisateurs: {user_input}\")\n","\n","messages = template.format_messages(\n","    name=\"Idriss\",\n","    user_input=\"Où sommes-nous?\"\n",")"]},{"cell_type":"code","execution_count":13,"metadata":{"id":"QwxEDSL8BS2y"},"outputs":[{"data":{"text/plain":["AIMessage(content=\"Nous sommes actuellement sur une plateforme de discussion en ligne. Comment puis-je vous aider aujourd'hui?\")"]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":["chat.invoke(messages)"]},{"cell_type":"markdown","metadata":{"id":"PQ003PZoBS2y"},"source":["# Exercice 6\n","\n","Output Parser\n","\n","À partir de la sortie du LLM ci-dessous, implémentez un `OutputParser` pour avoir la sortie du LLM en format JSON"]},{"cell_type":"code","execution_count":19,"metadata":{"id":"cAXZI3T1BS2y"},"outputs":[],"source":["from langchain_core.output_parsers import JsonOutputParser\n","\n","prompt = hub.pull(\"ripemetrics/aspect-based-sentiment-analysis\")\n","llm = OpenAI(model=\"gpt-3.5-turbo-instruct\",temperature=0)"]},{"cell_type":"code","execution_count":20,"metadata":{"id":"WgJJriTZBS2y"},"outputs":[],"source":["# Create runnable\n","runnable = prompt | llm #basic chain, on verra ca en detail au prochain TP\n","\n","output = runnable.invoke({\"user_comment\":\"Aujourd'hui j'ai eu une promotion grâce au nouveau produit que j'ai déployé!\"})"]},{"cell_type":"code","execution_count":24,"metadata":{"id":"8DZww8ZNBS2y","outputId":"8d98ebe5-54e2-46d7-9c43-be31493cc1e2"},"outputs":[{"data":{"text/plain":["{'aspects': [{'text': 'promotion',\n","   'sentiment': {'score': 0.9, 'label': 'positive'}},\n","  {'text': 'nouveau produit',\n","   'sentiment': {'score': 0.9, 'label': 'positive'}},\n","  {'text': 'déployé', 'sentiment': {'score': 0.9, 'label': 'positive'}}]}"]},"execution_count":24,"metadata":{},"output_type":"execute_result"}],"source":["# Implement the JSON Output Parser\n","\n","#TO DO\n","parser= JsonOutputParser()\n","parser.invoke(output)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bGChehgIBS2z"},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"base","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.10"}},"nbformat":4,"nbformat_minor":0}
