{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U langchain-openai langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define constante \n",
    "OPENAI_API_KEY= \"YOUR_API_KEY\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "installs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clé API "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Structure d'un Prompt\n",
    "\n",
    "Une prompt peut comporter plusieurs éléments :\n",
    "\n",
    "* Instructions\n",
    "* Informations externes ou contexte\n",
    "* Entrée ou requête de l'utilisateur\n",
    "* Indicateur de sortie\n",
    "\n",
    "Tous les prompt n'ont pas besoin de tous ces éléments, mais un bon prompt en utilise souvent au moins deux. Définissons plus précisément ce qu'ils sont:\n",
    "\n",
    "Les **instructions** indiquent au modèle ce qu'il doit faire, généralement comment il doit utiliser les entrées et/ou les informations externes pour produire la sortie souhaitée.\n",
    "\n",
    "**Les informations externes ou le contexte** sont des informations supplémentaires que nous insérons manuellement dans l'invite, que nous récupérons via une base de données vectorielle (mémoire à long terme) ou que nous obtenons par d'autres moyens (appels d'API, calculs, etc.).\n",
    "\n",
    "**L'entrée ou la requête de l'utilisateur** est généralement une requête directement introduite par l'utilisateur du système.\n",
    "\n",
    "**L'indicateur de sortie** est le début du texte généré. Pour un modèle générant du code Python, nous pouvons mettre `import ` (car la plupart des scripts Python commencent par un `import` de bibliothèque), ou un chatbot peut commencer par `Chatbot : ` (en supposant que nous formatons le script du chatbot comme des lignes de texte interchangées entre `User` et `Chatbot`).\n",
    "\n",
    "Chacune de ces composantes doit généralement être placé dans l'ordre où nous les avons décrits. Nous commençons par les instructions, nous fournissons le contexte (si nécessaire), puis nous ajoutons l'entrée de l'utilisateur, et enfin nous terminons par l'indicateur de sortie."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "exemple de prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt=\"\"\"Réponds à la question grâce au contexte si dessous. S'il n'y a pas de réponses possible à la question avec les informations fournies, \n",
    "réponds simplement \"jsp\".\n",
    "\n",
    "Context:Les grands modèles de langage (LLM) sont les modèles les plus récents utilisés dans le domaine du NLP.\n",
    "Leurs performances supérieures à celles des modèles plus petits les ont rendus incroyablement utiles pour les développeurs d'applications NLP.\n",
    "Ces modèles sont accessibles via la bibliothèque `transformers` de Hugging Face, via OpenAI en utilisant la bibliothèque `openai`, \n",
    "et via Cohere en utilisant la bibliothèque `cohere`.\n",
    "\n",
    "Question: Quelle bibliothèque et fournisseurs de modèles proposent des LLMs?\n",
    "\n",
    "Réponse:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \"La bibliothèque `transformers` de Hugging Face, via OpenAI en utilisant la bibliothèque `openai`, et via Cohere en utilisant la bibliothèque `cohere`.\" \n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import OpenAI\n",
    "\n",
    "# initialize the models\n",
    "openai = OpenAI(\n",
    "    model_name=\"gpt-3.5-turbo-instruct\",\n",
    "    openai_api_key=OPENAI_API_KEY\n",
    ")\n",
    "print(openai.invoke(prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `PromptTemplate`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate\n",
    "\n",
    "template = \"\"\"Réponds à la question grâce au contexte si dessous. S'il n'y a pas de réponses possible à la question avec les informations fournies, \n",
    "réponds simplement \"jsp\".\n",
    "\n",
    "Context:Les grands modèles de langage (LLM) sont les modèles les plus récents utilisés dans le domaine du NLP.\n",
    "Leurs performances supérieures à celles des modèles plus petits les ont rendus incroyablement utiles pour les développeurs d'applications NLP.\n",
    "Ces modèles sont accessibles via la bibliothèque `transformers` de Hugging Face, via OpenAI en utilisant la bibliothèque `openai`, \n",
    "et via Cohere en utilisant la bibliothèque `cohere`.\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "Answer: \"\"\"\n",
    "\n",
    "prompt_template = PromptTemplate(\n",
    "    input_variables=[\"query\"],\n",
    "    template=template\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Réponds à la question grâce au contexte si dessous. S'il n'y a pas de réponses possible à la question avec les informations fournies, \n",
      "réponds simplement \"jsp\".\n",
      "\n",
      "Context:Les grands modèles de langage (LLM) sont les modèles les plus récents utilisés dans le domaine du NLP.\n",
      "Leurs performances supérieures à celles des modèles plus petits les ont rendus incroyablement utiles pour les développeurs d'applications NLP.\n",
      "Ces modèles sont accessibles via la bibliothèque `transformers` de Hugging Face, via OpenAI en utilisant la bibliothèque `openai`, \n",
      "et via Cohere en utilisant la bibliothèque `cohere`.\n",
      "\n",
      "Question: Quelle bibliothèque et fournisseurs de modèles proposent des LLMs?\n",
      "\n",
      "Answer: \n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    prompt_template.format(\n",
    "        query=\"Quelle bibliothèque et fournisseurs de modèles proposent des LLMs?\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Et on peut ensuite le feed à notre LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La bibliothèque `transformers` de Hugging Face, OpenAI en utilisant la bibliothèque `openai` et Cohere en utilisant la bibliothèque `cohere`.\n"
     ]
    }
   ],
   "source": [
    "print(openai.invoke(\n",
    "    prompt_template.format(\n",
    "        query=\"Quelle bibliothèque et fournisseurs de modèles proposent des LLMs?\"\n",
    "    )\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Raconte-moi une sombre blague sur la police.'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt_template = PromptTemplate.from_template(\n",
    "    \"Raconte-moi un.e {adjective} blague sur {content}.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Pourquoi les policiers aiment-ils les escaliers ?\n",
      "\n",
      "Parce qu'ils peuvent monter les marches sans être vus !\n"
     ]
    }
   ],
   "source": [
    "print(openai.invoke(prompt_template.format(adjective=\"drole\", content=\"la police\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `FewShotPromptTemplate`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import FewShotPromptTemplate\n",
    "\n",
    "# create our examples\n",
    "examples = [\n",
    "    {\n",
    "        \"query\": \"Comment ca-va?\",\n",
    "        \"answer\": \"Je ne suis pas à plaindre me je le fais quand-même.\"\n",
    "    }, {\n",
    "        \"query\": \"Quel heure est-il?\",\n",
    "        \"answer\": \"L'heure de vous acheter une montre.\"\n",
    "    }, {\n",
    "        \"query\": \"À quoi sert un baromètre?\",\n",
    "        \"answer\": \"À feur.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# create a example template\n",
    "example_template = \"\"\"\n",
    "User: {query}\n",
    "AI: {answer}\n",
    "\"\"\"\n",
    "\n",
    "# create a prompt example from above template\n",
    "example_prompt = PromptTemplate(\n",
    "    input_variables=[\"query\", \"answer\"],\n",
    "    template=example_template\n",
    ")\n",
    "\n",
    "# now break our previous prompt into a prefix and suffix\n",
    "# the prefix is our instructions\n",
    "prefix = \"\"\"Voici des extraits de conversations avec un assistant d'IA.\n",
    "L'assistant est généralement sarcastique et plein d'esprit, produisant des réponses créatives et amusantes aux questions des utilisateurs. \n",
    "Voici quelques exemples: \n",
    "\"\"\"\n",
    "# and the suffix our user input and output indicator\n",
    "suffix = \"\"\"\n",
    "User: {query}\n",
    "AI: \"\"\"\n",
    "\n",
    "# now create the few shot prompt template\n",
    "few_shot_prompt_template = FewShotPromptTemplate(\n",
    "    examples=examples,\n",
    "    example_prompt=example_prompt,\n",
    "    prefix=prefix,\n",
    "    suffix=suffix,\n",
    "    input_variables=[\"query\"],\n",
    "    example_separator=\"\\n\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Voici des extraits de conversations avec un assistant d'IA.\n",
      "L'assistant est généralement sarcastique et plein d'esprit, produisant des réponses créatives et amusantes aux questions des utilisateurs. \n",
      "Voici quelques exemples: \n",
      "\n",
      "\n",
      "\n",
      "User: Comment ca-va?\n",
      "AI: Je ne suis pas à plaindre me je le fais quand-même.\n",
      "\n",
      "\n",
      "\n",
      "User: Quel heure est-il?\n",
      "AI: L'heure de vous acheter une montre.\n",
      "\n",
      "\n",
      "\n",
      "User: Quel est le sens de la vie?\n",
      "AI: \n"
     ]
    }
   ],
   "source": [
    "query = \"Quel est le sens de la vie?\"\n",
    "\n",
    "print(few_shot_prompt_template.format(query=query))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"42, selon les Monty Python. Mais je pense que c'est plus compliqué que ça.\""
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"Quel est le sens de la vie?\"\n",
    "openai.invoke(few_shot_prompt_template.format(query=query))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Autre exemple avec un assistant beaucoup plus concis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import FewShotPromptTemplate\n",
    "\n",
    "# create our examples\n",
    "examples = [\n",
    "    {\n",
    "        \"query\": \"Comment ca-va?\",\n",
    "        \"answer\": \"Ca-va.\"\n",
    "    }, {\n",
    "        \"query\": \"Quel heure est-il?\",\n",
    "        \"answer\": \"12:54\"\n",
    "    }, {\n",
    "        \"query\": \"À quoi sert un baromètre?\",\n",
    "        \"answer\": \"À mesurer la pression.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# create a example template\n",
    "example_template = \"\"\"\n",
    "User: {query}\n",
    "AI: {answer}\n",
    "\"\"\"\n",
    "\n",
    "# create a prompt example from above template\n",
    "example_prompt = PromptTemplate(\n",
    "    input_variables=[\"query\", \"answer\"],\n",
    "    template=example_template\n",
    ")\n",
    "\n",
    "# now break our previous prompt into a prefix and suffix\n",
    "# the prefix is our instructions\n",
    "prefix = \"\"\"Voici des extraits de conversations avec un assistant d'IA.\n",
    "L'assistant est sérieux et produits des réponses simples et courtes, d'un ou deux mots, aux questions des utilisateurs. \n",
    "Voici quelques exemples: \n",
    "\"\"\"\n",
    "# and the suffix our user input and output indicator\n",
    "suffix = \"\"\"\n",
    "User: {query}\n",
    "AI: \"\"\"\n",
    "\n",
    "# now create the few shot prompt template\n",
    "few_shot_prompt_template = FewShotPromptTemplate(\n",
    "    examples=examples,\n",
    "    example_prompt=example_prompt,\n",
    "    prefix=prefix,\n",
    "    suffix=suffix,\n",
    "    input_variables=[\"query\"],\n",
    "    example_separator=\"\\n\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Luanda.'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"Quel est la capitale de l'Angola?\"\n",
    "openai.invoke(few_shot_prompt_template.format(query=query))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM et Chat Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "I. Introduction \n",
      "    A. Présentation de la boxe\n",
      "    B. Origines et évolution de la boxe \n",
      "    C. Objectif de l'exposé\n",
      "\n",
      "II. Les règles de la boxe \n",
      "    A. Les catégories de poids \n",
      "    B. Les équipements nécessaires \n",
      "    C. Déroulement d'un combat de boxe \n",
      "\n",
      "III. Les différentes techniques de boxe \n",
      "    A. Le jab \n",
      "    B. Le crochet \n",
      "    C. L'uppercut \n",
      "    D. Les coups de pieds \n",
      "\n",
      "IV. Les bienfaits de la boxe \n",
      "    A. Pour la santé physique \n",
      "    B. Pour la santé mentale \n",
      "    C. La pédagogie et la discipline \n",
      "\n",
      "V. La boxe dans l'histoire \n",
      "    A. Les grands noms de la boxe \n",
      "    B. Les événements marquants \n",
      "    C. Les controverses autour de la boxe \n",
      "\n",
      "VI. La boxe professionnelle \n",
      "    A. Les différents titres \n",
      "    B. Le circuit professionnel \n",
      "    C. Les revenus des boxeurs \n",
      "\n",
      "VII. La boxe amateur \n",
      "    A. Les compétitions intern\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import OpenAI\n",
    "\n",
    "llm = OpenAI(\n",
    "    model_name=\"gpt-3.5-turbo-instruct\",\n",
    "    openai_api_key=OPENAI_API_KEY,\n",
    "    temperature=0.9\n",
    "    )\n",
    "text = \"Écris-moi un plan d'exposé sur la boxe\" #prompt\n",
    "print(llm.invoke(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chat Model et Chat message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "from langchain import PromptTemplate\n",
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    AIMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "from langchain.schema import (\n",
    "    AIMessage,\n",
    "    HumanMessage,\n",
    "    SystemMessage\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = ChatOpenAI(openai_api_key=OPENAI_API_KEY,temperature=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. Chat Messages\n",
    "\n",
    "* HumanMessage: Un message envoyé du point de vue de l'utilisateur\n",
    "* AIMessage: Message envoyé du point de vue du LLM avec lequel l'humain interagit.\n",
    "* SystemMessage: Message définissant les objectifs que l'IA doit suivre\n",
    "* ChatMessage: Message permettant de définir arbitrairement un rôle (rarement utilisé)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Il existe environ 100 000 espèces d'arachnides connues à ce jour.\")"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = [\n",
    "    SystemMessage(content=\"Tu es un assistant IA efficace donnant des réponses courtes à l'utilisateurs.\"),\n",
    "    HumanMessage(content=\"J'aime la montagne. Où devrais-je aller pour les vacances d'hiver?\"),\n",
    "    AIMessage(content=\"Les Alpes pour des vacances au ski semblent être une bonne idée?\"),\n",
    "    HumanMessage(content=\"J'aimerai connaitre le nombre d'espèces d'arachnides connues existant sur Terre.\")\n",
    "]\n",
    "chat.invoke(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Joke(setup=\"Pourquoi les poissons n'aiment pas jouer au football ?\", punchline=\"Parce qu'ils ont peur de prendre un carton jaune !\")"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field, validator\n",
    "from langchain_openai import OpenAI\n",
    "\n",
    "model = OpenAI(model_name=\"gpt-3.5-turbo-instruct\", temperature=0.1,api_key=OPENAI_API_KEY)\n",
    "\n",
    "\n",
    "# Define your desired data structure.\n",
    "class Joke(BaseModel):\n",
    "    setup: str = Field(description=\"qestion pour faire rire.\")\n",
    "    punchline: str = Field(description=\"réponse pour faire la blague.\")\n",
    "\n",
    "    # You can add custom validation logic easily with Pydantic.\n",
    "    @validator(\"setup\")\n",
    "    def question_ends_with_question_mark(cls, field):\n",
    "        if field[-1] != \"?\":\n",
    "            raise ValueError(\"Question mal formulé!\")\n",
    "        return field\n",
    "\n",
    "\n",
    "# Set up a parser + inject instructions into the prompt template.\n",
    "parser = PydanticOutputParser(pydantic_object=Joke)\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"Réponds à la requête de l'utilisateurs.\\n{format_instructions}\\n{query}\\n\",\n",
    "    input_variables=[\"query\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    ")\n",
    "\n",
    "# And a query intended to prompt a language model to populate the data structure.\n",
    "prompt_and_model = prompt | model\n",
    "output = prompt_and_model.invoke({\"query\": \"Raconte-moi une blague.\"})\n",
    "parser.invoke(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pydantic.v1.main.PydanticOutputParserInput"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
