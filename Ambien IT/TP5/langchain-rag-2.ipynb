{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip uninstall -y langchain\n",
    "!pip install langchain langchain-openai langchain-pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "OPENAI_API_KEY= \"YOUR_API_KEY\"\n",
    "PINECONE_API_KEY= \"YOUR_API_KEY\"\n",
    "os.environ['OPENAI_API_KEY']= OPENAI_API_KEY\n",
    "os.environ['PINECONE_API_KEY'] = PINECONE_API_KEY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieval Augmented Generation avec Langchain 2 \n",
    "### Retriver par utilisateurs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lorsque vous créez une application utilisant du retrieval, vous devez souvent la concevoir en pensant à plusieurs utilisateurs. Cela signifie que vous pouvez stocker des données non pas pour un seul utilisateur, mais pour de nombreux utilisateurs différents, et qu'ils ne doivent pas être en mesure de voir les données de chacun d'entre eux. Cela signifie que vous devez configurer votre chaîne de récupération pour ne récupérer que certaines informations. Cela implique généralement 3 étapes:\n",
    "\n",
    "##### 1. Assurez-vous que le système d'extraction que vous utilisez prend en charge plusieurs utilisateurs.\n",
    "\n",
    "Pour l'instant, il n'y a pas d'indicateur ou de filtre unifié pour cela dans LangChain. Au contraire, chaque vectorstore et retriver peut avoir le sien, et peut être appelé différemment (namespaces, multi-tenancy, etc.). Pour les vectorstores, ceci est généralement exposé comme un argument de mot-clé qui est passé lors de `similarity_search`. En lisant la documentation ou le code source, déterminez si le retriever que vous utilisez prend en charge plusieurs utilisateurs et, si c'est le cas, comment l'utiliser.\n",
    "\n",
    ">Note : ajouter de la documentation et/ou du support pour les utilisateurs multiples pour les extracteurs qui ne le supportent pas (ou ne le documentent pas) est une excellente façon de contribuer à LangChain.\n",
    "\n",
    "##### 2. Ajouter ce paramètre en tant que champ configurable pour la chaîne\n",
    "\n",
    "Cela vous permettra d'appeler facilement la chaîne et de configurer tous les drapeaux pertinents au moment de l'exécution. Voir [cette documentation](https://python.langchain.com/docs/expression_language/how_to/configure) pour plus d'informations sur la configuration.\n",
    "\n",
    "##### 3. Appeler la chaîne avec ce champ configurable\n",
    "\n",
    "Maintenant, au moment de l'exécution, vous pouvez appeler cette chaîne avec un champ configurable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exemple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voyons un exemple concret de ce que cela donne dans le code. Nous utiliserons Pinecone pour cet exemple.\n",
    "\n",
    "> Il faudra créerun vectorstore sur l'interface web de pinecone au préalable avec le nom d'index de votre choix\n",
    "\n",
    "Pour configurer Pinecone, définissez la variable d'environnement suivante :\n",
    "\n",
    "PINECONE_API_KEY : votre clé d'API Pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['d43f7b91-bcdd-4e5e-89e6-f16ab1ddcbc0']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "vectorstore = PineconeVectorStore(index_name=\"test-example\", embedding=embeddings)\n",
    "\n",
    "vectorstore.add_texts([\"j'ai travaillé chez meta\"], namespace=\"patrick\")\n",
    "vectorstore.add_texts([\"j'ai travaillé chez deepmind\"], namespace=\"dimitri\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ceci n'obtiendra des documents que pour Patrick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content=\"j'ai travaillé chez meta\")]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorstore.as_retriever(search_kwargs={\"namespace\": \"patrick\"}).get_relevant_documents(\n",
    "    \"Où travaillait-t-il?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ceci n'obtiendra des documents que pour Dimitri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content=\"j'ai travaillé chez deepmind\")]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorstore.as_retriever(search_kwargs={\"namespace\": \"dimitri\"}).get_relevant_documents(\n",
    "    \"Où travaillait-t-il?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous pouvons maintenant créer la chaîne que nous utiliserons pour répondre aux questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import (\n",
    "    ConfigurableField,\n",
    "    RunnableBinding,\n",
    "    RunnableLambda,\n",
    "    RunnablePassthrough,\n",
    ")\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "\n",
    "# Il s'agit d'une chaîne de questions-réponses de base.\n",
    "template = \"\"\"Réponds à la question en te basant uniquement sur le contexte suivant:\n",
    "{context}\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "model = ChatOpenAI()\n",
    "\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ici, nous indiquons que l'extracteur possède un champ configurable. Tous les récupérateurs de vectorstore ont un champ `search_kwargs`. Il s'agit simplement d'un dictionnaire, avec des champs spécifiques à vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "configurable_retriever = retriever.configurable_fields(\n",
    "    search_kwargs=ConfigurableField(\n",
    "        id=\"search_kwargs\",\n",
    "        name=\"Search Kwargs\",\n",
    "        description=\"The search kwargs to use\",\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous pouvons maintenant créer la chaîne à l'aide de notre retriever configurable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = (\n",
    "    {\"context\": configurable_retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous pouvons maintenant invoquer la chaîne avec des options configurables. `search_kwargs` est l'identifiant du champ configurable. La valeur est le kwargs de recherche à utiliser pour Pinecone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Cet utilisateur travaillait chez DeepMind.'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(\n",
    "    \"Où travaillait cet utilisateur?\",\n",
    "    config={\"configurable\": {\"search_kwargs\": {\"namespace\": \"dimitri\"}}},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"L'utilisateur travaillait chez Meta.\""
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(\n",
    "    \"Où travaillait cet utilisateur?\",\n",
    "    config={\"configurable\": {\"search_kwargs\": {\"namespace\": \"patrick\"}}},\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
