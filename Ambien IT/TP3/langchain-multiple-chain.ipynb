{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Chainage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les éléments exécutables peuvent facilement être utilisés pour enchaîner plusieurs chaînes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install –quiet langchain langchain-openai\n",
    "OPENAI_API_KEY= \"OPENAI_API_KEY\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['OPENAI_API_KEY']= OPENAI_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Ang lungsod ni Barack Obama ay galing sa Chicago, sa estado ng Illinois, sa Estados Unidos.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "prompt1 = ChatPromptTemplate.from_template(\"De quelle ville vient {person}?\")\n",
    "prompt2 = ChatPromptTemplate.from_template(\n",
    "    \"De quelle pays vient la ville {city}? Réponds en {language}\"\n",
    ")\n",
    "\n",
    "model = ChatOpenAI()\n",
    "\n",
    "chain1 = prompt1 | model | StrOutputParser()\n",
    "\n",
    "chain2 = (\n",
    "    {\"city\": chain1, \"language\": itemgetter(\"language\")}\n",
    "    | prompt2\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "chain2.invoke({\"person\": \"obama\", \"language\": \"tagalog\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "langchain_core.prompts.chat.ChatPromptTemplate"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(prompt1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "prompt1 = ChatPromptTemplate.from_template(\n",
    "    \"Génère une couleur {attribute}. Retourne le nom de la couleur et rien d'autre:\"\n",
    ")\n",
    "prompt2 = ChatPromptTemplate.from_template(\n",
    "    \"Quel est le fruit de la couleur: {color}. Retourne le nom du fruit et rien d'autre:\"\n",
    ")\n",
    "prompt3 = ChatPromptTemplate.from_template(\n",
    "    \"Quel est le pays dont le drapeau porte la couleur: {color}. Retourne le nom du pays et rien d'autre:\"\n",
    ")\n",
    "prompt4 = ChatPromptTemplate.from_template(\n",
    "    \"Quel est la couleur de {fruit} et du drapeau de {country}?\"\n",
    ")\n",
    "\n",
    "model_parser = model | StrOutputParser()\n",
    "\n",
    "color_generator = (\n",
    "    {\"attribute\": RunnablePassthrough()} | prompt1 | {\"color\": model_parser}\n",
    ")\n",
    "color_to_fruit = prompt2 | model_parser\n",
    "color_to_country = prompt3 | model_parser\n",
    "question_generator = (\n",
    "    color_generator | {\"fruit\": color_to_fruit, \"country\": color_to_country} | prompt4 \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'La couleur de la poire est vert et le drapeau de Bahreïn est rouge et blanc avec un triangle noir.'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question_generator.invoke(\"diplomatique\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Branchement et fusion de chaines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il se peut que vous souhaitiez que la sortie d'un composant soit traitée par deux autres composants ou plus. RunnableParallels vous permet de diviser ou de forker la chaîne afin que plusieurs composants puissent traiter l'entrée en parallèle. Par la suite, d'autres composants peuvent joindre ou fusionner les résultats pour synthétiser une réponse finale. Ce type de chaîne crée un graphe de calcul qui ressemble à ce qui suit:\n",
    "```     \n",
    "     Input\n",
    "      / \\\n",
    "     /   \\\n",
    " Branch1 Branch2\n",
    "     \\   /\n",
    "      \\ /\n",
    "    Combine\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "planner = (\n",
    "    ChatPromptTemplate.from_template(\"Génère-moi une dissertation sur: {input}\")\n",
    "    | ChatOpenAI()\n",
    "    | StrOutputParser()\n",
    "    | {\"base_response\": RunnablePassthrough()}\n",
    ")\n",
    "\n",
    "arguments_for = (\n",
    "    ChatPromptTemplate.from_template(\n",
    "        \"Liste les aspects positifs ou 'pros' sur {base_response}\"\n",
    "    )\n",
    "    | ChatOpenAI()\n",
    "    | StrOutputParser()\n",
    ")\n",
    "arguments_against = (\n",
    "    ChatPromptTemplate.from_template(\n",
    "        \"Liste les aspects négatifs ou 'cons' {base_response}\"\n",
    "    )\n",
    "    | ChatOpenAI()\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "final_responder = (\n",
    "    ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"ai\", \"{original_response}\"),\n",
    "            (\"human\", \"Pros:\\n{results_1}\\n\\nCons:\\n{results_2}\"),\n",
    "            (\"system\", \"Génère une conclusion en fonction de la critique reçu\"),\n",
    "        ]\n",
    "    )\n",
    "    | ChatOpenAI()\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "chain = (\n",
    "    planner\n",
    "    | {\n",
    "        \"results_1\": arguments_for,\n",
    "        \"results_2\": arguments_against,\n",
    "        \"original_response\": itemgetter(\"base_response\"),\n",
    "    }\n",
    "    | final_responder\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "En conclusion, le déploiement de l'intelligence artificielle (IA) dans les systèmes d'information (SI) présente à la fois des avantages significatifs et des défis importants à relever.\n",
      " Les aspects positifs tels que l'optimisation des processus métier, l'amélioration de la productivité, la personnalisation de l'expérience client, la prévention des risques et l'adaptabilité aux changements du marché offrent des opportunités considérables pour les entreprises.\n",
      "\n",
      "\n",
      "Cependant, il est crucial de ne pas sous-estimer les conséquences des défis identifiés, notamment en ce qui concerne la sécurité des données sensibles, la formation des collaborateurs, le contrôle de l'IA, les coûts impliqués, l'impact sur l'emploi, les risques de biais et de discrimination, la complexité de l'intégration et la conformité aux réglementations.\n",
      "\n",
      "\n",
      "En somme, il est primordial pour les entreprises d'adopter une approche équilibrée, en maximisant les avantages de l'IA tout en gérant efficacement les risques et enjeux associés.\n",
      " Une planification minutieuse, une communication transparente et une attention particulière portée à l'éthique et à la conformité réglementaire sont essentielles pour assurer le succès du déploiement de l'IA dans les SI.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "texte = chain.invoke({\"input\": \"Le déploiement de l'IA dans les SI\"})\n",
    "print(texte.replace(\".\", \".\\n\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> En conclusion, le déploiement de l'intelligence artificielle dans les systèmes d'information offre de nombreux avantages en termes d'automatisation des tâches, d'analyse des données en temps réel, de prédiction des tendances et d'amélioration de la performance des SI. Cependant, il est crucial de prendre en compte les risques liés à la protection de la vie privée, à la sécurité informatique et aux enjeux éthiques. Il est donc impératif de mettre en place des politiques de gouvernance des données, de former les utilisateurs, de sensibiliser les entreprises aux enjeux éthiques et de promouvoir une approche responsable de l'IA pour garantir que ses bénéfices profitent à l'ensemble de la société.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parallelisme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'joke': AIMessage(content=\"Pourquoi les montagnes sont-elles si drôles ? Parce qu'elles ont toujours une crête de quoi rire !\"),\n",
       " 'poem': AIMessage(content='La montagne majestueuse, couronnée de neige éternelle, garde les secrets du monde dans ses crevasses profondes.')}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnableParallel\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "model = ChatOpenAI()\n",
    "joke_chain = ChatPromptTemplate.from_template(\"écris-moi une blague sur {topic}\") | model\n",
    "poem_chain = (\n",
    "    ChatPromptTemplate.from_template(\"écrit un poème de deux lignes sur {topic}\") | model\n",
    ")\n",
    "\n",
    "map_chain = RunnableParallel(joke=joke_chain, poem=poem_chain)\n",
    "\n",
    "map_chain.invoke({\"topic\": \"la montagne\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
