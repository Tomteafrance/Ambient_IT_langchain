{"cells":[{"cell_type":"markdown","metadata":{"id":"Vutn2dTU74gA"},"source":["# Cours LangChain TP5"]},{"cell_type":"markdown","metadata":{"id":"E1dOAsGN74gH"},"source":["## Use case\n","\n","Ce TP est inspiré du cas d'utilisation de compréhension du [code émis dans la bibliothèque LangChain](https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/use_cases/code_understanding.ipynb). Il vous permettra d'utiliser LangChain pour analyser du code et en générer\n","\n","L'analyse du code source est l'une des applications LLM les plus populaires (par exemple, GitHub Copilot, Code Interpreter, Codium et Codeium) pour des cas d'utilisation tels que :\n","\n","* Q&R sur la base de code pour comprendre comment elle fonctionne\n","* Utilisation des LLM pour suggérer des refactors ou des améliorations\n","* Utilisation des LLM pour documenter le code\n","\n","![Image description](https://raw.githubusercontent.com/langchain-ai/langchain/master/docs/static/img/code_understanding.png)\n","\n","\n","## Vue d'ensemble\n","\n","Le pipeline pour l'assurance qualité sur le code suit les étapes que nous suivons pour répondre aux questions sur les documents, avec quelques différences :\n","\n","En particulier, nous pouvons employer une stratégie de splitting qui fait plusieurs choses :\n","\n","* Chaque fonction et classe de haut niveau du code est chargée dans des documents distincts.\n","* Le code source est chargé dans des documents distincts.\n","* Conserve les métadonnées sur l'origine de chaque fractionnement.\n","\n","## QuickStart"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"03_lV6q274gI"},"outputs":[],"source":["#!pip install --upgrade --quiet  langchain-openai tiktoken chromadb langchain gitpython\n","# Set env var OPENAI_API_KEY or load from a .env file\n","\n","import os\n","os.environ['OPENAI_API_KEY']=\"OPENAI_API_KEY\" # À Modifier"]},{"cell_type":"markdown","metadata":{"id":"A1k7qEOx74gK"},"source":["Nous suivrons la structure de [ce notebook](https://github.com/cristobalcl/LearningLangChain/blob/master/notebooks/04%20-%20QA%20with%20code.ipynb) et utiliserons le [context aware code splitting](https://python.langchain.com/docs/integrations/document_loaders/source_code)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JDCHI7-w74gK"},"outputs":[],"source":["from git import Repo\n","from langchain_community.document_loaders.generic import GenericLoader\n","from langchain_community.document_loaders.parsers import LanguageParser\n","from langchain_text_splitters import Language\n","\n","# Clone\n","repo_path = \"test_repo/\"\n","if not(os.path.isdir(repo_path)):\n","    repo = Repo.clone_from(\"https://github.com/langchain-ai/langchain\", to_path=repo_path)"]},{"cell_type":"markdown","metadata":{"id":"ndh0ZjTl74gK"},"source":["On charge le code Python en utilisant [`LanguageParser`](https://python.langchain.com/docs/integrations/document_loaders/source_code), qui va:\n","\n","* Conserver les fonctions et les classes de haut niveau ensemble (dans un seul document)\n","* Mettre le reste du code dans un document séparé\n","* Conserve les métadonnées sur l'origine de chaque split"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rAcXJKOv74gL","outputId":"1d0de8b1-6eaf-42c0-ecb2-535463af7547"},"outputs":[{"data":{"text/plain":["1562"]},"execution_count":26,"metadata":{},"output_type":"execute_result"}],"source":["# Load\n","loader = GenericLoader.from_filesystem(\n","    repo_path + \"/libs/langchain/langchain\",\n","    glob=\"**/*\",\n","    suffixes=[\".py\"],\n","    exclude=[\"**/non-utf8-encoding.py\"],\n","    parser=LanguageParser(language=Language.PYTHON, parser_threshold=500),\n",")\n","documents = loader.load()\n","len(documents)"]},{"cell_type":"markdown","metadata":{"id":"BrqWhq9M74gM"},"source":["### Splitting\n","\n","On fractionnne le `Document` en morceaux (chunks) pour les tranformer en embedding et les stocker en vectorDB\n","\n","Nous pouvons utiliser `RecursiveCharacterTextSplitter` avec `language` spécifié."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BhjNWEPo74gM","outputId":"16b1d86b-2bb8-498f-f45e-59dcd0770bd3"},"outputs":[{"data":{"text/plain":["3151"]},"execution_count":27,"metadata":{},"output_type":"execute_result"}],"source":["from langchain_text_splitters import RecursiveCharacterTextSplitter\n","\n","python_splitter = RecursiveCharacterTextSplitter.from_language(\n","    language=Language.PYTHON, chunk_size=1200, chunk_overlap=100\n",")\n","texts = python_splitter.split_documents(documents)\n","len(texts)"]},{"cell_type":"markdown","metadata":{"id":"SFikelfp74gM"},"source":["### RetrievalQ&R\n","\n","Nous devons stocker les documents de manière à pouvoir effectuer des recherches sémantiques sur leur contenu.\n","\n","L'approche la plus courante consiste à intégrer le contenu de chaque document, puis à stocker l'intégration et le document dans un magasin vectoriel.\n","\n","Lors de la configuration de l'extracteur vectoriel :\n","\n","* Nous testons [max marginal relevance](/docs/use_cases/question_answering) pour la recherche.\n","* Et 8 documents sont retournés"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2IxkswEM74gN"},"outputs":[],"source":["from langchain_community.vectorstores import Chroma\n","from langchain_openai import OpenAIEmbeddings\n","\n","db = Chroma.from_documents(texts, OpenAIEmbeddings(model=\"text-embedding-3-small\",disallowed_special=()))\n","retriever = db.as_retriever(\n","    search_type=\"mmr\",  # Also test \"similarity\"\n","    search_kwargs={\"k\": 8},\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nANyCkDg74gN","outputId":"a146bff1-0404-434b-aa52-ded6bfc95cbc"},"outputs":[{"data":{"text/plain":["[Document(page_content='# Code for: def _import_openllm() -> Any:\\n\\n\\n# Code for: def _import_openlm() -> Any:\\n\\n\\n# Code for: def _import_pai_eas_endpoint() -> Any:\\n\\n\\n# Code for: def _import_petals() -> Any:\\n\\n\\n# Code for: def _import_pipelineai() -> Any:\\n\\n\\n# Code for: def _import_predibase() -> Any:\\n\\n\\n# Code for: def _import_predictionguard() -> Any:\\n\\n\\n# Code for: def _import_promptlayer() -> Any:\\n\\n\\n# Code for: def _import_promptlayer_chat() -> Any:\\n\\n\\n# Code for: def _import_replicate() -> Any:\\n\\n\\n# Code for: def _import_rwkv() -> Any:\\n\\n\\n# Code for: def _import_sagemaker_endpoint() -> Any:\\n\\n\\n# Code for: def _import_self_hosted() -> Any:\\n\\n\\n# Code for: def _import_self_hosted_hugging_face() -> Any:\\n\\n\\n# Code for: def _import_stochasticai() -> Any:\\n\\n\\n# Code for: def _import_symblai_nebula() -> Any:\\n\\n\\n# Code for: def _import_textgen() -> Any:\\n\\n\\n# Code for: def _import_titan_takeoff() -> Any:\\n\\n\\n# Code for: def _import_titan_takeoff_pro() -> Any:\\n\\n\\n# Code for: def _import_together() -> Any:\\n\\n\\n# Code for: def _import_tongyi() -> Any:\\n\\n\\n# Code for: def _import_vertex() -> Any:\\n\\n\\n# Code for: def _import_vertex_model_garden() -> Any:\\n\\n\\n# Code for: def _import_vllm() -> Any:', metadata={'content_type': 'simplified_code', 'language': 'python', 'source': 'test_repo/libs/langchain/langchain/llms/__init__.py'}),\n"," Document(page_content='# TODO: this is in here to maintain backwards compatibility', metadata={'language': 'python', 'source': 'test_repo/libs/langchain/langchain/embeddings/__init__.py'}),\n"," Document(page_content='\"\"\"Adapted from https://github.com/jzbjyb/FLARE\"\"\"', metadata={'language': 'python', 'source': 'test_repo/libs/langchain/langchain/chains/flare/__init__.py'}),\n"," Document(page_content='%import common.CNAME\\n    %import common.ESCAPED_STRING\\n    %import common.SIGNED_FLOAT\\n    %import common.SIGNED_INT\\n    %import common.WS\\n    %ignore WS\\n\"\"\"', metadata={'language': 'python', 'source': 'test_repo/libs/langchain/langchain/chains/query_constructor/parser.py'})]"]},"execution_count":16,"metadata":{},"output_type":"execute_result"}],"source":["db.search(\"import\",search_type=\"mmr\")"]},{"cell_type":"markdown","metadata":{"id":"gVbJKGRc74gN"},"source":["### Chat"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vgR-kUCl74gN"},"outputs":[],"source":["from langchain.chains import ConversationalRetrievalChain\n","from langchain.memory import ConversationSummaryMemory\n","from langchain_openai import ChatOpenAI\n","\n","llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\")\n","memory = ConversationSummaryMemory(\n","    llm=llm, memory_key=\"chat_history\", return_messages=True\n",")\n","qr = ConversationalRetrievalChain.from_llm(llm, retriever=retriever, memory=memory)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TROwtr4E74gO","outputId":"a45e1608-c8a1-4795-bd9c-c340fa96bf06"},"outputs":[{"data":{"text/plain":["'Pour initialiser un Agent Python avec LangChain, vous pouvez suivre les étapes suivantes :\\n\\n1. Importez les outils nécessaires :\\n\\n    ```python\\n    from langchain_core.agents import AgentExecutor, create_json_chat_agent\\n    from langchain_community.chat_models import ChatOpenAI\\n    from langchain import hub\\n    ```\\n\\n2. Récupérez le modèle de prompt à partir du hub de LangChain :\\n\\n    ```python\\n    prompt = hub.pull(\"nom_du_modele\")\\n    ```\\n\\n3. Créez une instance du modèle que vous souhaitez utiliser pour l\\'Agent :\\n\\n    ```python\\n    model = ChatOpenAI()\\n    ```\\n\\n4. Créez l\\'Agent en utilisant le modèle, les outils et le prompt :\\n\\n    ```python\\n    agent = create_json_chat_agent(model, tools, prompt)\\n    ```\\n\\n5. Créez un exécuteur d\\'Agent et invoquez l\\'Agent avec une entrée :\\n\\n    ```python\\n    agent_executor = AgentExecutor(agent=agent, tools=tools)\\n    agent_executor.invoke({\"input\": \"votre_message_ici\"})\\n    ```\\n\\nAssurez-vous d\\'adapter les noms des modèles, des outils et des prompts à votre cas d\\'utilisation spécifique.'"]},"execution_count":19,"metadata":{},"output_type":"execute_result"}],"source":["question = \"Comment initialiser un Agent Python avec LangChain?\"\n","result = qr.invoke(question)\n","result[\"answer\"]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4genoWR174gO"},"outputs":[],"source":["#qr.memory.clear()"]},{"cell_type":"markdown","metadata":{"id":"itMv3hTa74gO"},"source":[]}],"metadata":{"kernelspec":{"display_name":"base","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.10"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}