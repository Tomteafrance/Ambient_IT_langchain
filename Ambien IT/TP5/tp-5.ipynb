{"cells":[{"cell_type":"markdown","metadata":{"id":"qtraIfmD_PU_"},"source":["# Cours LangChain TP5"]},{"cell_type":"markdown","metadata":{"id":"5f7WLa34_PVA"},"source":["## Use case\n","\n","Ce TP est inspiré du cas d'utilisation de compréhension du [code émis dans la bibliothèque LangChain](https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/use_cases/code_understanding.ipynb). Il vous permettra d'utiliser LangChain pour analyser du code et en générer\n","\n","L'analyse du code source est l'une des applications LLM les plus populaires (par exemple, GitHub Copilot, Code Interpreter, Codium et Codeium) pour des cas d'utilisation tels que :\n","\n","* Q&R sur la base de code pour comprendre comment elle fonctionne\n","* Utilisation des LLM pour suggérer des refactors ou des améliorations\n","* Utilisation des LLM pour documenter le code\n","\n","![Image description](https://raw.githubusercontent.com/langchain-ai/langchain/master/docs/static/img/code_understanding.png)\n","\n","\n","## Vue d'ensemble\n","\n","Le pipeline pour l'assurance qualité sur le code suit les étapes que nous suivons pour répondre aux questions sur les documents, avec quelques différences :\n","\n","En particulier, nous pouvons employer une stratégie de splitting qui fait plusieurs choses :\n","\n","* Chaque fonction et classe de haut niveau du code est chargée dans des documents distincts.\n","* Le code source est chargé dans des documents distincts.\n","* Conserve les métadonnées sur l'origine de chaque fractionnement.\n","\n","## QuickStart"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SkTcSL4I_PVB","executionInfo":{"status":"ok","timestamp":1710252742773,"user_tz":-60,"elapsed":37950,"user":{"displayName":"Tom TEA","userId":"00599091751588244729"}},"outputId":"064323ad-609f-4db5-9a44-5778ce58794f"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.8 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.9/1.8 MB\u001b[0m \u001b[31m26.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m31.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m23.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m525.5/525.5 kB\u001b[0m \u001b[31m28.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m807.5/807.5 kB\u001b[0m \u001b[31m34.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m195.4/195.4 kB\u001b[0m \u001b[31m22.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m256.9/256.9 kB\u001b[0m \u001b[31m26.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.4/227.4 kB\u001b[0m \u001b[31m21.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m44.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.1/92.1 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.3/41.3 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m31.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m48.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.4/58.4 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.7/105.7 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m698.9/698.9 kB\u001b[0m \u001b[31m46.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m70.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.6/67.6 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.5/138.5 kB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m78.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.6/66.6 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.5/71.5 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.8/50.8 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.4/341.4 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m88.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m59.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.8/77.8 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"]}],"source":["!pip install --upgrade --quiet  langchain-openai tiktoken chromadb langchain gitpython\n","# Set env var OPENAI_API_KEY or load from a .env file\n","\n","import os\n","os.environ['OPENAI_API_KEY'] = \"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\" # À Modifier"]},{"cell_type":"markdown","metadata":{"id":"GtCaohTe_PVB"},"source":["Nous suivrons la structure de [ce notebook](https://github.com/cristobalcl/LearningLangChain/blob/master/notebooks/04%20-%20QA%20with%20code.ipynb) et utiliserons le [context aware code splitting](https://python.langchain.com/docs/integrations/document_loaders/source_code)."]},{"cell_type":"code","execution_count":3,"metadata":{"id":"n8S-VXIo_PVC","executionInfo":{"status":"ok","timestamp":1710252793681,"user_tz":-60,"elapsed":23294,"user":{"displayName":"Tom TEA","userId":"00599091751588244729"}}},"outputs":[],"source":["from git import Repo\n","from langchain_community.document_loaders.generic import GenericLoader\n","from langchain_community.document_loaders.parsers import LanguageParser\n","from langchain_text_splitters import Language\n","\n","# Clone\n","repo_path = \"test_repo/\"\n","repo = Repo.clone_from(\"https://github.com/langchain-ai/langchain\", to_path=repo_path)"]},{"cell_type":"markdown","metadata":{"id":"izNedcqZ_PVC"},"source":["On charge le code Python en utilisant [`LanguageParser`](https://python.langchain.com/docs/integrations/document_loaders/source_code), qui va:\n","\n","* Conserver les fonctions et les classes de haut niveau ensemble (dans un seul document)\n","* Mettre le reste du code dans un document séparé\n","* Conserve les métadonnées sur l'origine de chaque split"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oWYGdVOh_PVC","executionInfo":{"status":"ok","timestamp":1710252808241,"user_tz":-60,"elapsed":1282,"user":{"displayName":"Tom TEA","userId":"00599091751588244729"}},"outputId":"e8479e1d-130f-4034-e340-770b7d2a1912"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["1562"]},"metadata":{},"execution_count":4}],"source":["# Load\n","loader = GenericLoader.from_filesystem(\n","    repo_path + \"/libs/langchain/langchain\",\n","    glob=\"**/*\",\n","    suffixes=[\".py\"],\n","    exclude=[\"**/non-utf8-encoding.py\"],\n","    parser=LanguageParser(language=Language.PYTHON, parser_threshold=500),\n",")\n","documents = loader.load()\n","len(documents)"]},{"cell_type":"code","source":["type(documents[1])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":121},"id":"fe2LAf_jNj5R","executionInfo":{"status":"ok","timestamp":1710256231890,"user_tz":-60,"elapsed":502,"user":{"displayName":"Tom TEA","userId":"00599091751588244729"}},"outputId":"e5b17567-4644-4c5c-ab0a-624243521a05"},"execution_count":33,"outputs":[{"output_type":"execute_result","data":{"text/plain":["langchain_core.documents.base.Document"],"text/html":["<div style=\"max-width:800px; border: 1px solid var(--colab-border-color);\"><style>\n","      pre.function-repr-contents {\n","        overflow-x: auto;\n","        padding: 8px 12px;\n","        max-height: 500px;\n","      }\n","\n","      pre.function-repr-contents.function-repr-contents-collapsed {\n","        cursor: pointer;\n","        max-height: 100px;\n","      }\n","    </style>\n","    <pre style=\"white-space: initial; background:\n","         var(--colab-secondary-surface-color); padding: 8px 12px;\n","         border-bottom: 1px solid var(--colab-border-color);\"><b>langchain_core.documents.base.Document</b><br/>def __init__(page_content: str, **kwargs: Any) -&gt; None</pre><pre class=\"function-repr-contents function-repr-contents-collapsed\" style=\"\"><a class=\"filepath\" style=\"display:none\" href=\"#\">/usr/local/lib/python3.10/dist-packages/langchain_core/documents/base.py</a>Class for storing a piece of text and associated metadata.</pre>\n","      <script>\n","      if (google.colab.kernel.accessAllowed && google.colab.files && google.colab.files.view) {\n","        for (const element of document.querySelectorAll('.filepath')) {\n","          element.style.display = 'block'\n","          element.onclick = (event) => {\n","            event.preventDefault();\n","            event.stopPropagation();\n","            google.colab.files.view(element.textContent, 9);\n","          };\n","        }\n","      }\n","      for (const element of document.querySelectorAll('.function-repr-contents')) {\n","        element.onclick = (event) => {\n","          event.preventDefault();\n","          event.stopPropagation();\n","          element.classList.toggle('function-repr-contents-collapsed');\n","        };\n","      }\n","      </script>\n","      </div>"]},"metadata":{},"execution_count":33}]},{"cell_type":"markdown","metadata":{"id":"9HzrYCFF_PVC"},"source":["### Splitting\n","\n","On fractionnne le `Document` en morceaux (chunks) pour les tranformer en embedding et les stocker en vectorDB\n","\n","Nous pouvons utiliser `RecursiveCharacterTextSplitter` avec `language` spécifié."]},{"cell_type":"markdown","metadata":{"id":"tJF-8mU3_PVC"},"source":["**Exercice**\n","\n","Utiliser `RecursiveCharacterTextSplitter` pour fractionner le document contenant le repo github de langchain"]},{"cell_type":"code","execution_count":20,"metadata":{"id":"WSsvPLX9_PVC","executionInfo":{"status":"ok","timestamp":1710253353520,"user_tz":-60,"elapsed":453,"user":{"displayName":"Tom TEA","userId":"00599091751588244729"}}},"outputs":[],"source":["from langchain_text_splitters import RecursiveCharacterTextSplitter\n","\n","python_splitter = RecursiveCharacterTextSplitter.from_language(language=Language.PYTHON, chunk_size=1000, chunk_overlap=100)\n","texts = python_splitter.split_documents(documents)"]},{"cell_type":"code","source":["print(len(texts))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vTKXZiYYGvzu","executionInfo":{"status":"ok","timestamp":1710254397744,"user_tz":-60,"elapsed":4,"user":{"displayName":"Tom TEA","userId":"00599091751588244729"}},"outputId":"535efb7a-051c-4d6b-83ca-0ba05b3d654c"},"execution_count":29,"outputs":[{"output_type":"stream","name":"stdout","text":["3607\n"]}]},{"cell_type":"markdown","metadata":{"id":"r7Oox2Nd_PVD"},"source":["### RetrievalQ&R\n","\n","Nous devons stocker les documents de manière à pouvoir effectuer des recherches sémantiques sur leur contenu.\n","\n","L'approche la plus courante consiste à intégrer le contenu de chaque document, puis à stocker l'intégration et le document dans un magasin vectoriel.\n","\n","Lors de la configuration de l'extracteur vectoriel :\n","\n","* Nous testons [max marginal relevance](/docs/use_cases/question_answering) pour la recherche.\n","* Et 8 documents sont retournés"]},{"cell_type":"code","execution_count":21,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bMm3-aoA_PVD","executionInfo":{"status":"ok","timestamp":1710253356453,"user_tz":-60,"elapsed":32,"user":{"displayName":"Tom TEA","userId":"00599091751588244729"}},"outputId":"83510794-3fac-4861-a004-d30696c9c8dd"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["[Document(page_content='\"\"\"For backwards compatibility.\"\"\"\\nfrom langchain_community.utilities.python import PythonREPL\\n\\n__all__ = [\"PythonREPL\"]', metadata={'source': 'test_repo/libs/langchain/langchain/python.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='\"\"\"Deprecated module for BaseLanguageModel class, kept for backwards compatibility.\"\"\"\\nfrom __future__ import annotations\\n\\nfrom langchain_core.language_models import BaseLanguageModel\\n\\n__all__ = [\"BaseLanguageModel\"]', metadata={'source': 'test_repo/libs/langchain/langchain/base_language.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='\"\"\"DEPRECATED: Kept for backwards compatibility.\"\"\"\\nfrom langchain_core.utils.formatting import StrictFormatter, formatter\\n\\n__all__ = [\"StrictFormatter\", \"formatter\"]', metadata={'source': 'test_repo/libs/langchain/langchain/formatting.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='# ruff: noqa: E402\\n\"\"\"Main entrypoint into package.\"\"\"\\nimport warnings\\nfrom importlib import metadata\\nfrom typing import Any, Optional\\n\\nfrom langchain_core._api.deprecation import surface_langchain_deprecation_warnings\\n\\ntry:\\n    __version__ = metadata.version(__package__)\\nexcept metadata.PackageNotFoundError:\\n    # Case where package metadata is not available.\\n    __version__ = \"\"\\ndel metadata  # optional, avoids polluting the results of dir(__package__)', metadata={'source': 'test_repo/libs/langchain/langchain/__init__.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='def _warn_on_import(name: str, replacement: Optional[str] = None) -> None:\\n    \"\"\"Warn on import of deprecated module.\"\"\"\\n    from langchain.utils.interactive_env import is_interactive_env\\n\\n    if is_interactive_env():\\n        # No warnings for interactive environments.\\n        # This is done to avoid polluting the output of interactive environments\\n        # where users rely on auto-complete and may trigger this warning\\n        # even if they are not using any deprecated modules\\n        return\\n\\n    if replacement:\\n        warnings.warn(\\n            f\"Importing {name} from langchain root module is no longer supported. \"\\n            f\"Please use {replacement} instead.\"\\n        )\\n    else:\\n        warnings.warn(\\n            f\"Importing {name} from langchain root module is no longer supported.\"\\n        )\\n\\n\\n# Surfaces Deprecation and Pending Deprecation warnings from langchain.\\nsurface_langchain_deprecation_warnings()', metadata={'source': 'test_repo/libs/langchain/langchain/__init__.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='def __getattr__(name: str) -> Any:\\n    if name == \"MRKLChain\":\\n        from langchain.agents import MRKLChain\\n\\n        _warn_on_import(name, replacement=\"langchain.agents.MRKLChain\")\\n\\n        return MRKLChain\\n    elif name == \"ReActChain\":\\n        from langchain.agents import ReActChain\\n\\n        _warn_on_import(name, replacement=\"langchain.agents.ReActChain\")\\n\\n        return ReActChain\\n    elif name == \"SelfAskWithSearchChain\":\\n        from langchain.agents import SelfAskWithSearchChain\\n\\n        _warn_on_import(name, replacement=\"langchain.agents.SelfAskWithSearchChain\")\\n\\n        return SelfAskWithSearchChain\\n    elif name == \"ConversationChain\":\\n        from langchain.chains import ConversationChain\\n\\n        _warn_on_import(name, replacement=\"langchain.chains.ConversationChain\")', metadata={'source': 'test_repo/libs/langchain/langchain/__init__.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='_warn_on_import(name, replacement=\"langchain.chains.ConversationChain\")\\n\\n        return ConversationChain\\n    elif name == \"LLMBashChain\":\\n        raise ImportError(\\n            \"This module has been moved to langchain-experimental. \"\\n            \"For more details: \"\\n            \"https://github.com/langchain-ai/langchain/discussions/11352.\"\\n            \"To access this code, install it with `pip install langchain-experimental`.\"\\n            \"`from langchain_experimental.llm_bash.base \"\\n            \"import LLMBashChain`\"\\n        )\\n\\n    elif name == \"LLMChain\":\\n        from langchain.chains import LLMChain\\n\\n        _warn_on_import(name, replacement=\"langchain.chains.LLMChain\")\\n\\n        return LLMChain\\n    elif name == \"LLMCheckerChain\":\\n        from langchain.chains import LLMCheckerChain\\n\\n        _warn_on_import(name, replacement=\"langchain.chains.LLMCheckerChain\")\\n\\n        return LLMCheckerChain\\n    elif name == \"LLMMathChain\":\\n        from langchain.chains import LLMMathChain', metadata={'source': 'test_repo/libs/langchain/langchain/__init__.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='_warn_on_import(name, replacement=\"langchain.chains.LLMMathChain\")\\n\\n        return LLMMathChain\\n    elif name == \"QAWithSourcesChain\":\\n        from langchain.chains import QAWithSourcesChain\\n\\n        _warn_on_import(name, replacement=\"langchain.chains.QAWithSourcesChain\")\\n\\n        return QAWithSourcesChain\\n    elif name == \"VectorDBQA\":\\n        from langchain.chains import VectorDBQA\\n\\n        _warn_on_import(name, replacement=\"langchain.chains.VectorDBQA\")\\n\\n        return VectorDBQA\\n    elif name == \"VectorDBQAWithSourcesChain\":\\n        from langchain.chains import VectorDBQAWithSourcesChain\\n\\n        _warn_on_import(name, replacement=\"langchain.chains.VectorDBQAWithSourcesChain\")\\n\\n        return VectorDBQAWithSourcesChain\\n    elif name == \"InMemoryDocstore\":\\n        from langchain.docstore import InMemoryDocstore\\n\\n        _warn_on_import(name, replacement=\"langchain.docstore.InMemoryDocstore\")', metadata={'source': 'test_repo/libs/langchain/langchain/__init__.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='_warn_on_import(name, replacement=\"langchain.docstore.InMemoryDocstore\")\\n\\n        return InMemoryDocstore\\n    elif name == \"Wikipedia\":\\n        from langchain.docstore import Wikipedia\\n\\n        _warn_on_import(name, replacement=\"langchain.docstore.Wikipedia\")\\n\\n        return Wikipedia\\n    elif name == \"Anthropic\":\\n        from langchain_community.llms import Anthropic\\n\\n        _warn_on_import(name, replacement=\"langchain_community.llms.Anthropic\")\\n\\n        return Anthropic\\n    elif name == \"Banana\":\\n        from langchain_community.llms import Banana\\n\\n        _warn_on_import(name, replacement=\"langchain_community.llms.Banana\")\\n\\n        return Banana\\n    elif name == \"CerebriumAI\":\\n        from langchain_community.llms import CerebriumAI\\n\\n        _warn_on_import(name, replacement=\"langchain_community.llms.CerebriumAI\")\\n\\n        return CerebriumAI\\n    elif name == \"Cohere\":\\n        from langchain_community.llms import Cohere', metadata={'source': 'test_repo/libs/langchain/langchain/__init__.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='_warn_on_import(name, replacement=\"langchain_community.llms.Cohere\")\\n\\n        return Cohere\\n    elif name == \"ForefrontAI\":\\n        from langchain_community.llms import ForefrontAI\\n\\n        _warn_on_import(name, replacement=\"langchain_community.llms.ForefrontAI\")\\n\\n        return ForefrontAI\\n    elif name == \"GooseAI\":\\n        from langchain_community.llms import GooseAI\\n\\n        _warn_on_import(name, replacement=\"langchain_community.llms.GooseAI\")\\n\\n        return GooseAI\\n    elif name == \"HuggingFaceHub\":\\n        from langchain_community.llms import HuggingFaceHub\\n\\n        _warn_on_import(name, replacement=\"langchain_community.llms.HuggingFaceHub\")\\n\\n        return HuggingFaceHub\\n    elif name == \"HuggingFaceTextGenInference\":\\n        from langchain_community.llms import HuggingFaceTextGenInference\\n\\n        _warn_on_import(\\n            name, replacement=\"langchain_community.llms.HuggingFaceTextGenInference\"\\n        )', metadata={'source': 'test_repo/libs/langchain/langchain/__init__.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='return HuggingFaceTextGenInference\\n    elif name == \"LlamaCpp\":\\n        from langchain_community.llms import LlamaCpp\\n\\n        _warn_on_import(name, replacement=\"langchain_community.llms.LlamaCpp\")\\n\\n        return LlamaCpp\\n    elif name == \"Modal\":\\n        from langchain_community.llms import Modal\\n\\n        _warn_on_import(name, replacement=\"langchain_community.llms.Modal\")\\n\\n        return Modal\\n    elif name == \"OpenAI\":\\n        from langchain_community.llms import OpenAI\\n\\n        _warn_on_import(name, replacement=\"langchain_community.llms.OpenAI\")\\n\\n        return OpenAI\\n    elif name == \"Petals\":\\n        from langchain_community.llms import Petals\\n\\n        _warn_on_import(name, replacement=\"langchain_community.llms.Petals\")\\n\\n        return Petals\\n    elif name == \"PipelineAI\":\\n        from langchain_community.llms import PipelineAI\\n\\n        _warn_on_import(name, replacement=\"langchain_community.llms.PipelineAI\")', metadata={'source': 'test_repo/libs/langchain/langchain/__init__.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='_warn_on_import(name, replacement=\"langchain_community.llms.PipelineAI\")\\n\\n        return PipelineAI\\n    elif name == \"SagemakerEndpoint\":\\n        from langchain_community.llms import SagemakerEndpoint\\n\\n        _warn_on_import(name, replacement=\"langchain_community.llms.SagemakerEndpoint\")\\n\\n        return SagemakerEndpoint\\n    elif name == \"StochasticAI\":\\n        from langchain_community.llms import StochasticAI\\n\\n        _warn_on_import(name, replacement=\"langchain_community.llms.StochasticAI\")\\n\\n        return StochasticAI\\n    elif name == \"Writer\":\\n        from langchain_community.llms import Writer\\n\\n        _warn_on_import(name, replacement=\"langchain_community.llms.Writer\")\\n\\n        return Writer\\n    elif name == \"HuggingFacePipeline\":\\n        from langchain_community.llms.huggingface_pipeline import HuggingFacePipeline\\n\\n        _warn_on_import(\\n            name,\\n            replacement=\"langchain_community.llms.huggingface_pipeline.HuggingFacePipeline\",\\n        )', metadata={'source': 'test_repo/libs/langchain/langchain/__init__.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='return HuggingFacePipeline\\n    elif name == \"FewShotPromptTemplate\":\\n        from langchain_core.prompts import FewShotPromptTemplate\\n\\n        _warn_on_import(name, replacement=\"langchain.prompts.FewShotPromptTemplate\")\\n\\n        return FewShotPromptTemplate\\n    elif name == \"Prompt\":\\n        from langchain.prompts import Prompt\\n\\n        _warn_on_import(name, replacement=\"langchain.prompts.Prompt\")\\n\\n        return Prompt\\n    elif name == \"PromptTemplate\":\\n        from langchain_core.prompts import PromptTemplate\\n\\n        _warn_on_import(name, replacement=\"langchain.prompts.PromptTemplate\")\\n\\n        return PromptTemplate\\n    elif name == \"BasePromptTemplate\":\\n        from langchain_core.prompts import BasePromptTemplate\\n\\n        _warn_on_import(\\n            name, replacement=\"langchain.schema.prompt_template.BasePromptTemplate\"\\n        )\\n\\n        return BasePromptTemplate\\n    elif name == \"ArxivAPIWrapper\":\\n        from langchain_community.utilities import ArxivAPIWrapper', metadata={'source': 'test_repo/libs/langchain/langchain/__init__.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='_warn_on_import(\\n            name, replacement=\"langchain_community.utilities.ArxivAPIWrapper\"\\n        )\\n\\n        return ArxivAPIWrapper\\n    elif name == \"GoldenQueryAPIWrapper\":\\n        from langchain_community.utilities import GoldenQueryAPIWrapper\\n\\n        _warn_on_import(\\n            name, replacement=\"langchain_community.utilities.GoldenQueryAPIWrapper\"\\n        )\\n\\n        return GoldenQueryAPIWrapper\\n    elif name == \"GoogleSearchAPIWrapper\":\\n        from langchain_community.utilities import GoogleSearchAPIWrapper\\n\\n        _warn_on_import(\\n            name, replacement=\"langchain_community.utilities.GoogleSearchAPIWrapper\"\\n        )\\n\\n        return GoogleSearchAPIWrapper\\n    elif name == \"GoogleSerperAPIWrapper\":\\n        from langchain_community.utilities import GoogleSerperAPIWrapper\\n\\n        _warn_on_import(\\n            name, replacement=\"langchain_community.utilities.GoogleSerperAPIWrapper\"\\n        )', metadata={'source': 'test_repo/libs/langchain/langchain/__init__.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='return GoogleSerperAPIWrapper\\n    elif name == \"PowerBIDataset\":\\n        from langchain_community.utilities import PowerBIDataset\\n\\n        _warn_on_import(\\n            name, replacement=\"langchain_community.utilities.PowerBIDataset\"\\n        )\\n\\n        return PowerBIDataset\\n    elif name == \"SearxSearchWrapper\":\\n        from langchain_community.utilities import SearxSearchWrapper\\n\\n        _warn_on_import(\\n            name, replacement=\"langchain_community.utilities.SearxSearchWrapper\"\\n        )\\n\\n        return SearxSearchWrapper\\n    elif name == \"WikipediaAPIWrapper\":\\n        from langchain_community.utilities import WikipediaAPIWrapper\\n\\n        _warn_on_import(\\n            name, replacement=\"langchain_community.utilities.WikipediaAPIWrapper\"\\n        )\\n\\n        return WikipediaAPIWrapper\\n    elif name == \"WolframAlphaAPIWrapper\":\\n        from langchain_community.utilities import WolframAlphaAPIWrapper', metadata={'source': 'test_repo/libs/langchain/langchain/__init__.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='_warn_on_import(\\n            name, replacement=\"langchain_community.utilities.WolframAlphaAPIWrapper\"\\n        )\\n\\n        return WolframAlphaAPIWrapper\\n    elif name == \"SQLDatabase\":\\n        from langchain_community.utilities import SQLDatabase\\n\\n        _warn_on_import(name, replacement=\"langchain_community.utilities.SQLDatabase\")\\n\\n        return SQLDatabase\\n    elif name == \"FAISS\":\\n        from langchain_community.vectorstores import FAISS\\n\\n        _warn_on_import(name, replacement=\"langchain_community.vectorstores.FAISS\")\\n\\n        return FAISS\\n    elif name == \"ElasticVectorSearch\":\\n        from langchain_community.vectorstores import ElasticVectorSearch\\n\\n        _warn_on_import(\\n            name, replacement=\"langchain_community.vectorstores.ElasticVectorSearch\"\\n        )\\n\\n        return ElasticVectorSearch\\n    # For backwards compatibility\\n    elif name == \"SerpAPIChain\" or name == \"SerpAPIWrapper\":\\n        from langchain_community.utilities import SerpAPIWrapper', metadata={'source': 'test_repo/libs/langchain/langchain/__init__.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='_warn_on_import(\\n            name, replacement=\"langchain_community.utilities.SerpAPIWrapper\"\\n        )\\n\\n        return SerpAPIWrapper\\n    elif name == \"verbose\":\\n        from langchain.globals import _verbose\\n\\n        _warn_on_import(\\n            name,\\n            replacement=(\\n                \"langchain.globals.set_verbose() / langchain.globals.get_verbose()\"\\n            ),\\n        )\\n\\n        return _verbose\\n    elif name == \"debug\":\\n        from langchain.globals import _debug\\n\\n        _warn_on_import(\\n            name,\\n            replacement=(\\n                \"langchain.globals.set_debug() / langchain.globals.get_debug()\"\\n            ),\\n        )\\n\\n        return _debug\\n    elif name == \"llm_cache\":\\n        from langchain.globals import _llm_cache\\n\\n        _warn_on_import(\\n            name,\\n            replacement=(\\n                \"langchain.globals.set_llm_cache() / langchain.globals.get_llm_cache()\"\\n            ),\\n        )', metadata={'source': 'test_repo/libs/langchain/langchain/__init__.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='return _llm_cache\\n    else:\\n        raise AttributeError(f\"Could not find: {name}\")', metadata={'source': 'test_repo/libs/langchain/langchain/__init__.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='__all__ = [\\n    \"LLMChain\",\\n    \"LLMCheckerChain\",\\n    \"LLMMathChain\",\\n    \"ArxivAPIWrapper\",\\n    \"GoldenQueryAPIWrapper\",\\n    \"SelfAskWithSearchChain\",\\n    \"SerpAPIWrapper\",\\n    \"SerpAPIChain\",\\n    \"SearxSearchWrapper\",\\n    \"GoogleSearchAPIWrapper\",\\n    \"GoogleSerperAPIWrapper\",\\n    \"WolframAlphaAPIWrapper\",\\n    \"WikipediaAPIWrapper\",\\n    \"Anthropic\",\\n    \"Banana\",\\n    \"CerebriumAI\",\\n    \"Cohere\",\\n    \"ForefrontAI\",\\n    \"GooseAI\",\\n    \"Modal\",\\n    \"OpenAI\",\\n    \"Petals\",\\n    \"PipelineAI\",\\n    \"StochasticAI\",\\n    \"Writer\",\\n    \"BasePromptTemplate\",\\n    \"Prompt\",\\n    \"FewShotPromptTemplate\",\\n    \"PromptTemplate\",\\n    \"ReActChain\",\\n    \"Wikipedia\",\\n    \"HuggingFaceHub\",\\n    \"SagemakerEndpoint\",\\n    \"HuggingFacePipeline\",\\n    \"SQLDatabase\",\\n    \"PowerBIDataset\",\\n    \"FAISS\",\\n    \"MRKLChain\",\\n    \"VectorDBQA\",\\n    \"ElasticVectorSearch\",\\n    \"InMemoryDocstore\",\\n    \"ConversationChain\",\\n    \"VectorDBQAWithSourcesChain\",\\n    \"QAWithSourcesChain\",\\n    \"LlamaCpp\",', metadata={'source': 'test_repo/libs/langchain/langchain/__init__.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='\"VectorDBQAWithSourcesChain\",\\n    \"QAWithSourcesChain\",\\n    \"LlamaCpp\",\\n    \"HuggingFaceTextGenInference\",\\n]', metadata={'source': 'test_repo/libs/langchain/langchain/__init__.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='\"\"\"DEPRECATED: Kept for backwards compatibility.\"\"\"\\nfrom langchain_core.utils.input import (\\n    get_bolded_text,\\n    get_color_mapping,\\n    get_colored_text,\\n    print_text,\\n)\\n\\n__all__ = [\\n    \"get_bolded_text\",\\n    \"get_color_mapping\",\\n    \"get_colored_text\",\\n    \"print_text\",\\n]', metadata={'source': 'test_repo/libs/langchain/langchain/input.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='\"\"\"Kept for backwards compatibility.\"\"\"\\nfrom langchain_text_splitters import (\\n    Language,\\n    RecursiveCharacterTextSplitter,\\n    TextSplitter,\\n    Tokenizer,\\n    TokenTextSplitter,\\n)\\nfrom langchain_text_splitters.base import split_text_on_tokens\\nfrom langchain_text_splitters.character import CharacterTextSplitter\\nfrom langchain_text_splitters.html import ElementType, HTMLHeaderTextSplitter\\nfrom langchain_text_splitters.json import RecursiveJsonSplitter\\nfrom langchain_text_splitters.konlpy import KonlpyTextSplitter\\nfrom langchain_text_splitters.latex import LatexTextSplitter\\nfrom langchain_text_splitters.markdown import (\\n    HeaderType,\\n    LineType,\\n    MarkdownHeaderTextSplitter,\\n    MarkdownTextSplitter,\\n)\\nfrom langchain_text_splitters.nltk import NLTKTextSplitter\\nfrom langchain_text_splitters.python import PythonCodeTextSplitter\\nfrom langchain_text_splitters.sentence_transformers import (\\n    SentenceTransformersTokenTextSplitter,\\n)', metadata={'source': 'test_repo/libs/langchain/langchain/text_splitter.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='SentenceTransformersTokenTextSplitter,\\n)\\nfrom langchain_text_splitters.spacy import SpacyTextSplitter', metadata={'source': 'test_repo/libs/langchain/langchain/text_splitter.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='__all__ = [\\n    \"TokenTextSplitter\",\\n    \"TextSplitter\",\\n    \"Tokenizer\",\\n    \"Language\",\\n    \"RecursiveCharacterTextSplitter\",\\n    \"RecursiveJsonSplitter\",\\n    \"LatexTextSplitter\",\\n    \"PythonCodeTextSplitter\",\\n    \"KonlpyTextSplitter\",\\n    \"SpacyTextSplitter\",\\n    \"NLTKTextSplitter\",\\n    \"split_text_on_tokens\",\\n    \"SentenceTransformersTokenTextSplitter\",\\n    \"ElementType\",\\n    \"HeaderType\",\\n    \"LineType\",\\n    \"HTMLHeaderTextSplitter\",\\n    \"MarkdownHeaderTextSplitter\",\\n    \"MarkdownTextSplitter\",\\n    \"CharacterTextSplitter\",\\n]', metadata={'source': 'test_repo/libs/langchain/langchain/text_splitter.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='\"\"\"Interface with the LangChain Hub.\"\"\"\\n\\nfrom __future__ import annotations\\n\\nimport json\\nfrom typing import TYPE_CHECKING, Any, Optional\\n\\nfrom langchain_core.load.dump import dumps\\nfrom langchain_core.load.load import loads\\nfrom langchain_core.prompts import BasePromptTemplate\\n\\nif TYPE_CHECKING:\\n    from langchainhub import Client\\n\\n\\ndef _get_client(api_url: Optional[str] = None, api_key: Optional[str] = None) -> Client:\\n    try:\\n        from langchainhub import Client\\n    except ImportError as e:\\n        raise ImportError(\\n            \"Could not import langchainhub, please install with `pip install \"\\n            \"langchainhub`.\"\\n        ) from e\\n\\n    # Client logic will also attempt to load URL/key from environment variables\\n    return Client(api_url, api_key=api_key)', metadata={'source': 'test_repo/libs/langchain/langchain/hub.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='def push(\\n    repo_full_name: str,\\n    object: Any,\\n    *,\\n    api_url: Optional[str] = None,\\n    api_key: Optional[str] = None,\\n    parent_commit_hash: Optional[str] = \"latest\",\\n    new_repo_is_public: bool = True,\\n    new_repo_description: str = \"\",\\n) -> str:\\n    \"\"\"\\n    Pushes an object to the hub and returns the URL it can be viewed at in a browser.', metadata={'source': 'test_repo/libs/langchain/langchain/hub.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content=':param repo_full_name: The full name of the repo to push to in the format of\\n        `owner/repo`.\\n    :param object: The LangChain to serialize and push to the hub.\\n    :param api_url: The URL of the LangChain Hub API. Defaults to the hosted API service\\n        if you have an api key set, or a localhost instance if not.\\n    :param api_key: The API key to use to authenticate with the LangChain Hub API.\\n    :param parent_commit_hash: The commit hash of the parent commit to push to. Defaults\\n        to the latest commit automatically.\\n    :param new_repo_is_public: Whether the repo should be public. Defaults to\\n        True (Public by default).\\n    :param new_repo_description: The description of the repo. Defaults to an empty\\n        string.\\n    \"\"\"\\n    client = _get_client(api_url=api_url, api_key=api_key)\\n    manifest_json = dumps(object)\\n    message = client.push(\\n        repo_full_name,\\n        manifest_json,\\n        parent_commit_hash=parent_commit_hash,', metadata={'source': 'test_repo/libs/langchain/langchain/hub.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='repo_full_name,\\n        manifest_json,\\n        parent_commit_hash=parent_commit_hash,\\n        new_repo_is_public=new_repo_is_public,\\n        new_repo_description=new_repo_description,\\n    )\\n    return message', metadata={'source': 'test_repo/libs/langchain/langchain/hub.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='def pull(\\n    owner_repo_commit: str,\\n    *,\\n    api_url: Optional[str] = None,\\n    api_key: Optional[str] = None,\\n) -> Any:\\n    \"\"\"\\n    Pulls an object from the hub and returns it as a LangChain object.', metadata={'source': 'test_repo/libs/langchain/langchain/hub.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content=':param owner_repo_commit: The full name of the repo to pull from in the format of\\n        `owner/repo:commit_hash`.\\n    :param api_url: The URL of the LangChain Hub API. Defaults to the hosted API service\\n        if you have an api key set, or a localhost instance if not.\\n    :param api_key: The API key to use to authenticate with the LangChain Hub API.\\n    \"\"\"\\n    client = _get_client(api_url=api_url, api_key=api_key)\\n    res_dict = client.pull_repo(owner_repo_commit)\\n    obj = loads(json.dumps(res_dict[\"manifest\"]))\\n    if isinstance(obj, BasePromptTemplate):\\n        if obj.metadata is None:\\n            obj.metadata = {}\\n        obj.metadata[\"lc_hub_owner\"] = res_dict[\"owner\"]\\n        obj.metadata[\"lc_hub_repo\"] = res_dict[\"repo\"]\\n        obj.metadata[\"lc_hub_commit_hash\"] = res_dict[\"commit_hash\"]\\n    return obj', metadata={'source': 'test_repo/libs/langchain/langchain/hub.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='\"\"\"Keep here for backwards compatibility.\"\"\"\\nfrom langchain.chains.example_generator import generate_example\\n\\n__all__ = [\"generate_example\"]', metadata={'source': 'test_repo/libs/langchain/langchain/example_generator.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='\"\"\"Keep here for backwards compatibility.\"\"\"\\nfrom langchain_community.utilities.sql_database import SQLDatabase\\n\\n__all__ = [\"SQLDatabase\"]', metadata={'source': 'test_repo/libs/langchain/langchain/sql_database.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='\"\"\"Experiment with different models.\"\"\"\\n\\nfrom __future__ import annotations\\n\\nfrom typing import List, Optional, Sequence\\n\\nfrom langchain_core.language_models.llms import BaseLLM\\nfrom langchain_core.prompts.prompt import PromptTemplate\\nfrom langchain_core.utils.input import get_color_mapping, print_text\\n\\nfrom langchain.chains.base import Chain\\nfrom langchain.chains.llm import LLMChain', metadata={'source': 'test_repo/libs/langchain/langchain/model_laboratory.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='class ModelLaboratory:\\n    \"\"\"Experiment with different models.\"\"\"\\n\\n    def __init__(self, chains: Sequence[Chain], names: Optional[List[str]] = None):\\n        \"\"\"Initialize with chains to experiment with.', metadata={'source': 'test_repo/libs/langchain/langchain/model_laboratory.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='Args:\\n            chains: list of chains to experiment with.\\n        \"\"\"\\n        for chain in chains:\\n            if not isinstance(chain, Chain):\\n                raise ValueError(\\n                    \"ModelLaboratory should now be initialized with Chains. \"\\n                    \"If you want to initialize with LLMs, use the `from_llms` method \"\\n                    \"instead (`ModelLaboratory.from_llms(...)`)\"\\n                )\\n            if len(chain.input_keys) != 1:\\n                raise ValueError(\\n                    \"Currently only support chains with one input variable, \"\\n                    f\"got {chain.input_keys}\"\\n                )\\n            if len(chain.output_keys) != 1:\\n                raise ValueError(\\n                    \"Currently only support chains with one output variable, \"\\n                    f\"got {chain.output_keys}\"\\n                )\\n        if names is not None:\\n            if len(names) != len(chains):', metadata={'source': 'test_repo/libs/langchain/langchain/model_laboratory.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content=')\\n        if names is not None:\\n            if len(names) != len(chains):\\n                raise ValueError(\"Length of chains does not match length of names.\")\\n        self.chains = chains\\n        chain_range = [str(i) for i in range(len(self.chains))]\\n        self.chain_colors = get_color_mapping(chain_range)\\n        self.names = names', metadata={'source': 'test_repo/libs/langchain/langchain/model_laboratory.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='@classmethod\\n    def from_llms(\\n        cls, llms: List[BaseLLM], prompt: Optional[PromptTemplate] = None\\n    ) -> ModelLaboratory:\\n        \"\"\"Initialize with LLMs to experiment with and optional prompt.\\n\\n        Args:\\n            llms: list of LLMs to experiment with\\n            prompt: Optional prompt to use to prompt the LLMs. Defaults to None.\\n                If a prompt was provided, it should only have one input variable.\\n        \"\"\"\\n        if prompt is None:\\n            prompt = PromptTemplate(input_variables=[\"_input\"], template=\"{_input}\")\\n        chains = [LLMChain(llm=llm, prompt=prompt) for llm in llms]\\n        names = [str(llm) for llm in llms]\\n        return cls(chains, names=names)\\n\\n    def compare(self, text: str) -> None:\\n        \"\"\"Compare model outputs on an input text.\\n\\n        If a prompt was provided with starting the laboratory, then this text will be\\n        fed into the prompt. If no prompt was provided, then the input text is the\\n        entire prompt.', metadata={'source': 'test_repo/libs/langchain/langchain/model_laboratory.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='Args:\\n            text: input text to run all models on.\\n        \"\"\"\\n        print(f\"\\\\033[1mInput:\\\\033[0m\\\\n{text}\\\\n\")  # noqa: T201\\n        for i, chain in enumerate(self.chains):\\n            if self.names is not None:\\n                name = self.names[i]\\n            else:\\n                name = str(chain)\\n            print_text(name, end=\"\\\\n\")\\n            output = chain.run(text)\\n            print_text(output, color=self.chain_colors[str(i)], end=\"\\\\n\\\\n\")', metadata={'source': 'test_repo/libs/langchain/langchain/model_laboratory.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='\"\"\"DEPRECATED: Kept for backwards compatibility.\"\"\"\\nfrom langchain_community.utilities import Requests, RequestsWrapper, TextRequestsWrapper\\n\\n__all__ = [\\n    \"Requests\",\\n    \"RequestsWrapper\",\\n    \"TextRequestsWrapper\",\\n]', metadata={'source': 'test_repo/libs/langchain/langchain/requests.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='\"\"\"For backwards compatibility.\"\"\"\\nfrom langchain_community.utilities.serpapi import SerpAPIWrapper\\n\\n__all__ = [\"SerpAPIWrapper\"]', metadata={'source': 'test_repo/libs/langchain/langchain/serpapi.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from langchain_community.cache import (\\n    AstraDBCache,\\n    AstraDBSemanticCache,\\n    CassandraCache,\\n    CassandraSemanticCache,\\n    FullLLMCache,\\n    FullMd5LLMCache,\\n    GPTCache,\\n    InMemoryCache,\\n    MomentoCache,\\n    RedisCache,\\n    RedisSemanticCache,\\n    SQLAlchemyCache,\\n    SQLAlchemyMd5Cache,\\n    SQLiteCache,\\n    UpstashRedisCache,\\n)\\n\\n__all__ = [\\n    \"InMemoryCache\",\\n    \"FullLLMCache\",\\n    \"SQLAlchemyCache\",\\n    \"SQLiteCache\",\\n    \"UpstashRedisCache\",\\n    \"RedisCache\",\\n    \"RedisSemanticCache\",\\n    \"GPTCache\",\\n    \"MomentoCache\",\\n    \"CassandraCache\",\\n    \"CassandraSemanticCache\",\\n    \"FullMd5LLMCache\",\\n    \"SQLAlchemyMd5Cache\",\\n    \"AstraDBCache\",\\n    \"AstraDBSemanticCache\",\\n]', metadata={'source': 'test_repo/libs/langchain/langchain/cache.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='import platform\\nfrom functools import lru_cache\\n\\n\\n@lru_cache(maxsize=1)\\ndef get_runtime_environment() -> dict:\\n    \"\"\"Get information about the LangChain runtime environment.\"\"\"\\n    # Lazy import to avoid circular imports\\n    from langchain import __version__\\n\\n    return {\\n        \"library_version\": __version__,\\n        \"library\": \"langchain\",\\n        \"platform\": platform.platform(),\\n        \"runtime\": \"python\",\\n        \"runtime_version\": platform.python_version(),\\n    }', metadata={'source': 'test_repo/libs/langchain/langchain/env.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from importlib import metadata\\n\\n## Create namespaces for pydantic v1 and v2.\\n# This code must stay at the top of the file before other modules may\\n# attempt to import pydantic since it adds pydantic_v1 and pydantic_v2 to sys.modules.\\n#\\n# This hack is done for the following reasons:\\n# * Langchain will attempt to remain compatible with both pydantic v1 and v2 since\\n#   both dependencies and dependents may be stuck on either version of v1 or v2.\\n# * Creating namespaces for pydantic v1 and v2 should allow us to write code that\\n#   unambiguously uses either v1 or v2 API.\\n# * This change is easier to roll out and roll back.\\n\\ntry:\\n    from pydantic.v1 import *  # noqa: F403\\nexcept ImportError:\\n    from pydantic import *  # noqa: F403\\n\\n\\ntry:\\n    _PYDANTIC_MAJOR_VERSION: int = int(metadata.version(\"pydantic\").split(\".\")[0])\\nexcept metadata.PackageNotFoundError:\\n    _PYDANTIC_MAJOR_VERSION = 0', metadata={'source': 'test_repo/libs/langchain/langchain/pydantic_v1/__init__.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='try:\\n    from pydantic.v1.dataclasses import *  # noqa: F403\\nexcept ImportError:\\n    from pydantic.dataclasses import *  # noqa: F403', metadata={'source': 'test_repo/libs/langchain/langchain/pydantic_v1/dataclasses.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='try:\\n    from pydantic.v1.main import *  # noqa: F403\\nexcept ImportError:\\n    from pydantic.main import *  # noqa: F403', metadata={'source': 'test_repo/libs/langchain/langchain/pydantic_v1/main.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from langchain_core.prompts.few_shot_with_templates import FewShotPromptWithTemplates\\n\\n__all__ = [\"FewShotPromptWithTemplates\"]', metadata={'source': 'test_repo/libs/langchain/langchain/prompts/few_shot_with_templates.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='\"\"\"**Prompt** is the input to the model.\\n\\nPrompt is often constructed\\nfrom multiple components. Prompt classes and functions make constructing\\n and working with prompts easy.\\n\\n**Class hierarchy:**\\n\\n.. code-block::\\n\\n    BasePromptTemplate --> PipelinePromptTemplate\\n                           StringPromptTemplate --> PromptTemplate\\n                                                    FewShotPromptTemplate\\n                                                    FewShotPromptWithTemplates\\n                           BaseChatPromptTemplate --> AutoGPTPrompt\\n                                                      ChatPromptTemplate --> AgentScratchPadChatPromptTemplate', metadata={'source': 'test_repo/libs/langchain/langchain/prompts/__init__.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='BaseMessagePromptTemplate --> MessagesPlaceholder\\n                                  BaseStringMessagePromptTemplate --> ChatMessagePromptTemplate\\n                                                                      HumanMessagePromptTemplate\\n                                                                      AIMessagePromptTemplate\\n                                                                      SystemMessagePromptTemplate\\n\\n    PromptValue --> StringPromptValue\\n                    ChatPromptValue', metadata={'source': 'test_repo/libs/langchain/langchain/prompts/__init__.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='PromptValue --> StringPromptValue\\n                    ChatPromptValue\\n\\n\"\"\"  # noqa: E501\\nfrom langchain_core.example_selectors import (\\n    LengthBasedExampleSelector,\\n    MaxMarginalRelevanceExampleSelector,\\n    SemanticSimilarityExampleSelector,\\n)\\nfrom langchain_core.prompts import (\\n    AIMessagePromptTemplate,\\n    BaseChatPromptTemplate,\\n    BasePromptTemplate,\\n    ChatMessagePromptTemplate,\\n    ChatPromptTemplate,\\n    FewShotChatMessagePromptTemplate,\\n    FewShotPromptTemplate,\\n    FewShotPromptWithTemplates,\\n    HumanMessagePromptTemplate,\\n    MessagesPlaceholder,\\n    PipelinePromptTemplate,\\n    PromptTemplate,\\n    StringPromptTemplate,\\n    SystemMessagePromptTemplate,\\n    load_prompt,\\n)\\n\\nfrom langchain.prompts.example_selector import NGramOverlapExampleSelector\\nfrom langchain.prompts.prompt import Prompt', metadata={'source': 'test_repo/libs/langchain/langchain/prompts/__init__.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='__all__ = [\\n    \"AIMessagePromptTemplate\",\\n    \"BaseChatPromptTemplate\",\\n    \"BasePromptTemplate\",\\n    \"ChatMessagePromptTemplate\",\\n    \"ChatPromptTemplate\",\\n    \"FewShotPromptTemplate\",\\n    \"FewShotPromptWithTemplates\",\\n    \"HumanMessagePromptTemplate\",\\n    \"LengthBasedExampleSelector\",\\n    \"MaxMarginalRelevanceExampleSelector\",\\n    \"MessagesPlaceholder\",\\n    \"NGramOverlapExampleSelector\",\\n    \"PipelinePromptTemplate\",\\n    \"PromptTemplate\",\\n    \"SemanticSimilarityExampleSelector\",\\n    \"StringPromptTemplate\",\\n    \"SystemMessagePromptTemplate\",\\n    \"load_prompt\",\\n    \"FewShotChatMessagePromptTemplate\",\\n    \"Prompt\",\\n]', metadata={'source': 'test_repo/libs/langchain/langchain/prompts/__init__.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from langchain_core.prompts.pipeline import PipelinePromptTemplate, _get_inputs\\n\\n__all__ = [\"PipelinePromptTemplate\", \"_get_inputs\"]', metadata={'source': 'test_repo/libs/langchain/langchain/prompts/pipeline.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from langchain_core.prompts.few_shot import (\\n    FewShotChatMessagePromptTemplate,\\n    FewShotPromptTemplate,\\n    _FewShotPromptTemplateMixin,\\n)\\n\\n__all__ = [\\n    \"FewShotPromptTemplate\",\\n    \"FewShotChatMessagePromptTemplate\",\\n    \"_FewShotPromptTemplateMixin\",\\n]', metadata={'source': 'test_repo/libs/langchain/langchain/prompts/few_shot.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from langchain_core.prompts.loading import (\\n    _load_examples,\\n    _load_few_shot_prompt,\\n    _load_output_parser,\\n    _load_prompt,\\n    _load_prompt_from_file,\\n    _load_template,\\n    load_prompt,\\n    load_prompt_from_config,\\n)\\nfrom langchain_core.utils.loading import try_load_from_hub\\n\\n__all__ = [\\n    \"load_prompt_from_config\",\\n    \"load_prompt\",\\n    \"try_load_from_hub\",\\n    \"_load_examples\",\\n    \"_load_few_shot_prompt\",\\n    \"_load_output_parser\",\\n    \"_load_prompt\",\\n    \"_load_prompt_from_file\",\\n    \"_load_template\",\\n]', metadata={'source': 'test_repo/libs/langchain/langchain/prompts/loading.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from langchain_core.prompt_values import StringPromptValue\\nfrom langchain_core.prompts import (\\n    BasePromptTemplate,\\n    StringPromptTemplate,\\n    check_valid_template,\\n    get_template_variables,\\n    jinja2_formatter,\\n    validate_jinja2,\\n)\\nfrom langchain_core.prompts.string import _get_jinja2_variables_from_template\\n\\n__all__ = [\\n    \"jinja2_formatter\",\\n    \"validate_jinja2\",\\n    \"check_valid_template\",\\n    \"get_template_variables\",\\n    \"StringPromptTemplate\",\\n    \"BasePromptTemplate\",\\n    \"StringPromptValue\",\\n    \"_get_jinja2_variables_from_template\",\\n]', metadata={'source': 'test_repo/libs/langchain/langchain/prompts/base.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from langchain_core.prompts.prompt import PromptTemplate\\n\\n# For backwards compatibility.\\nPrompt = PromptTemplate\\n\\n__all__ = [\"PromptTemplate\", \"Prompt\"]', metadata={'source': 'test_repo/libs/langchain/langchain/prompts/prompt.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from langchain_core.prompt_values import ChatPromptValue, ChatPromptValueConcrete\\nfrom langchain_core.prompts.chat import (\\n    AIMessagePromptTemplate,\\n    BaseChatPromptTemplate,\\n    BaseMessagePromptTemplate,\\n    BaseStringMessagePromptTemplate,\\n    ChatMessagePromptTemplate,\\n    ChatPromptTemplate,\\n    HumanMessagePromptTemplate,\\n    MessageLike,\\n    MessageLikeRepresentation,\\n    MessagePromptTemplateT,\\n    MessagesPlaceholder,\\n    SystemMessagePromptTemplate,\\n    _convert_to_message,\\n    _create_template_from_message_type,\\n)', metadata={'source': 'test_repo/libs/langchain/langchain/prompts/chat.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='__all__ = [\\n    \"BaseMessagePromptTemplate\",\\n    \"MessagesPlaceholder\",\\n    \"BaseStringMessagePromptTemplate\",\\n    \"ChatMessagePromptTemplate\",\\n    \"HumanMessagePromptTemplate\",\\n    \"AIMessagePromptTemplate\",\\n    \"SystemMessagePromptTemplate\",\\n    \"BaseChatPromptTemplate\",\\n    \"ChatPromptTemplate\",\\n    \"ChatPromptValue\",\\n    \"ChatPromptValueConcrete\",\\n    \"_convert_to_message\",\\n    \"_create_template_from_message_type\",\\n    \"MessagePromptTemplateT\",\\n    \"MessageLike\",\\n    \"MessageLikeRepresentation\",\\n]', metadata={'source': 'test_repo/libs/langchain/langchain/prompts/chat.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='\"\"\"Logic for selecting examples to include in prompts.\"\"\"\\nfrom langchain_core.example_selectors.length_based import (\\n    LengthBasedExampleSelector,\\n)\\nfrom langchain_core.example_selectors.semantic_similarity import (\\n    MaxMarginalRelevanceExampleSelector,\\n    SemanticSimilarityExampleSelector,\\n)\\n\\nfrom langchain.prompts.example_selector.ngram_overlap import (\\n    NGramOverlapExampleSelector,\\n)\\n\\n__all__ = [\\n    \"LengthBasedExampleSelector\",\\n    \"MaxMarginalRelevanceExampleSelector\",\\n    \"NGramOverlapExampleSelector\",\\n    \"SemanticSimilarityExampleSelector\",\\n]', metadata={'source': 'test_repo/libs/langchain/langchain/prompts/example_selector/__init__.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from langchain_core.example_selectors.length_based import (\\n    LengthBasedExampleSelector,\\n)\\n\\n__all__ = [\"LengthBasedExampleSelector\"]', metadata={'source': 'test_repo/libs/langchain/langchain/prompts/example_selector/length_based.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from langchain_core.example_selectors.base import BaseExampleSelector\\n\\n__all__ = [\"BaseExampleSelector\"]', metadata={'source': 'test_repo/libs/langchain/langchain/prompts/example_selector/base.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from langchain_community.example_selectors.ngram_overlap import (\\n    NGramOverlapExampleSelector,\\n    ngram_overlap_score,\\n)\\n\\n__all__ = [\\n    \"NGramOverlapExampleSelector\",\\n    \"ngram_overlap_score\",\\n]', metadata={'source': 'test_repo/libs/langchain/langchain/prompts/example_selector/ngram_overlap.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from langchain_core.example_selectors.semantic_similarity import (\\n    MaxMarginalRelevanceExampleSelector,\\n    SemanticSimilarityExampleSelector,\\n    sorted_values,\\n)\\n\\n__all__ = [\\n    \"sorted_values\",\\n    \"SemanticSimilarityExampleSelector\",\\n    \"MaxMarginalRelevanceExampleSelector\",\\n]', metadata={'source': 'test_repo/libs/langchain/langchain/prompts/example_selector/semantic_similarity.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from langchain_community.chat_loaders.slack import SlackChatLoader\\n\\n__all__ = [\"SlackChatLoader\"]', metadata={'source': 'test_repo/libs/langchain/langchain/chat_loaders/slack.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='\"\"\"**Chat Loaders** load chat messages from common communications platforms.\\n\\nLoad chat messages from various\\ncommunications platforms such as Facebook Messenger, Telegram, and\\nWhatsApp. The loaded chat messages can be used for fine-tuning models.\\n\\n**Class hierarchy:**\\n\\n.. code-block::\\n\\n    BaseChatLoader --> <name>ChatLoader  # Examples: WhatsAppChatLoader, IMessageChatLoader\\n\\n**Main helpers:**\\n\\n.. code-block::\\n\\n    ChatSession\\n\\n\"\"\"  # noqa: E501', metadata={'source': 'test_repo/libs/langchain/langchain/chat_loaders/__init__.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from langchain_community.chat_loaders.telegram import TelegramChatLoader\\n\\n__all__ = [\"TelegramChatLoader\"]', metadata={'source': 'test_repo/libs/langchain/langchain/chat_loaders/telegram.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from langchain_community.chat_loaders.langsmith import (\\n    LangSmithDatasetChatLoader,\\n    LangSmithRunChatLoader,\\n)\\n\\n__all__ = [\"LangSmithRunChatLoader\", \"LangSmithDatasetChatLoader\"]', metadata={'source': 'test_repo/libs/langchain/langchain/chat_loaders/langsmith.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from langchain_community.chat_loaders.facebook_messenger import (\\n    FolderFacebookMessengerChatLoader,\\n    SingleFileFacebookMessengerChatLoader,\\n)\\n\\n__all__ = [\"SingleFileFacebookMessengerChatLoader\", \"FolderFacebookMessengerChatLoader\"]', metadata={'source': 'test_repo/libs/langchain/langchain/chat_loaders/facebook_messenger.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from langchain_community.chat_loaders.gmail import (\\n    GMailLoader,\\n)\\n\\n__all__ = [\"GMailLoader\"]', metadata={'source': 'test_repo/libs/langchain/langchain/chat_loaders/gmail.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from langchain_community.chat_loaders.utils import (\\n    map_ai_messages,\\n    map_ai_messages_in_session,\\n    merge_chat_runs,\\n    merge_chat_runs_in_session,\\n)\\n\\n__all__ = [\\n    \"merge_chat_runs_in_session\",\\n    \"merge_chat_runs\",\\n    \"map_ai_messages_in_session\",\\n    \"map_ai_messages\",\\n]', metadata={'source': 'test_repo/libs/langchain/langchain/chat_loaders/utils.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from langchain_community.chat_loaders.base import BaseChatLoader\\n\\n__all__ = [\"BaseChatLoader\"]', metadata={'source': 'test_repo/libs/langchain/langchain/chat_loaders/base.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from langchain_community.chat_loaders.whatsapp import WhatsAppChatLoader\\n\\n__all__ = [\"WhatsAppChatLoader\"]', metadata={'source': 'test_repo/libs/langchain/langchain/chat_loaders/whatsapp.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from langchain_community.chat_loaders.imessage import IMessageChatLoader\\n\\n__all__ = [\"IMessageChatLoader\"]', metadata={'source': 'test_repo/libs/langchain/langchain/chat_loaders/imessage.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='\"\"\"Serialization and deserialization.\"\"\"\\nfrom langchain_core.load.dump import dumpd, dumps\\nfrom langchain_core.load.load import load, loads\\n\\n__all__ = [\\n    \"dumpd\",\\n    \"dumps\",\\n    \"load\",\\n    \"loads\",\\n]', metadata={'source': 'test_repo/libs/langchain/langchain/load/__init__.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from langchain_core.load.load import Reviver, load, loads\\n\\n__all__ = [\"Reviver\", \"loads\", \"load\"]', metadata={'source': 'test_repo/libs/langchain/langchain/load/load.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from langchain_core.load.dump import default, dumpd, dumps\\n\\n__all__ = [\"default\", \"dumps\", \"dumpd\"]', metadata={'source': 'test_repo/libs/langchain/langchain/load/dump.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from langchain_core.load.serializable import (\\n    BaseSerialized,\\n    Serializable,\\n    SerializedConstructor,\\n    SerializedNotImplemented,\\n    SerializedSecret,\\n    to_json_not_implemented,\\n    try_neq_default,\\n)\\n\\n__all__ = [\\n    \"BaseSerialized\",\\n    \"SerializedConstructor\",\\n    \"SerializedSecret\",\\n    \"SerializedNotImplemented\",\\n    \"try_neq_default\",\\n    \"Serializable\",\\n    \"to_json_not_implemented\",\\n]', metadata={'source': 'test_repo/libs/langchain/langchain/load/serializable.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from langchain_community.utilities.redis import (\\n    TokenEscaper,\\n    check_redis_module_exist,\\n    get_client,\\n)\\n\\n__all__ = [\\n    \"TokenEscaper\",\\n    \"check_redis_module_exist\",\\n    \"get_client\",\\n]', metadata={'source': 'test_repo/libs/langchain/langchain/utilities/redis.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from langchain_community.utilities.python import PythonREPL\\n\\n__all__ = [\"PythonREPL\"]', metadata={'source': 'test_repo/libs/langchain/langchain/utilities/python.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from langchain_community.utilities.github import GitHubAPIWrapper\\n\\n__all__ = [\"GitHubAPIWrapper\"]', metadata={'source': 'test_repo/libs/langchain/langchain/utilities/github.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from langchain_community.utilities.tensorflow_datasets import TensorflowDatasets\\n\\n__all__ = [\"TensorflowDatasets\"]', metadata={'source': 'test_repo/libs/langchain/langchain/utilities/tensorflow_datasets.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from langchain_community.utilities.google_search import GoogleSearchAPIWrapper\\n\\n__all__ = [\"GoogleSearchAPIWrapper\"]', metadata={'source': 'test_repo/libs/langchain/langchain/utilities/google_search.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='\"\"\"Shims for asyncio features that may be missing from older python versions\"\"\"\\n\\nimport sys\\n\\nif sys.version_info[:2] < (3, 11):\\n    from async_timeout import timeout as asyncio_timeout\\nelse:\\n    from asyncio import timeout as asyncio_timeout\\n\\n\\n__all__ = [\"asyncio_timeout\"]', metadata={'source': 'test_repo/libs/langchain/langchain/utilities/asyncio.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from langchain_community.utilities.graphql import GraphQLAPIWrapper\\n\\n__all__ = [\"GraphQLAPIWrapper\"]', metadata={'source': 'test_repo/libs/langchain/langchain/utilities/graphql.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from langchain_community.utilities.jira import JiraAPIWrapper\\n\\n__all__ = [\"JiraAPIWrapper\"]', metadata={'source': 'test_repo/libs/langchain/langchain/utilities/jira.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from langchain_community.utilities.dalle_image_generator import DallEAPIWrapper\\n\\n__all__ = [\"DallEAPIWrapper\"]', metadata={'source': 'test_repo/libs/langchain/langchain/utilities/dalle_image_generator.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from langchain_community.utilities.merriam_webster import (\\n    MerriamWebsterAPIWrapper,\\n)\\n\\n__all__ = [\\n    \"MerriamWebsterAPIWrapper\",\\n]', metadata={'source': 'test_repo/libs/langchain/langchain/utilities/merriam_webster.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from langchain_community.utilities.arxiv import ArxivAPIWrapper\\n\\n__all__ = [\"ArxivAPIWrapper\"]', metadata={'source': 'test_repo/libs/langchain/langchain/utilities/arxiv.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from langchain_community.utilities.google_jobs import GoogleJobsAPIWrapper\\n\\n__all__ = [\"GoogleJobsAPIWrapper\"]', metadata={'source': 'test_repo/libs/langchain/langchain/utilities/google_jobs.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from langchain_community.utilities.steam import SteamWebAPIWrapper\\n\\n__all__ = [\"SteamWebAPIWrapper\"]', metadata={'source': 'test_repo/libs/langchain/langchain/utilities/steam.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from langchain_community.utilities.clickup import (\\n    ClickupAPIWrapper,\\n    Component,\\n    CUList,\\n    Member,\\n    Space,\\n    Task,\\n    Team,\\n)\\n\\n__all__ = [\\n    \"Component\",\\n    \"Task\",\\n    \"CUList\",\\n    \"Member\",\\n    \"Team\",\\n    \"Space\",\\n    \"ClickupAPIWrapper\",\\n]', metadata={'source': 'test_repo/libs/langchain/langchain/utilities/clickup.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from langchain_community.utilities.anthropic import (\\n    get_num_tokens_anthropic,\\n    get_token_ids_anthropic,\\n)\\n\\n__all__ = [\\n    \"get_num_tokens_anthropic\",\\n    \"get_token_ids_anthropic\",\\n]', metadata={'source': 'test_repo/libs/langchain/langchain/utilities/anthropic.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='\"\"\"**Utilities** are the integrations with third-part systems and packages.\\n\\nOther LangChain classes use **Utilities** to interact with third-part systems\\nand packages.\\n\"\"\"\\nimport warnings\\nfrom typing import Any\\n\\nfrom langchain_community.utilities.requests import (\\n    Requests,\\n    RequestsWrapper,\\n    TextRequestsWrapper,\\n)\\nfrom langchain_core._api import LangChainDeprecationWarning\\n\\nfrom langchain.utils.interactive_env import is_interactive_env', metadata={'source': 'test_repo/libs/langchain/langchain/utilities/__init__.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='def __getattr__(name: str) -> Any:\\n    from langchain_community import utilities\\n\\n    # If not in interactive env, raise warning.\\n    if not is_interactive_env():\\n        warnings.warn(\\n            \"Importing this utility from langchain is deprecated. Importing it from \"\\n            \"langchain will no longer be supported as of langchain==0.2.0. \"\\n            \"Please import from langchain-community instead:\\\\n\\\\n\"\\n            f\"`from langchain_community.utilities import {name}`.\\\\n\\\\n\"\\n            \"To install langchain-community run `pip install -U langchain-community`.\",\\n            category=LangChainDeprecationWarning,\\n        )\\n\\n    return getattr(utilities, name)', metadata={'source': 'test_repo/libs/langchain/langchain/utilities/__init__.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='__all__ = [\\n    \"AlphaVantageAPIWrapper\",\\n    \"ApifyWrapper\",\\n    \"ArceeWrapper\",\\n    \"ArxivAPIWrapper\",\\n    \"BibtexparserWrapper\",\\n    \"BingSearchAPIWrapper\",\\n    \"BraveSearchWrapper\",\\n    \"DuckDuckGoSearchAPIWrapper\",\\n    \"GoldenQueryAPIWrapper\",\\n    \"GoogleFinanceAPIWrapper\",\\n    \"GoogleLensAPIWrapper\",\\n    \"GoogleJobsAPIWrapper\",\\n    \"GooglePlacesAPIWrapper\",\\n    \"GoogleScholarAPIWrapper\",\\n    \"GoogleTrendsAPIWrapper\",\\n    \"GoogleSearchAPIWrapper\",\\n    \"GoogleSerperAPIWrapper\",\\n    \"GraphQLAPIWrapper\",\\n    \"JiraAPIWrapper\",\\n    \"LambdaWrapper\",\\n    \"MaxComputeAPIWrapper\",\\n    \"MerriamWebsterAPIWrapper\",\\n    \"MetaphorSearchAPIWrapper\",\\n    \"NasaAPIWrapper\",\\n    \"OpenWeatherMapAPIWrapper\",\\n    \"OutlineAPIWrapper\",\\n    \"Portkey\",\\n    \"PowerBIDataset\",\\n    \"PubMedAPIWrapper\",\\n    \"PythonREPL\",\\n    \"Requests\",\\n    \"RequestsWrapper\",\\n    \"SteamWebAPIWrapper\",\\n    \"SQLDatabase\",\\n    \"SceneXplainAPIWrapper\",\\n    \"SearchApiAPIWrapper\",\\n    \"SearxSearchWrapper\",\\n    \"SerpAPIWrapper\",', metadata={'source': 'test_repo/libs/langchain/langchain/utilities/__init__.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='\"SearchApiAPIWrapper\",\\n    \"SearxSearchWrapper\",\\n    \"SerpAPIWrapper\",\\n    \"SparkSQL\",\\n    \"StackExchangeAPIWrapper\",\\n    \"TensorflowDatasets\",\\n    \"TextRequestsWrapper\",\\n    \"TwilioAPIWrapper\",\\n    \"WikipediaAPIWrapper\",\\n    \"WolframAlphaAPIWrapper\",\\n    \"ZapierNLAWrapper\",\\n]', metadata={'source': 'test_repo/libs/langchain/langchain/utilities/__init__.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from langchain_community.utilities.opaqueprompts import desanitize, sanitize\\n\\n__all__ = [\"sanitize\", \"desanitize\"]', metadata={'source': 'test_repo/libs/langchain/langchain/utilities/opaqueprompts.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from langchain_community.utilities.tavily_search import (\\n    TavilySearchAPIWrapper,\\n)\\n\\n__all__ = [\"TavilySearchAPIWrapper\"]', metadata={'source': 'test_repo/libs/langchain/langchain/utilities/tavily_search.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from langchain_community.utilities.nasa import (\\n    NasaAPIWrapper,\\n)\\n\\n__all__ = [\"NasaAPIWrapper\"]', metadata={'source': 'test_repo/libs/langchain/langchain/utilities/nasa.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from langchain_community.utilities.alpha_vantage import AlphaVantageAPIWrapper\\n\\n__all__ = [\"AlphaVantageAPIWrapper\"]', metadata={'source': 'test_repo/libs/langchain/langchain/utilities/alpha_vantage.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from langchain_community.utilities.golden_query import (\\n    GoldenQueryAPIWrapper,\\n)\\n\\n__all__ = [\"GoldenQueryAPIWrapper\"]', metadata={'source': 'test_repo/libs/langchain/langchain/utilities/golden_query.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from langchain_community.utilities.reddit_search import RedditSearchAPIWrapper\\n\\n__all__ = [\"RedditSearchAPIWrapper\"]', metadata={'source': 'test_repo/libs/langchain/langchain/utilities/reddit_search.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from langchain_community.utilities.wikipedia import (\\n    WikipediaAPIWrapper,\\n)\\n\\n__all__ = [\"WikipediaAPIWrapper\"]', metadata={'source': 'test_repo/libs/langchain/langchain/utilities/wikipedia.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from langchain_community.utilities.searx_search import (\\n    SearxResults,\\n    SearxSearchWrapper,\\n)\\n\\n__all__ = [\"SearxResults\", \"SearxSearchWrapper\"]', metadata={'source': 'test_repo/libs/langchain/langchain/utilities/searx_search.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from langchain_community.utilities.bing_search import BingSearchAPIWrapper\\n\\n__all__ = [\"BingSearchAPIWrapper\"]', metadata={'source': 'test_repo/libs/langchain/langchain/utilities/bing_search.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from langchain_community.utilities.vertexai import (\\n    create_retry_decorator,\\n    get_client_info,\\n    init_vertexai,\\n    raise_vertex_import_error,\\n)\\n\\n__all__ = [\\n    \"create_retry_decorator\",\\n    \"raise_vertex_import_error\",\\n    \"init_vertexai\",\\n    \"get_client_info\",\\n]', metadata={'source': 'test_repo/libs/langchain/langchain/utilities/vertexai.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from langchain_community.utilities.duckduckgo_search import DuckDuckGoSearchAPIWrapper\\n\\n__all__ = [\"DuckDuckGoSearchAPIWrapper\"]', metadata={'source': 'test_repo/libs/langchain/langchain/utilities/duckduckgo_search.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from langchain_community.utilities.openapi import HTTPVerb, OpenAPISpec\\n\\n__all__ = [\"HTTPVerb\", \"OpenAPISpec\"]', metadata={'source': 'test_repo/libs/langchain/langchain/utilities/openapi.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from langchain_community.utilities.max_compute import MaxComputeAPIWrapper\\n\\n__all__ = [\"MaxComputeAPIWrapper\"]', metadata={'source': 'test_repo/libs/langchain/langchain/utilities/max_compute.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from langchain_community.utilities.powerbi import (\\n    PowerBIDataset,\\n)\\n\\n__all__ = [\\n    \"PowerBIDataset\",\\n]', metadata={'source': 'test_repo/libs/langchain/langchain/utilities/powerbi.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from langchain_community.utilities.stackexchange import StackExchangeAPIWrapper\\n\\n__all__ = [\"StackExchangeAPIWrapper\"]', metadata={'source': 'test_repo/libs/langchain/langchain/utilities/stackexchange.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from langchain_community.utilities.brave_search import BraveSearchWrapper\\n\\n__all__ = [\"BraveSearchWrapper\"]', metadata={'source': 'test_repo/libs/langchain/langchain/utilities/brave_search.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from langchain_community.utilities.gitlab import GitLabAPIWrapper\\n\\n__all__ = [\"GitLabAPIWrapper\"]', metadata={'source': 'test_repo/libs/langchain/langchain/utilities/gitlab.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from langchain_community.utilities.sql_database import (\\n    SQLDatabase,\\n    truncate_word,\\n)\\n\\n__all__ = [\"truncate_word\", \"SQLDatabase\"]', metadata={'source': 'test_repo/libs/langchain/langchain/utilities/sql_database.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from langchain_community.utilities.google_finance import GoogleFinanceAPIWrapper\\n\\n__all__ = [\"GoogleFinanceAPIWrapper\"]', metadata={'source': 'test_repo/libs/langchain/langchain/utilities/google_finance.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from langchain_community.utilities.zapier import ZapierNLAWrapper\\n\\n__all__ = [\"ZapierNLAWrapper\"]', metadata={'source': 'test_repo/libs/langchain/langchain/utilities/zapier.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from langchain_core.utils.loading import try_load_from_hub\\n\\n# For backwards compatibility\\n__all__ = [\"try_load_from_hub\"]', metadata={'source': 'test_repo/libs/langchain/langchain/utilities/loading.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from langchain_community.utilities.portkey import Portkey\\n\\n__all__ = [\"Portkey\"]', metadata={'source': 'test_repo/libs/langchain/langchain/utilities/portkey.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from langchain_community.utilities.searchapi import SearchApiAPIWrapper\\n\\n__all__ = [\"SearchApiAPIWrapper\"]', metadata={'source': 'test_repo/libs/langchain/langchain/utilities/searchapi.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from langchain_community.utilities.pubmed import PubMedAPIWrapper\\n\\n__all__ = [\"PubMedAPIWrapper\"]', metadata={'source': 'test_repo/libs/langchain/langchain/utilities/pubmed.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from langchain_community.utilities.google_trends import GoogleTrendsAPIWrapper\\n\\n__all__ = [\"GoogleTrendsAPIWrapper\"]', metadata={'source': 'test_repo/libs/langchain/langchain/utilities/google_trends.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from langchain_community.utilities.requests import (\\n    Requests,\\n    RequestsWrapper,\\n    TextRequestsWrapper,\\n)\\n\\n__all__ = [\"Requests\", \"TextRequestsWrapper\", \"RequestsWrapper\"]', metadata={'source': 'test_repo/libs/langchain/langchain/utilities/requests.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from langchain_community.utilities.openweathermap import OpenWeatherMapAPIWrapper\\n\\n__all__ = [\"OpenWeatherMapAPIWrapper\"]', metadata={'source': 'test_repo/libs/langchain/langchain/utilities/openweathermap.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from langchain_community.utilities.wolfram_alpha import WolframAlphaAPIWrapper\\n\\n__all__ = [\"WolframAlphaAPIWrapper\"]', metadata={'source': 'test_repo/libs/langchain/langchain/utilities/wolfram_alpha.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from langchain_community.utilities.metaphor_search import (\\n    MetaphorSearchAPIWrapper,\\n)\\n\\n__all__ = [\"MetaphorSearchAPIWrapper\"]', metadata={'source': 'test_repo/libs/langchain/langchain/utilities/metaphor_search.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from langchain_community.utilities.scenexplain import SceneXplainAPIWrapper\\n\\n__all__ = [\"SceneXplainAPIWrapper\"]', metadata={'source': 'test_repo/libs/langchain/langchain/utilities/scenexplain.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from langchain_community.utilities.awslambda import LambdaWrapper\\n\\n__all__ = [\"LambdaWrapper\"]', metadata={'source': 'test_repo/libs/langchain/langchain/utilities/awslambda.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from langchain_community.utilities.dataforseo_api_search import DataForSeoAPIWrapper\\n\\n__all__ = [\"DataForSeoAPIWrapper\"]', metadata={'source': 'test_repo/libs/langchain/langchain/utilities/dataforseo_api_search.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from langchain_community.utilities.serpapi import HiddenPrints, SerpAPIWrapper\\n\\n__all__ = [\"HiddenPrints\", \"SerpAPIWrapper\"]', metadata={'source': 'test_repo/libs/langchain/langchain/utilities/serpapi.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from langchain_community.utilities.apify import ApifyWrapper\\n\\n__all__ = [\"ApifyWrapper\"]', metadata={'source': 'test_repo/libs/langchain/langchain/utilities/apify.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from langchain_community.utilities.google_serper import GoogleSerperAPIWrapper\\n\\n__all__ = [\"GoogleSerperAPIWrapper\"]', metadata={'source': 'test_repo/libs/langchain/langchain/utilities/google_serper.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from langchain_community.utilities.twilio import TwilioAPIWrapper\\n\\n__all__ = [\"TwilioAPIWrapper\"]', metadata={'source': 'test_repo/libs/langchain/langchain/utilities/twilio.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from langchain_community.utilities.outline import (\\n    OutlineAPIWrapper,\\n)\\n\\n__all__ = [\"OutlineAPIWrapper\"]', metadata={'source': 'test_repo/libs/langchain/langchain/utilities/outline.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from langchain_community.utilities.spark_sql import SparkSQL\\n\\n__all__ = [\"SparkSQL\"]', metadata={'source': 'test_repo/libs/langchain/langchain/utilities/spark_sql.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from langchain_community.utilities.google_places_api import GooglePlacesAPIWrapper\\n\\n__all__ = [\"GooglePlacesAPIWrapper\"]', metadata={'source': 'test_repo/libs/langchain/langchain/utilities/google_places_api.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from langchain_community.utilities.google_scholar import GoogleScholarAPIWrapper\\n\\n__all__ = [\"GoogleScholarAPIWrapper\"]', metadata={'source': 'test_repo/libs/langchain/langchain/utilities/google_scholar.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from langchain_community.utilities.google_lens import GoogleLensAPIWrapper\\n\\n__all__ = [\"GoogleLensAPIWrapper\"]', metadata={'source': 'test_repo/libs/langchain/langchain/utilities/google_lens.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from langchain_community.utilities.arcee import (\\n    ArceeDocument,\\n    ArceeDocumentAdapter,\\n    ArceeDocumentSource,\\n    ArceeRoute,\\n    ArceeWrapper,\\n    DALMFilter,\\n    DALMFilterType,\\n)\\n\\n__all__ = [\\n    \"ArceeRoute\",\\n    \"DALMFilterType\",\\n    \"DALMFilter\",\\n    \"ArceeDocumentSource\",\\n    \"ArceeDocument\",\\n    \"ArceeDocumentAdapter\",\\n    \"ArceeWrapper\",\\n]', metadata={'source': 'test_repo/libs/langchain/langchain/utilities/arcee.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from langchain_community.utilities.bibtex import BibtexparserWrapper\\n\\n__all__ = [\"BibtexparserWrapper\"]', metadata={'source': 'test_repo/libs/langchain/langchain/utilities/bibtex.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from langchain_core.utils.json_schema import (\\n    _dereference_refs_helper,\\n    _infer_skip_keys,\\n    _retrieve_ref,\\n    dereference_refs,\\n)\\n\\n__all__ = [\\n    \"_retrieve_ref\",\\n    \"_dereference_refs_helper\",\\n    \"_infer_skip_keys\",\\n    \"dereference_refs\",\\n]', metadata={'source': 'test_repo/libs/langchain/langchain/utils/json_schema.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from langchain_core.utils.aiter import NoLock, Tee, py_anext\\n\\n__all__ = [\"py_anext\", \"NoLock\", \"Tee\"]', metadata={'source': 'test_repo/libs/langchain/langchain/utils/aiter.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from langchain_core.utils.pydantic import get_pydantic_major_version\\n\\n__all__ = [\"get_pydantic_major_version\"]', metadata={'source': 'test_repo/libs/langchain/langchain/utils/pydantic.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from langchain_core.utils.formatting import StrictFormatter\\n\\n__all__ = [\"StrictFormatter\"]', metadata={'source': 'test_repo/libs/langchain/langchain/utils/formatting.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='def is_interactive_env() -> bool:\\n    \"\"\"Determine if running within IPython or Jupyter.\"\"\"\\n    import sys\\n\\n    return hasattr(sys, \"ps2\")', metadata={'source': 'test_repo/libs/langchain/langchain/utils/interactive_env.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='\"\"\"\\n**Utility functions** for LangChain.\\n\\nThese functions do not depend on any other LangChain module.\\n\"\"\"\\n\\nfrom langchain_core.utils.formatting import StrictFormatter, formatter\\nfrom langchain_core.utils.input import (\\n    get_bolded_text,\\n    get_color_mapping,\\n    get_colored_text,\\n    print_text,\\n)\\nfrom langchain_core.utils.utils import (\\n    check_package_version,\\n    convert_to_secret_str,\\n    get_pydantic_field_names,\\n    guard_import,\\n    mock_now,\\n    raise_for_status_with_text,\\n    xor_args,\\n)\\n\\nfrom langchain.utils.env import get_from_dict_or_env, get_from_env\\nfrom langchain.utils.math import cosine_similarity, cosine_similarity_top_k\\nfrom langchain.utils.strings import comma_list, stringify_dict, stringify_value', metadata={'source': 'test_repo/libs/langchain/langchain/utils/__init__.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='__all__ = [\\n    \"StrictFormatter\",\\n    \"check_package_version\",\\n    \"comma_list\",\\n    \"convert_to_secret_str\",\\n    \"cosine_similarity\",\\n    \"cosine_similarity_top_k\",\\n    \"formatter\",\\n    \"get_bolded_text\",\\n    \"get_color_mapping\",\\n    \"get_colored_text\",\\n    \"get_from_dict_or_env\",\\n    \"get_from_env\",\\n    \"get_pydantic_field_names\",\\n    \"guard_import\",\\n    \"mock_now\",\\n    \"print_text\",\\n    \"raise_for_status_with_text\",\\n    \"stringify_dict\",\\n    \"stringify_value\",\\n    \"xor_args\",\\n]', metadata={'source': 'test_repo/libs/langchain/langchain/utils/__init__.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from langchain_core.utils.input import (\\n    get_bolded_text,\\n    get_color_mapping,\\n    get_colored_text,\\n    print_text,\\n)\\n\\n__all__ = [\"get_color_mapping\", \"get_colored_text\", \"get_bolded_text\", \"print_text\"]', metadata={'source': 'test_repo/libs/langchain/langchain/utils/input.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from langchain_core.utils.strings import comma_list, stringify_dict, stringify_value\\n\\n__all__ = [\"stringify_value\", \"stringify_dict\", \"comma_list\"]', metadata={'source': 'test_repo/libs/langchain/langchain/utils/strings.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from langchain_core.utils.utils import (\\n    build_extra_kwargs,\\n    check_package_version,\\n    convert_to_secret_str,\\n    get_pydantic_field_names,\\n    guard_import,\\n    mock_now,\\n    raise_for_status_with_text,\\n    xor_args,\\n)\\n\\n__all__ = [\\n    \"xor_args\",\\n    \"raise_for_status_with_text\",\\n    \"mock_now\",\\n    \"guard_import\",\\n    \"check_package_version\",\\n    \"get_pydantic_field_names\",\\n    \"build_extra_kwargs\",\\n    \"convert_to_secret_str\",\\n]', metadata={'source': 'test_repo/libs/langchain/langchain/utils/utils.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from langchain_community.utils.math import (\\n    Matrix,\\n    cosine_similarity,\\n    cosine_similarity_top_k,\\n)\\n\\n__all__ = [\"Matrix\", \"cosine_similarity\", \"cosine_similarity_top_k\"]', metadata={'source': 'test_repo/libs/langchain/langchain/utils/math.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from langchain_core.utils.loading import try_load_from_hub\\n\\n__all__ = [\"try_load_from_hub\"]', metadata={'source': 'test_repo/libs/langchain/langchain/utils/loading.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from langchain_community.utils.openai import is_openai_v1\\n\\n__all__ = [\"is_openai_v1\"]', metadata={'source': 'test_repo/libs/langchain/langchain/utils/openai.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from langchain_core.utils.iter import NoLock, Tee, batch_iterate, tee_peer\\n\\n__all__ = [\"NoLock\", \"tee_peer\", \"Tee\", \"batch_iterate\"]', metadata={'source': 'test_repo/libs/langchain/langchain/utils/iter.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from langchain_community.utils.ernie_functions import (\\n    FunctionDescription,\\n    ToolDescription,\\n    convert_pydantic_to_ernie_function,\\n    convert_pydantic_to_ernie_tool,\\n)\\n\\n__all__ = [\\n    \"FunctionDescription\",\\n    \"ToolDescription\",\\n    \"convert_pydantic_to_ernie_function\",\\n    \"convert_pydantic_to_ernie_tool\",\\n]', metadata={'source': 'test_repo/libs/langchain/langchain/utils/ernie_functions.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from langchain_core.utils.env import get_from_dict_or_env, get_from_env\\n\\n__all__ = [\"get_from_dict_or_env\", \"get_from_env\"]', metadata={'source': 'test_repo/libs/langchain/langchain/utils/env.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from langchain_community.utils.openai_functions import (\\n    FunctionDescription,\\n    ToolDescription,\\n    convert_pydantic_to_openai_function,\\n    convert_pydantic_to_openai_tool,\\n)\\n\\n__all__ = [\\n    \"FunctionDescription\",\\n    \"ToolDescription\",\\n    \"convert_pydantic_to_openai_function\",\\n    \"convert_pydantic_to_openai_tool\",\\n]', metadata={'source': 'test_repo/libs/langchain/langchain/utils/openai_functions.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from langchain_core.utils.html import (\\n    DEFAULT_LINK_REGEX,\\n    PREFIXES_TO_IGNORE,\\n    PREFIXES_TO_IGNORE_REGEX,\\n    SUFFIXES_TO_IGNORE,\\n    SUFFIXES_TO_IGNORE_REGEX,\\n    extract_sub_links,\\n    find_all_links,\\n)\\n\\n__all__ = [\\n    \"PREFIXES_TO_IGNORE\",\\n    \"SUFFIXES_TO_IGNORE\",\\n    \"SUFFIXES_TO_IGNORE_REGEX\",\\n    \"PREFIXES_TO_IGNORE_REGEX\",\\n    \"DEFAULT_LINK_REGEX\",\\n    \"find_all_links\",\\n    \"extract_sub_links\",\\n]', metadata={'source': 'test_repo/libs/langchain/langchain/utils/html.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from langchain_community.retrievers.chatgpt_plugin_retriever import (\\n    ChatGPTPluginRetriever,\\n)\\n\\n__all__ = [\"ChatGPTPluginRetriever\"]', metadata={'source': 'test_repo/libs/langchain/langchain/retrievers/chatgpt_plugin_retriever.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from enum import Enum\\nfrom typing import Dict, List, Optional\\n\\nfrom langchain_core.callbacks import (\\n    AsyncCallbackManagerForRetrieverRun,\\n    CallbackManagerForRetrieverRun,\\n)\\nfrom langchain_core.documents import Document\\nfrom langchain_core.pydantic_v1 import Field, root_validator\\nfrom langchain_core.retrievers import BaseRetriever\\nfrom langchain_core.stores import BaseStore, ByteStore\\nfrom langchain_core.vectorstores import VectorStore\\n\\nfrom langchain.storage._lc_store import create_kv_docstore\\n\\n\\nclass SearchType(str, Enum):\\n    \"\"\"Enumerator of the types of search to perform.\"\"\"\\n\\n    similarity = \"similarity\"\\n    \"\"\"Similarity search.\"\"\"\\n    mmr = \"mmr\"\\n    \"\"\"Maximal Marginal Relevance reranking of similarity search.\"\"\"', metadata={'source': 'test_repo/libs/langchain/langchain/retrievers/multi_vector.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='class MultiVectorRetriever(BaseRetriever):\\n    \"\"\"Retrieve from a set of multiple embeddings for the same document.\"\"\"\\n\\n    vectorstore: VectorStore\\n    \"\"\"The underlying vectorstore to use to store small chunks\\n    and their embedding vectors\"\"\"\\n    byte_store: Optional[ByteStore] = None\\n    \"\"\"The lower-level backing storage layer for the parent documents\"\"\"\\n    docstore: BaseStore[str, Document]\\n    \"\"\"The storage interface for the parent documents\"\"\"\\n    id_key: str = \"doc_id\"\\n    search_kwargs: dict = Field(default_factory=dict)\\n    \"\"\"Keyword arguments to pass to the search function.\"\"\"\\n    search_type: SearchType = SearchType.similarity\\n    \"\"\"Type of search to perform (similarity / mmr)\"\"\"', metadata={'source': 'test_repo/libs/langchain/langchain/retrievers/multi_vector.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='@root_validator(pre=True)\\n    def shim_docstore(cls, values: Dict) -> Dict:\\n        byte_store = values.get(\"byte_store\")\\n        docstore = values.get(\"docstore\")\\n        if byte_store is not None:\\n            docstore = create_kv_docstore(byte_store)\\n        elif docstore is None:\\n            raise Exception(\"You must pass a `byte_store` parameter.\")\\n        values[\"docstore\"] = docstore\\n        return values', metadata={'source': 'test_repo/libs/langchain/langchain/retrievers/multi_vector.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='def _get_relevant_documents(\\n        self, query: str, *, run_manager: CallbackManagerForRetrieverRun\\n    ) -> List[Document]:\\n        \"\"\"Get documents relevant to a query.\\n        Args:\\n            query: String to find relevant documents for\\n            run_manager: The callbacks handler to use\\n        Returns:\\n            List of relevant documents\\n        \"\"\"\\n        if self.search_type == SearchType.mmr:\\n            sub_docs = self.vectorstore.max_marginal_relevance_search(\\n                query, **self.search_kwargs\\n            )\\n        else:\\n            sub_docs = self.vectorstore.similarity_search(query, **self.search_kwargs)\\n\\n        # We do this to maintain the order of the ids that are returned\\n        ids = []\\n        for d in sub_docs:\\n            if self.id_key in d.metadata and d.metadata[self.id_key] not in ids:\\n                ids.append(d.metadata[self.id_key])\\n        docs = self.docstore.mget(ids)\\n        return [d for d in docs if d is not None]', metadata={'source': 'test_repo/libs/langchain/langchain/retrievers/multi_vector.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='async def _aget_relevant_documents(\\n        self, query: str, *, run_manager: AsyncCallbackManagerForRetrieverRun\\n    ) -> List[Document]:\\n        \"\"\"Asynchronously get documents relevant to a query.\\n        Args:\\n            query: String to find relevant documents for\\n            run_manager: The callbacks handler to use\\n        Returns:\\n            List of relevant documents\\n        \"\"\"\\n        if self.search_type == SearchType.mmr:\\n            sub_docs = await self.vectorstore.amax_marginal_relevance_search(\\n                query, **self.search_kwargs\\n            )\\n        else:\\n            sub_docs = await self.vectorstore.asimilarity_search(\\n                query, **self.search_kwargs\\n            )', metadata={'source': 'test_repo/libs/langchain/langchain/retrievers/multi_vector.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='# We do this to maintain the order of the ids that are returned\\n        ids = []\\n        for d in sub_docs:\\n            if self.id_key in d.metadata and d.metadata[self.id_key] not in ids:\\n                ids.append(d.metadata[self.id_key])\\n        docs = await self.docstore.amget(ids)\\n        return [d for d in docs if d is not None]', metadata={'source': 'test_repo/libs/langchain/langchain/retrievers/multi_vector.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from langchain_community.retrievers.arxiv import ArxivRetriever\\n\\n__all__ = [\"ArxivRetriever\"]', metadata={'source': 'test_repo/libs/langchain/langchain/retrievers/arxiv.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from langchain_community.retrievers.metal import MetalRetriever\\n\\n__all__ = [\"MetalRetriever\"]', metadata={'source': 'test_repo/libs/langchain/langchain/retrievers/metal.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='\"\"\"**Retriever** class returns Documents given a text **query**.\\n\\nIt is more general than a vector store. A retriever does not need to be able to\\nstore documents, only to return (or retrieve) it. Vector stores can be used as\\nthe backbone of a retriever, but there are other types of retrievers as well.\\n\\n**Class hierarchy:**\\n\\n.. code-block::\\n\\n    BaseRetriever --> <name>Retriever  # Examples: ArxivRetriever, MergerRetriever\\n\\n**Main helpers:**\\n\\n.. code-block::\\n\\n    Document, Serializable, Callbacks,\\n    CallbackManagerForRetrieverRun, AsyncCallbackManagerForRetrieverRun\\n\"\"\"\\nimport warnings\\nfrom typing import Any\\n\\nfrom langchain_core._api import LangChainDeprecationWarning', metadata={'source': 'test_repo/libs/langchain/langchain/retrievers/__init__.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from langchain_core._api import LangChainDeprecationWarning\\n\\nfrom langchain.retrievers.contextual_compression import ContextualCompressionRetriever\\nfrom langchain.retrievers.ensemble import EnsembleRetriever\\nfrom langchain.retrievers.merger_retriever import MergerRetriever\\nfrom langchain.retrievers.multi_query import MultiQueryRetriever\\nfrom langchain.retrievers.multi_vector import MultiVectorRetriever\\nfrom langchain.retrievers.outline import OutlineRetriever\\nfrom langchain.retrievers.parent_document_retriever import ParentDocumentRetriever\\nfrom langchain.retrievers.re_phraser import RePhraseQueryRetriever\\nfrom langchain.retrievers.self_query.base import SelfQueryRetriever\\nfrom langchain.retrievers.time_weighted_retriever import (\\n    TimeWeightedVectorStoreRetriever,\\n)\\nfrom langchain.retrievers.web_research import WebResearchRetriever\\nfrom langchain.utils.interactive_env import is_interactive_env', metadata={'source': 'test_repo/libs/langchain/langchain/retrievers/__init__.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='def __getattr__(name: str) -> Any:\\n    from langchain_community import retrievers\\n\\n    # If not in interactive env, raise warning.\\n    if not is_interactive_env():\\n        warnings.warn(\\n            \"Importing this retriever from langchain is deprecated. Importing it from \"\\n            \"langchain will no longer be supported as of langchain==0.2.0. \"\\n            \"Please import from langchain-community instead:\\\\n\\\\n\"\\n            f\"`from langchain_community.retrievers import {name}`.\\\\n\\\\n\"\\n            \"To install langchain-community run `pip install -U langchain-community`.\",\\n            category=LangChainDeprecationWarning,\\n        )\\n\\n    return getattr(retrievers, name)', metadata={'source': 'test_repo/libs/langchain/langchain/retrievers/__init__.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='__all__ = [\\n    \"AmazonKendraRetriever\",\\n    \"AmazonKnowledgeBasesRetriever\",\\n    \"ArceeRetriever\",\\n    \"ArxivRetriever\",\\n    \"AzureCognitiveSearchRetriever\",\\n    \"ChatGPTPluginRetriever\",\\n    \"ContextualCompressionRetriever\",\\n    \"ChaindeskRetriever\",\\n    \"CohereRagRetriever\",\\n    \"ElasticSearchBM25Retriever\",\\n    \"EmbedchainRetriever\",\\n    \"GoogleDocumentAIWarehouseRetriever\",\\n    \"GoogleCloudEnterpriseSearchRetriever\",\\n    \"GoogleVertexAIMultiTurnSearchRetriever\",\\n    \"GoogleVertexAISearchRetriever\",\\n    \"KayAiRetriever\",\\n    \"KNNRetriever\",\\n    \"LlamaIndexGraphRetriever\",\\n    \"LlamaIndexRetriever\",\\n    \"MergerRetriever\",\\n    \"MetalRetriever\",\\n    \"MilvusRetriever\",\\n    \"MultiQueryRetriever\",\\n    \"OutlineRetriever\",\\n    \"PineconeHybridSearchRetriever\",\\n    \"PubMedRetriever\",\\n    \"RemoteLangChainRetriever\",\\n    \"SVMRetriever\",\\n    \"SelfQueryRetriever\",\\n    \"TavilySearchAPIRetriever\",\\n    \"TFIDFRetriever\",\\n    \"BM25Retriever\",\\n    \"TimeWeightedVectorStoreRetriever\",', metadata={'source': 'test_repo/libs/langchain/langchain/retrievers/__init__.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='\"TFIDFRetriever\",\\n    \"BM25Retriever\",\\n    \"TimeWeightedVectorStoreRetriever\",\\n    \"VespaRetriever\",\\n    \"WeaviateHybridSearchRetriever\",\\n    \"WikipediaRetriever\",\\n    \"ZepRetriever\",\\n    \"ZillizRetriever\",\\n    \"DocArrayRetriever\",\\n    \"RePhraseQueryRetriever\",\\n    \"WebResearchRetriever\",\\n    \"EnsembleRetriever\",\\n    \"ParentDocumentRetriever\",\\n    \"MultiVectorRetriever\",\\n]', metadata={'source': 'test_repo/libs/langchain/langchain/retrievers/__init__.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from langchain_community.retrievers.knn import KNNRetriever\\n\\n__all__ = [\"KNNRetriever\"]', metadata={'source': 'test_repo/libs/langchain/langchain/retrievers/knn.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from langchain_community.retrievers.elastic_search_bm25 import (\\n    ElasticSearchBM25Retriever,\\n)\\n\\n__all__ = [\"ElasticSearchBM25Retriever\"]', metadata={'source': 'test_repo/libs/langchain/langchain/retrievers/elastic_search_bm25.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from langchain_community.retrievers.chaindesk import ChaindeskRetriever\\n\\n__all__ = [\"ChaindeskRetriever\"]', metadata={'source': 'test_repo/libs/langchain/langchain/retrievers/chaindesk.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from langchain_community.retrievers.you import YouRetriever\\n\\n__all__ = [\"YouRetriever\"]', metadata={'source': 'test_repo/libs/langchain/langchain/retrievers/you.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from langchain_community.retrievers.wikipedia import WikipediaRetriever\\n\\n__all__ = [\"WikipediaRetriever\"]', metadata={'source': 'test_repo/libs/langchain/langchain/retrievers/wikipedia.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='import logging\\nimport re\\nfrom typing import List, Optional\\n\\nfrom langchain_community.document_loaders import AsyncHtmlLoader\\nfrom langchain_community.document_transformers import Html2TextTransformer\\nfrom langchain_community.llms import LlamaCpp\\nfrom langchain_community.utilities import GoogleSearchAPIWrapper\\nfrom langchain_core.callbacks import (\\n    AsyncCallbackManagerForRetrieverRun,\\n    CallbackManagerForRetrieverRun,\\n)\\nfrom langchain_core.documents import Document\\nfrom langchain_core.language_models import BaseLLM\\nfrom langchain_core.output_parsers import BaseOutputParser\\nfrom langchain_core.prompts import BasePromptTemplate, PromptTemplate\\nfrom langchain_core.pydantic_v1 import BaseModel, Field\\nfrom langchain_core.retrievers import BaseRetriever\\nfrom langchain_core.vectorstores import VectorStore\\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter, TextSplitter', metadata={'source': 'test_repo/libs/langchain/langchain/retrievers/web_research.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from langchain.chains import LLMChain\\nfrom langchain.chains.prompt_selector import ConditionalPromptSelector\\n\\nlogger = logging.getLogger(__name__)', metadata={'source': 'test_repo/libs/langchain/langchain/retrievers/web_research.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='class SearchQueries(BaseModel):\\n    \"\"\"Search queries to research for the user\\'s goal.\"\"\"\\n\\n    queries: List[str] = Field(\\n        ..., description=\"List of search queries to look up on Google\"\\n    )\\n\\n\\nDEFAULT_LLAMA_SEARCH_PROMPT = PromptTemplate(\\n    input_variables=[\"question\"],\\n    template=\"\"\"<<SYS>> \\\\n You are an assistant tasked with improving Google search \\\\\\nresults. \\\\n <</SYS>> \\\\n\\\\n [INST] Generate THREE Google search queries that \\\\\\nare similar to this question. The output should be a numbered list of questions \\\\\\nand each should have a question mark at the end: \\\\n\\\\n {question} [/INST]\"\"\",\\n)\\n\\nDEFAULT_SEARCH_PROMPT = PromptTemplate(\\n    input_variables=[\"question\"],\\n    template=\"\"\"You are an assistant tasked with improving Google search \\\\\\nresults. Generate THREE Google search queries that are similar to \\\\\\nthis question. The output should be a numbered list of questions and each \\\\\\nshould have a question mark at the end: {question}\"\"\",\\n)', metadata={'source': 'test_repo/libs/langchain/langchain/retrievers/web_research.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='class QuestionListOutputParser(BaseOutputParser[List[str]]):\\n    \"\"\"Output parser for a list of numbered questions.\"\"\"\\n\\n    def parse(self, text: str) -> List[str]:\\n        lines = re.findall(r\"\\\\d+\\\\..*?(?:\\\\n|$)\", text)\\n        return lines', metadata={'source': 'test_repo/libs/langchain/langchain/retrievers/web_research.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='class WebResearchRetriever(BaseRetriever):\\n    \"\"\"`Google Search API` retriever.\"\"\"\\n\\n    # Inputs\\n    vectorstore: VectorStore = Field(\\n        ..., description=\"Vector store for storing web pages\"\\n    )\\n    llm_chain: LLMChain\\n    search: GoogleSearchAPIWrapper = Field(..., description=\"Google Search API Wrapper\")\\n    num_search_results: int = Field(1, description=\"Number of pages per Google search\")\\n    text_splitter: TextSplitter = Field(\\n        RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=50),\\n        description=\"Text splitter for splitting web pages into chunks\",\\n    )\\n    url_database: List[str] = Field(\\n        default_factory=list, description=\"List of processed URLs\"\\n    )', metadata={'source': 'test_repo/libs/langchain/langchain/retrievers/web_research.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='@classmethod\\n    def from_llm(\\n        cls,\\n        vectorstore: VectorStore,\\n        llm: BaseLLM,\\n        search: GoogleSearchAPIWrapper,\\n        prompt: Optional[BasePromptTemplate] = None,\\n        num_search_results: int = 1,\\n        text_splitter: RecursiveCharacterTextSplitter = RecursiveCharacterTextSplitter(\\n            chunk_size=1500, chunk_overlap=150\\n        ),\\n    ) -> \"WebResearchRetriever\":\\n        \"\"\"Initialize from llm using default template.\\n\\n        Args:\\n            vectorstore: Vector store for storing web pages\\n            llm: llm for search question generation\\n            search: GoogleSearchAPIWrapper\\n            prompt: prompt to generating search questions\\n            num_search_results: Number of pages per Google search\\n            text_splitter: Text splitter for splitting web pages into chunks\\n\\n        Returns:\\n            WebResearchRetriever\\n        \"\"\"', metadata={'source': 'test_repo/libs/langchain/langchain/retrievers/web_research.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='Returns:\\n            WebResearchRetriever\\n        \"\"\"\\n\\n        if not prompt:\\n            QUESTION_PROMPT_SELECTOR = ConditionalPromptSelector(\\n                default_prompt=DEFAULT_SEARCH_PROMPT,\\n                conditionals=[\\n                    (lambda llm: isinstance(llm, LlamaCpp), DEFAULT_LLAMA_SEARCH_PROMPT)\\n                ],\\n            )\\n            prompt = QUESTION_PROMPT_SELECTOR.get_prompt(llm)\\n\\n        # Use chat model prompt\\n        llm_chain = LLMChain(\\n            llm=llm,\\n            prompt=prompt,\\n            output_parser=QuestionListOutputParser(),\\n        )\\n\\n        return cls(\\n            vectorstore=vectorstore,\\n            llm_chain=llm_chain,\\n            search=search,\\n            num_search_results=num_search_results,\\n            text_splitter=text_splitter,\\n        )', metadata={'source': 'test_repo/libs/langchain/langchain/retrievers/web_research.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='def clean_search_query(self, query: str) -> str:\\n        # Some search tools (e.g., Google) will\\n        # fail to return results if query has a\\n        # leading digit: 1. \"LangCh...\"\\n        # Check if the first character is a digit\\n        if query[0].isdigit():\\n            # Find the position of the first quote\\n            first_quote_pos = query.find(\\'\"\\')\\n            if first_quote_pos != -1:\\n                # Extract the part of the string after the quote\\n                query = query[first_quote_pos + 1 :]\\n                # Remove the trailing quote if present\\n                if query.endswith(\\'\"\\'):\\n                    query = query[:-1]\\n        return query.strip()\\n\\n    def search_tool(self, query: str, num_search_results: int = 1) -> List[dict]:\\n        \"\"\"Returns num_search_results pages per Google search.\"\"\"\\n        query_clean = self.clean_search_query(query)\\n        result = self.search.results(query_clean, num_search_results)\\n        return result', metadata={'source': 'test_repo/libs/langchain/langchain/retrievers/web_research.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='def _get_relevant_documents(\\n        self,\\n        query: str,\\n        *,\\n        run_manager: CallbackManagerForRetrieverRun,\\n    ) -> List[Document]:\\n        \"\"\"Search Google for documents related to the query input.\\n\\n        Args:\\n            query: user query\\n\\n        Returns:\\n            Relevant documents from all various urls.\\n        \"\"\"\\n\\n        # Get search questions\\n        logger.info(\"Generating questions for Google Search ...\")\\n        result = self.llm_chain({\"question\": query})\\n        logger.info(f\"Questions for Google Search (raw): {result}\")\\n        questions = result[\"text\"]\\n        logger.info(f\"Questions for Google Search: {questions}\")', metadata={'source': 'test_repo/libs/langchain/langchain/retrievers/web_research.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='# Get urls\\n        logger.info(\"Searching for relevant urls...\")\\n        urls_to_look = []\\n        for query in questions:\\n            # Google search\\n            search_results = self.search_tool(query, self.num_search_results)\\n            logger.info(\"Searching for relevant urls...\")\\n            logger.info(f\"Search results: {search_results}\")\\n            for res in search_results:\\n                if res.get(\"link\", None):\\n                    urls_to_look.append(res[\"link\"])\\n\\n        # Relevant urls\\n        urls = set(urls_to_look)\\n\\n        # Check for any new urls that we have not processed\\n        new_urls = list(urls.difference(self.url_database))', metadata={'source': 'test_repo/libs/langchain/langchain/retrievers/web_research.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='logger.info(f\"New URLs to load: {new_urls}\")\\n        # Load, split, and add new urls to vectorstore\\n        if new_urls:\\n            loader = AsyncHtmlLoader(new_urls, ignore_load_errors=True)\\n            html2text = Html2TextTransformer()\\n            logger.info(\"Indexing new urls...\")\\n            docs = loader.load()\\n            docs = list(html2text.transform_documents(docs))\\n            docs = self.text_splitter.split_documents(docs)\\n            self.vectorstore.add_documents(docs)\\n            self.url_database.extend(new_urls)\\n\\n        # Search for relevant splits\\n        # TODO: make this async\\n        logger.info(\"Grabbing most relevant splits from urls...\")\\n        docs = []\\n        for query in questions:\\n            docs.extend(self.vectorstore.similarity_search(query))', metadata={'source': 'test_repo/libs/langchain/langchain/retrievers/web_research.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='# Get unique docs\\n        unique_documents_dict = {\\n            (doc.page_content, tuple(sorted(doc.metadata.items()))): doc for doc in docs\\n        }\\n        unique_documents = list(unique_documents_dict.values())\\n        return unique_documents\\n\\n    async def _aget_relevant_documents(\\n        self,\\n        query: str,\\n        *,\\n        run_manager: AsyncCallbackManagerForRetrieverRun,\\n    ) -> List[Document]:\\n        raise NotImplementedError', metadata={'source': 'test_repo/libs/langchain/langchain/retrievers/web_research.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from langchain_community.retrievers.cohere_rag_retriever import (\\n    CohereRagRetriever,\\n)\\n\\n__all__ = [\"CohereRagRetriever\"]', metadata={'source': 'test_repo/libs/langchain/langchain/retrievers/cohere_rag_retriever.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='import asyncio\\nimport logging\\nfrom typing import List, Optional, Sequence\\n\\nfrom langchain_core.callbacks import (\\n    AsyncCallbackManagerForRetrieverRun,\\n    CallbackManagerForRetrieverRun,\\n)\\nfrom langchain_core.documents import Document\\nfrom langchain_core.language_models import BaseLanguageModel\\nfrom langchain_core.output_parsers import BaseOutputParser\\nfrom langchain_core.prompts.prompt import PromptTemplate\\nfrom langchain_core.retrievers import BaseRetriever\\n\\nfrom langchain.chains.llm import LLMChain\\n\\nlogger = logging.getLogger(__name__)', metadata={'source': 'test_repo/libs/langchain/langchain/retrievers/multi_query.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='class LineListOutputParser(BaseOutputParser[List[str]]):\\n    \"\"\"Output parser for a list of lines.\"\"\"\\n\\n    def parse(self, text: str) -> List[str]:\\n        lines = text.strip().split(\"\\\\n\")\\n        return lines\\n\\n\\n# Default prompt\\nDEFAULT_QUERY_PROMPT = PromptTemplate(\\n    input_variables=[\"question\"],\\n    template=\"\"\"You are an AI language model assistant. Your task is \\n    to generate 3 different versions of the given user \\n    question to retrieve relevant documents from a vector  database. \\n    By generating multiple perspectives on the user question, \\n    your goal is to help the user overcome some of the limitations \\n    of distance-based similarity search. Provide these alternative \\n    questions separated by newlines. Original question: {question}\"\"\",\\n)\\n\\n\\ndef _unique_documents(documents: Sequence[Document]) -> List[Document]:\\n    return [doc for i, doc in enumerate(documents) if doc not in documents[:i]]', metadata={'source': 'test_repo/libs/langchain/langchain/retrievers/multi_query.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='class MultiQueryRetriever(BaseRetriever):\\n    \"\"\"Given a query, use an LLM to write a set of queries.\\n\\n    Retrieve docs for each query. Return the unique union of all retrieved docs.\\n    \"\"\"\\n\\n    retriever: BaseRetriever\\n    llm_chain: LLMChain\\n    verbose: bool = True\\n    parser_key: str = \"lines\"\\n    \"\"\"DEPRECATED. parser_key is no longer used and should not be specified.\"\"\"\\n    include_original: bool = False\\n    \"\"\"Whether to include the original query in the list of generated queries.\"\"\"\\n\\n    @classmethod\\n    def from_llm(\\n        cls,\\n        retriever: BaseRetriever,\\n        llm: BaseLanguageModel,\\n        prompt: PromptTemplate = DEFAULT_QUERY_PROMPT,\\n        parser_key: Optional[str] = None,\\n        include_original: bool = False,\\n    ) -> \"MultiQueryRetriever\":\\n        \"\"\"Initialize from llm using default template.', metadata={'source': 'test_repo/libs/langchain/langchain/retrievers/multi_query.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='Args:\\n            retriever: retriever to query documents from\\n            llm: llm for query generation using DEFAULT_QUERY_PROMPT\\n            include_original: Whether to include the original query in the list of\\n                generated queries.\\n\\n        Returns:\\n            MultiQueryRetriever\\n        \"\"\"\\n        output_parser = LineListOutputParser()\\n        llm_chain = LLMChain(llm=llm, prompt=prompt, output_parser=output_parser)\\n        return cls(\\n            retriever=retriever,\\n            llm_chain=llm_chain,\\n            include_original=include_original,\\n        )\\n\\n    async def _aget_relevant_documents(\\n        self,\\n        query: str,\\n        *,\\n        run_manager: AsyncCallbackManagerForRetrieverRun,\\n    ) -> List[Document]:\\n        \"\"\"Get relevant documents given a user query.\\n\\n        Args:\\n            question: user query', metadata={'source': 'test_repo/libs/langchain/langchain/retrievers/multi_query.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='Args:\\n            question: user query\\n\\n        Returns:\\n            Unique union of relevant documents from all generated queries\\n        \"\"\"\\n        queries = await self.agenerate_queries(query, run_manager)\\n        if self.include_original:\\n            queries.append(query)\\n        documents = await self.aretrieve_documents(queries, run_manager)\\n        return self.unique_union(documents)\\n\\n    async def agenerate_queries(\\n        self, question: str, run_manager: AsyncCallbackManagerForRetrieverRun\\n    ) -> List[str]:\\n        \"\"\"Generate queries based upon user input.\\n\\n        Args:\\n            question: user query', metadata={'source': 'test_repo/libs/langchain/langchain/retrievers/multi_query.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='Args:\\n            question: user query\\n\\n        Returns:\\n            List of LLM generated queries that are similar to the user input\\n        \"\"\"\\n        response = await self.llm_chain.acall(\\n            inputs={\"question\": question}, callbacks=run_manager.get_child()\\n        )\\n        lines = response[\"text\"]\\n        if self.verbose:\\n            logger.info(f\"Generated queries: {lines}\")\\n        return lines\\n\\n    async def aretrieve_documents(\\n        self, queries: List[str], run_manager: AsyncCallbackManagerForRetrieverRun\\n    ) -> List[Document]:\\n        \"\"\"Run all LLM generated queries.\\n\\n        Args:\\n            queries: query list', metadata={'source': 'test_repo/libs/langchain/langchain/retrievers/multi_query.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='Args:\\n            queries: query list\\n\\n        Returns:\\n            List of retrieved Documents\\n        \"\"\"\\n        document_lists = await asyncio.gather(\\n            *(\\n                self.retriever.aget_relevant_documents(\\n                    query, callbacks=run_manager.get_child()\\n                )\\n                for query in queries\\n            )\\n        )\\n        return [doc for docs in document_lists for doc in docs]\\n\\n    def _get_relevant_documents(\\n        self,\\n        query: str,\\n        *,\\n        run_manager: CallbackManagerForRetrieverRun,\\n    ) -> List[Document]:\\n        \"\"\"Get relevant documents given a user query.\\n\\n        Args:\\n            question: user query', metadata={'source': 'test_repo/libs/langchain/langchain/retrievers/multi_query.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='Args:\\n            question: user query\\n\\n        Returns:\\n            Unique union of relevant documents from all generated queries\\n        \"\"\"\\n        queries = self.generate_queries(query, run_manager)\\n        if self.include_original:\\n            queries.append(query)\\n        documents = self.retrieve_documents(queries, run_manager)\\n        return self.unique_union(documents)\\n\\n    def generate_queries(\\n        self, question: str, run_manager: CallbackManagerForRetrieverRun\\n    ) -> List[str]:\\n        \"\"\"Generate queries based upon user input.\\n\\n        Args:\\n            question: user query\\n\\n        Returns:\\n            List of LLM generated queries that are similar to the user input\\n        \"\"\"\\n        response = self.llm_chain(\\n            {\"question\": question}, callbacks=run_manager.get_child()\\n        )\\n        lines = response[\"text\"]\\n        if self.verbose:\\n            logger.info(f\"Generated queries: {lines}\")\\n        return lines', metadata={'source': 'test_repo/libs/langchain/langchain/retrievers/multi_query.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='def retrieve_documents(\\n        self, queries: List[str], run_manager: CallbackManagerForRetrieverRun\\n    ) -> List[Document]:\\n        \"\"\"Run all LLM generated queries.\\n\\n        Args:\\n            queries: query list\\n\\n        Returns:\\n            List of retrieved Documents\\n        \"\"\"\\n        documents = []\\n        for query in queries:\\n            docs = self.retriever.get_relevant_documents(\\n                query, callbacks=run_manager.get_child()\\n            )\\n            documents.extend(docs)\\n        return documents\\n\\n    def unique_union(self, documents: List[Document]) -> List[Document]:\\n        \"\"\"Get unique Documents.\\n\\n        Args:\\n            documents: List of retrieved Documents\\n\\n        Returns:\\n            List of unique retrieved Documents\\n        \"\"\"\\n        return _unique_documents(documents)', metadata={'source': 'test_repo/libs/langchain/langchain/retrievers/multi_query.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='\"\"\"\\nEnsemble retriever that ensemble the results of \\nmultiple retrievers by using weighted  Reciprocal Rank Fusion\\n\"\"\"\\nimport asyncio\\nfrom typing import Any, Dict, List, Optional, cast\\n\\nfrom langchain_core.callbacks import (\\n    AsyncCallbackManagerForRetrieverRun,\\n    CallbackManagerForRetrieverRun,\\n)\\nfrom langchain_core.documents import Document\\nfrom langchain_core.load.dump import dumpd\\nfrom langchain_core.pydantic_v1 import root_validator\\nfrom langchain_core.retrievers import BaseRetriever, RetrieverLike\\nfrom langchain_core.runnables import RunnableConfig\\nfrom langchain_core.runnables.config import ensure_config, patch_config\\nfrom langchain_core.runnables.utils import (\\n    ConfigurableFieldSpec,\\n    get_unique_config_specs,\\n)', metadata={'source': 'test_repo/libs/langchain/langchain/retrievers/ensemble.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='class EnsembleRetriever(BaseRetriever):\\n    \"\"\"Retriever that ensembles the multiple retrievers.\\n\\n    It uses a rank fusion.\\n\\n    Args:\\n        retrievers: A list of retrievers to ensemble.\\n        weights: A list of weights corresponding to the retrievers. Defaults to equal\\n            weighting for all retrievers.\\n        c: A constant added to the rank, controlling the balance between the importance\\n            of high-ranked items and the consideration given to lower-ranked items.\\n            Default is 60.\\n    \"\"\"\\n\\n    retrievers: List[RetrieverLike]\\n    weights: List[float]\\n    c: int = 60\\n\\n    @property\\n    def config_specs(self) -> List[ConfigurableFieldSpec]:\\n        \"\"\"List configurable fields for this runnable.\"\"\"\\n        return get_unique_config_specs(\\n            spec for retriever in self.retrievers for spec in retriever.config_specs\\n        )', metadata={'source': 'test_repo/libs/langchain/langchain/retrievers/ensemble.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='@root_validator(pre=True)\\n    def set_weights(cls, values: Dict[str, Any]) -> Dict[str, Any]:\\n        if not values.get(\"weights\"):\\n            n_retrievers = len(values[\"retrievers\"])\\n            values[\"weights\"] = [1 / n_retrievers] * n_retrievers\\n        return values\\n\\n    def invoke(\\n        self, input: str, config: Optional[RunnableConfig] = None, **kwargs: Any\\n    ) -> List[Document]:\\n        from langchain_core.callbacks import CallbackManager', metadata={'source': 'test_repo/libs/langchain/langchain/retrievers/ensemble.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='config = ensure_config(config)\\n        callback_manager = CallbackManager.configure(\\n            config.get(\"callbacks\"),\\n            None,\\n            verbose=kwargs.get(\"verbose\", False),\\n            inheritable_tags=config.get(\"tags\", []),\\n            local_tags=self.tags,\\n            inheritable_metadata=config.get(\"metadata\", {}),\\n            local_metadata=self.metadata,\\n        )\\n        run_manager = callback_manager.on_retriever_start(\\n            dumpd(self),\\n            input,\\n            name=config.get(\"run_name\"),\\n            **kwargs,\\n        )\\n        try:\\n            result = self.rank_fusion(input, run_manager=run_manager, config=config)\\n        except Exception as e:\\n            run_manager.on_retriever_error(e)\\n            raise e\\n        else:\\n            run_manager.on_retriever_end(\\n                result,\\n                **kwargs,\\n            )\\n            return result', metadata={'source': 'test_repo/libs/langchain/langchain/retrievers/ensemble.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='async def ainvoke(\\n        self, input: str, config: Optional[RunnableConfig] = None, **kwargs: Any\\n    ) -> List[Document]:\\n        from langchain_core.callbacks import AsyncCallbackManager', metadata={'source': 'test_repo/libs/langchain/langchain/retrievers/ensemble.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='config = ensure_config(config)\\n        callback_manager = AsyncCallbackManager.configure(\\n            config.get(\"callbacks\"),\\n            None,\\n            verbose=kwargs.get(\"verbose\", False),\\n            inheritable_tags=config.get(\"tags\", []),\\n            local_tags=self.tags,\\n            inheritable_metadata=config.get(\"metadata\", {}),\\n            local_metadata=self.metadata,\\n        )\\n        run_manager = await callback_manager.on_retriever_start(\\n            dumpd(self),\\n            input,\\n            name=config.get(\"run_name\"),\\n            **kwargs,\\n        )\\n        try:\\n            result = await self.arank_fusion(\\n                input, run_manager=run_manager, config=config\\n            )\\n        except Exception as e:\\n            await run_manager.on_retriever_error(e)\\n            raise e\\n        else:\\n            await run_manager.on_retriever_end(\\n                result,\\n                **kwargs,\\n            )\\n            return result', metadata={'source': 'test_repo/libs/langchain/langchain/retrievers/ensemble.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='def _get_relevant_documents(\\n        self,\\n        query: str,\\n        *,\\n        run_manager: CallbackManagerForRetrieverRun,\\n    ) -> List[Document]:\\n        \"\"\"\\n        Get the relevant documents for a given query.\\n\\n        Args:\\n            query: The query to search for.\\n\\n        Returns:\\n            A list of reranked documents.\\n        \"\"\"\\n\\n        # Get fused result of the retrievers.\\n        fused_documents = self.rank_fusion(query, run_manager)\\n\\n        return fused_documents\\n\\n    async def _aget_relevant_documents(\\n        self,\\n        query: str,\\n        *,\\n        run_manager: AsyncCallbackManagerForRetrieverRun,\\n    ) -> List[Document]:\\n        \"\"\"\\n        Asynchronously get the relevant documents for a given query.\\n\\n        Args:\\n            query: The query to search for.\\n\\n        Returns:\\n            A list of reranked documents.\\n        \"\"\"\\n\\n        # Get fused result of the retrievers.\\n        fused_documents = await self.arank_fusion(query, run_manager)', metadata={'source': 'test_repo/libs/langchain/langchain/retrievers/ensemble.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='return fused_documents\\n\\n    def rank_fusion(\\n        self,\\n        query: str,\\n        run_manager: CallbackManagerForRetrieverRun,\\n        *,\\n        config: Optional[RunnableConfig] = None,\\n    ) -> List[Document]:\\n        \"\"\"\\n        Retrieve the results of the retrievers and use rank_fusion_func to get\\n        the final result.\\n\\n        Args:\\n            query: The query to search for.\\n\\n        Returns:\\n            A list of reranked documents.\\n        \"\"\"\\n\\n        # Get the results of all retrievers.\\n        retriever_docs = [\\n            retriever.invoke(\\n                query,\\n                patch_config(\\n                    config, callbacks=run_manager.get_child(tag=f\"retriever_{i+1}\")\\n                ),\\n            )\\n            for i, retriever in enumerate(self.retrievers)\\n        ]', metadata={'source': 'test_repo/libs/langchain/langchain/retrievers/ensemble.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='# Enforce that retrieved docs are Documents for each list in retriever_docs\\n        for i in range(len(retriever_docs)):\\n            retriever_docs[i] = [\\n                Document(page_content=cast(str, doc)) if isinstance(doc, str) else doc\\n                for doc in retriever_docs[i]\\n            ]\\n\\n        # apply rank fusion\\n        fused_documents = self.weighted_reciprocal_rank(retriever_docs)\\n\\n        return fused_documents\\n\\n    async def arank_fusion(\\n        self,\\n        query: str,\\n        run_manager: AsyncCallbackManagerForRetrieverRun,\\n        *,\\n        config: Optional[RunnableConfig] = None,\\n    ) -> List[Document]:\\n        \"\"\"\\n        Asynchronously retrieve the results of the retrievers\\n        and use rank_fusion_func to get the final result.\\n\\n        Args:\\n            query: The query to search for.\\n\\n        Returns:\\n            A list of reranked documents.\\n        \"\"\"', metadata={'source': 'test_repo/libs/langchain/langchain/retrievers/ensemble.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='Returns:\\n            A list of reranked documents.\\n        \"\"\"\\n\\n        # Get the results of all retrievers.\\n        retriever_docs = await asyncio.gather(\\n            *[\\n                retriever.ainvoke(\\n                    query,\\n                    patch_config(\\n                        config, callbacks=run_manager.get_child(tag=f\"retriever_{i+1}\")\\n                    ),\\n                )\\n                for i, retriever in enumerate(self.retrievers)\\n            ]\\n        )\\n\\n        # Enforce that retrieved docs are Documents for each list in retriever_docs\\n        for i in range(len(retriever_docs)):\\n            retriever_docs[i] = [\\n                Document(page_content=doc) if not isinstance(doc, Document) else doc\\n                for doc in retriever_docs[i]\\n            ]\\n\\n        # apply rank fusion\\n        fused_documents = self.weighted_reciprocal_rank(retriever_docs)\\n\\n        return fused_documents', metadata={'source': 'test_repo/libs/langchain/langchain/retrievers/ensemble.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='return fused_documents\\n\\n    def weighted_reciprocal_rank(\\n        self, doc_lists: List[List[Document]]\\n    ) -> List[Document]:\\n        \"\"\"\\n        Perform weighted Reciprocal Rank Fusion on multiple rank lists.\\n        You can find more details about RRF here:\\n        https://plg.uwaterloo.ca/~gvcormac/cormacksigir09-rrf.pdf\\n\\n        Args:\\n            doc_lists: A list of rank lists, where each rank list contains unique items.\\n\\n        Returns:\\n            list: The final aggregated list of items sorted by their weighted RRF\\n                    scores in descending order.\\n        \"\"\"\\n        if len(doc_lists) != len(self.weights):\\n            raise ValueError(\\n                \"Number of rank lists must be equal to the number of weights.\"\\n            )\\n\\n        # Create a union of all unique documents in the input doc_lists\\n        all_documents = set()\\n        for doc_list in doc_lists:\\n            for doc in doc_list:\\n                all_documents.add(doc.page_content)', metadata={'source': 'test_repo/libs/langchain/langchain/retrievers/ensemble.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='# Initialize the RRF score dictionary for each document\\n        rrf_score_dic = {doc: 0.0 for doc in all_documents}\\n\\n        # Calculate RRF scores for each document\\n        for doc_list, weight in zip(doc_lists, self.weights):\\n            for rank, doc in enumerate(doc_list, start=1):\\n                rrf_score = weight * (1 / (rank + self.c))\\n                rrf_score_dic[doc.page_content] += rrf_score\\n\\n        # Sort documents by their RRF scores in descending order\\n        sorted_documents = sorted(\\n            rrf_score_dic.keys(), key=lambda x: rrf_score_dic[x], reverse=True\\n        )\\n\\n        # Map the sorted page_content back to the original document objects\\n        page_content_to_doc_map = {\\n            doc.page_content: doc for doc_list in doc_lists for doc in doc_list\\n        }\\n        sorted_docs = [\\n            page_content_to_doc_map[page_content] for page_content in sorted_documents\\n        ]\\n\\n        return sorted_docs', metadata={'source': 'test_repo/libs/langchain/langchain/retrievers/ensemble.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from langchain_community.retrievers.weaviate_hybrid_search import (\\n    WeaviateHybridSearchRetriever,\\n)\\n\\n__all__ = [\"WeaviateHybridSearchRetriever\"]', metadata={'source': 'test_repo/libs/langchain/langchain/retrievers/weaviate_hybrid_search.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from langchain_community.retrievers.tavily_search_api import (\\n    SearchDepth,\\n    TavilySearchAPIRetriever,\\n)\\n\\n__all__ = [\"SearchDepth\", \"TavilySearchAPIRetriever\"]', metadata={'source': 'test_repo/libs/langchain/langchain/retrievers/tavily_search_api.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from langchain_community.retrievers.bm25 import (\\n    BM25Retriever,\\n    default_preprocessing_func,\\n)\\n\\n__all__ = [\"default_preprocessing_func\", \"BM25Retriever\"]', metadata={'source': 'test_repo/libs/langchain/langchain/retrievers/bm25.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from langchain_community.retrievers.kendra import (\\n    AdditionalResultAttribute,\\n    AdditionalResultAttributeValue,\\n    AmazonKendraRetriever,\\n    DocumentAttribute,\\n    DocumentAttributeValue,\\n    DocumentAttributeValueType,\\n    Highlight,\\n    QueryResult,\\n    QueryResultItem,\\n    ResultItem,\\n    RetrieveResult,\\n    RetrieveResultItem,\\n    TextWithHighLights,\\n    clean_excerpt,\\n    combined_text,\\n)\\n\\n__all__ = [\\n    \"clean_excerpt\",\\n    \"combined_text\",\\n    \"DocumentAttributeValueType\",\\n    \"Highlight\",\\n    \"TextWithHighLights\",\\n    \"AdditionalResultAttributeValue\",\\n    \"AdditionalResultAttribute\",\\n    \"DocumentAttributeValue\",\\n    \"DocumentAttribute\",\\n    \"ResultItem\",\\n    \"QueryResultItem\",\\n    \"RetrieveResultItem\",\\n    \"QueryResult\",\\n    \"RetrieveResult\",\\n    \"AmazonKendraRetriever\",\\n]', metadata={'source': 'test_repo/libs/langchain/langchain/retrievers/kendra.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from langchain_community.retrievers.azure_cognitive_search import (\\n    AzureCognitiveSearchRetriever,\\n)\\n\\n__all__ = [\"AzureCognitiveSearchRetriever\"]', metadata={'source': 'test_repo/libs/langchain/langchain/retrievers/azure_cognitive_search.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from langchain.retrievers.pubmed import PubMedRetriever\\n\\n__all__ = [\\n    \"PubMedRetriever\",\\n]', metadata={'source': 'test_repo/libs/langchain/langchain/retrievers/pupmed.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from typing import Any, List\\n\\nfrom langchain_core.callbacks import (\\n    AsyncCallbackManagerForRetrieverRun,\\n    CallbackManagerForRetrieverRun,\\n)\\nfrom langchain_core.documents import Document\\nfrom langchain_core.retrievers import BaseRetriever\\n\\nfrom langchain.retrievers.document_compressors.base import (\\n    BaseDocumentCompressor,\\n)', metadata={'source': 'test_repo/libs/langchain/langchain/retrievers/contextual_compression.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='class ContextualCompressionRetriever(BaseRetriever):\\n    \"\"\"Retriever that wraps a base retriever and compresses the results.\"\"\"\\n\\n    base_compressor: BaseDocumentCompressor\\n    \"\"\"Compressor for compressing retrieved documents.\"\"\"\\n\\n    base_retriever: BaseRetriever\\n    \"\"\"Base Retriever to use for getting relevant documents.\"\"\"\\n\\n    class Config:\\n        \"\"\"Configuration for this pydantic object.\"\"\"\\n\\n        arbitrary_types_allowed = True\\n\\n    def _get_relevant_documents(\\n        self,\\n        query: str,\\n        *,\\n        run_manager: CallbackManagerForRetrieverRun,\\n        **kwargs: Any,\\n    ) -> List[Document]:\\n        \"\"\"Get documents relevant for a query.\\n\\n        Args:\\n            query: string to find relevant documents for', metadata={'source': 'test_repo/libs/langchain/langchain/retrievers/contextual_compression.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='Args:\\n            query: string to find relevant documents for\\n\\n        Returns:\\n            Sequence of relevant documents\\n        \"\"\"\\n        docs = self.base_retriever.get_relevant_documents(\\n            query, callbacks=run_manager.get_child(), **kwargs\\n        )\\n        if docs:\\n            compressed_docs = self.base_compressor.compress_documents(\\n                docs, query, callbacks=run_manager.get_child()\\n            )\\n            return list(compressed_docs)\\n        else:\\n            return []\\n\\n    async def _aget_relevant_documents(\\n        self,\\n        query: str,\\n        *,\\n        run_manager: AsyncCallbackManagerForRetrieverRun,\\n        **kwargs: Any,\\n    ) -> List[Document]:\\n        \"\"\"Get documents relevant for a query.\\n\\n        Args:\\n            query: string to find relevant documents for', metadata={'source': 'test_repo/libs/langchain/langchain/retrievers/contextual_compression.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='Args:\\n            query: string to find relevant documents for\\n\\n        Returns:\\n            List of relevant documents\\n        \"\"\"\\n        docs = await self.base_retriever.aget_relevant_documents(\\n            query, callbacks=run_manager.get_child(), **kwargs\\n        )\\n        if docs:\\n            compressed_docs = await self.base_compressor.acompress_documents(\\n                docs, query, callbacks=run_manager.get_child()\\n            )\\n            return list(compressed_docs)\\n        else:\\n            return []', metadata={'source': 'test_repo/libs/langchain/langchain/retrievers/contextual_compression.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from langchain_community.retrievers.vespa_retriever import VespaRetriever\\n\\n__all__ = [\"VespaRetriever\"]', metadata={'source': 'test_repo/libs/langchain/langchain/retrievers/vespa_retriever.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from langchain_community.retrievers.pinecone_hybrid_search import (\\n    PineconeHybridSearchRetriever,\\n)\\n\\n__all__ = [\"PineconeHybridSearchRetriever\"]', metadata={'source': 'test_repo/libs/langchain/langchain/retrievers/pinecone_hybrid_search.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from langchain_community.retrievers.google_vertex_ai_search import (\\n    GoogleCloudEnterpriseSearchRetriever,\\n    GoogleVertexAIMultiTurnSearchRetriever,\\n    GoogleVertexAISearchRetriever,\\n)\\n\\n__all__ = [\\n    \"GoogleVertexAISearchRetriever\",\\n    \"GoogleVertexAIMultiTurnSearchRetriever\",\\n    \"GoogleCloudEnterpriseSearchRetriever\",\\n]', metadata={'source': 'test_repo/libs/langchain/langchain/retrievers/google_vertex_ai_search.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='import asyncio\\nfrom typing import List\\n\\nfrom langchain_core.callbacks import (\\n    AsyncCallbackManagerForRetrieverRun,\\n    CallbackManagerForRetrieverRun,\\n)\\nfrom langchain_core.documents import Document\\nfrom langchain_core.retrievers import BaseRetriever', metadata={'source': 'test_repo/libs/langchain/langchain/retrievers/merger_retriever.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='class MergerRetriever(BaseRetriever):\\n    \"\"\"Retriever that merges the results of multiple retrievers.\"\"\"\\n\\n    retrievers: List[BaseRetriever]\\n    \"\"\"A list of retrievers to merge.\"\"\"\\n\\n    def _get_relevant_documents(\\n        self,\\n        query: str,\\n        *,\\n        run_manager: CallbackManagerForRetrieverRun,\\n    ) -> List[Document]:\\n        \"\"\"\\n        Get the relevant documents for a given query.\\n\\n        Args:\\n            query: The query to search for.\\n\\n        Returns:\\n            A list of relevant documents.\\n        \"\"\"\\n\\n        # Merge the results of the retrievers.\\n        merged_documents = self.merge_documents(query, run_manager)\\n\\n        return merged_documents\\n\\n    async def _aget_relevant_documents(\\n        self,\\n        query: str,\\n        *,\\n        run_manager: AsyncCallbackManagerForRetrieverRun,\\n    ) -> List[Document]:\\n        \"\"\"\\n        Asynchronously get the relevant documents for a given query.\\n\\n        Args:\\n            query: The query to search for.', metadata={'source': 'test_repo/libs/langchain/langchain/retrievers/merger_retriever.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='Args:\\n            query: The query to search for.\\n\\n        Returns:\\n            A list of relevant documents.\\n        \"\"\"\\n\\n        # Merge the results of the retrievers.\\n        merged_documents = await self.amerge_documents(query, run_manager)\\n\\n        return merged_documents\\n\\n    def merge_documents(\\n        self, query: str, run_manager: CallbackManagerForRetrieverRun\\n    ) -> List[Document]:\\n        \"\"\"\\n        Merge the results of the retrievers.\\n\\n        Args:\\n            query: The query to search for.\\n\\n        Returns:\\n            A list of merged documents.\\n        \"\"\"\\n\\n        # Get the results of all retrievers.\\n        retriever_docs = [\\n            retriever.get_relevant_documents(\\n                query, callbacks=run_manager.get_child(\"retriever_{}\".format(i + 1))\\n            )\\n            for i, retriever in enumerate(self.retrievers)\\n        ]', metadata={'source': 'test_repo/libs/langchain/langchain/retrievers/merger_retriever.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='# Merge the results of the retrievers.\\n        merged_documents = []\\n        max_docs = max(len(docs) for docs in retriever_docs)\\n        for i in range(max_docs):\\n            for retriever, doc in zip(self.retrievers, retriever_docs):\\n                if i < len(doc):\\n                    merged_documents.append(doc[i])\\n\\n        return merged_documents\\n\\n    async def amerge_documents(\\n        self, query: str, run_manager: AsyncCallbackManagerForRetrieverRun\\n    ) -> List[Document]:\\n        \"\"\"\\n        Asynchronously merge the results of the retrievers.\\n\\n        Args:\\n            query: The query to search for.\\n\\n        Returns:\\n            A list of merged documents.\\n        \"\"\"', metadata={'source': 'test_repo/libs/langchain/langchain/retrievers/merger_retriever.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='Returns:\\n            A list of merged documents.\\n        \"\"\"\\n\\n        # Get the results of all retrievers.\\n        retriever_docs = await asyncio.gather(\\n            *(\\n                retriever.aget_relevant_documents(\\n                    query, callbacks=run_manager.get_child(\"retriever_{}\".format(i + 1))\\n                )\\n                for i, retriever in enumerate(self.retrievers)\\n            )\\n        )\\n\\n        # Merge the results of the retrievers.\\n        merged_documents = []\\n        max_docs = max(len(docs) for docs in retriever_docs)\\n        for i in range(max_docs):\\n            for retriever, doc in zip(self.retrievers, retriever_docs):\\n                if i < len(doc):\\n                    merged_documents.append(doc[i])\\n\\n        return merged_documents', metadata={'source': 'test_repo/libs/langchain/langchain/retrievers/merger_retriever.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from langchain_community.retrievers.embedchain import EmbedchainRetriever\\n\\n__all__ = [\"EmbedchainRetriever\"]', metadata={'source': 'test_repo/libs/langchain/langchain/retrievers/embedchain.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from langchain_community.retrievers.zep import SearchScope, SearchType, ZepRetriever\\n\\n__all__ = [\"SearchScope\", \"SearchType\", \"ZepRetriever\"]', metadata={'source': 'test_repo/libs/langchain/langchain/retrievers/zep.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from langchain_community.retrievers.pubmed import PubMedRetriever\\n\\n__all__ = [\"PubMedRetriever\"]', metadata={'source': 'test_repo/libs/langchain/langchain/retrievers/pubmed.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from langchain_community.retrievers.remote_retriever import RemoteLangChainRetriever\\n\\n__all__ = [\"RemoteLangChainRetriever\"]', metadata={'source': 'test_repo/libs/langchain/langchain/retrievers/remote_retriever.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from langchain_community.retrievers.bedrock import (\\n    AmazonKnowledgeBasesRetriever,\\n    RetrievalConfig,\\n    VectorSearchConfig,\\n)\\n\\n__all__ = [\"VectorSearchConfig\", \"RetrievalConfig\", \"AmazonKnowledgeBasesRetriever\"]', metadata={'source': 'test_repo/libs/langchain/langchain/retrievers/bedrock.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from langchain_community.retrievers.zilliz import ZillizRetreiver, ZillizRetriever\\n\\n__all__ = [\"ZillizRetriever\", \"ZillizRetreiver\"]', metadata={'source': 'test_repo/libs/langchain/langchain/retrievers/zilliz.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from langchain_community.retrievers.milvus import MilvusRetreiver, MilvusRetriever\\n\\n__all__ = [\"MilvusRetriever\", \"MilvusRetreiver\"]', metadata={'source': 'test_repo/libs/langchain/langchain/retrievers/milvus.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from langchain_community.retrievers.tfidf import TFIDFRetriever\\n\\n__all__ = [\"TFIDFRetriever\"]', metadata={'source': 'test_repo/libs/langchain/langchain/retrievers/tfidf.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from langchain_community.retrievers.databerry import DataberryRetriever\\n\\n__all__ = [\"DataberryRetriever\"]', metadata={'source': 'test_repo/libs/langchain/langchain/retrievers/databerry.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from langchain_community.retrievers.kay import KayAiRetriever\\n\\n__all__ = [\"KayAiRetriever\"]', metadata={'source': 'test_repo/libs/langchain/langchain/retrievers/kay.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='import uuid\\nfrom typing import List, Optional, Sequence\\n\\nfrom langchain_core.documents import Document\\nfrom langchain_text_splitters import TextSplitter\\n\\nfrom langchain.retrievers import MultiVectorRetriever', metadata={'source': 'test_repo/libs/langchain/langchain/retrievers/parent_document_retriever.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='class ParentDocumentRetriever(MultiVectorRetriever):\\n    \"\"\"Retrieve small chunks then retrieve their parent documents.\\n\\n    When splitting documents for retrieval, there are often conflicting desires:\\n\\n    1. You may want to have small documents, so that their embeddings can most\\n        accurately reflect their meaning. If too long, then the embeddings can\\n        lose meaning.\\n    2. You want to have long enough documents that the context of each chunk is\\n        retained.\\n\\n    The ParentDocumentRetriever strikes that balance by splitting and storing\\n    small chunks of data. During retrieval, it first fetches the small chunks\\n    but then looks up the parent ids for those chunks and returns those larger\\n    documents.\\n\\n    Note that \"parent document\" refers to the document that a small chunk\\n    originated from. This can either be the whole raw document OR a larger\\n    chunk.\\n\\n    Examples:\\n\\n        .. code-block:: python', metadata={'source': 'test_repo/libs/langchain/langchain/retrievers/parent_document_retriever.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='Examples:\\n\\n        .. code-block:: python\\n\\n            from langchain_community.embeddings import OpenAIEmbeddings\\n            from langchain_community.vectorstores import Chroma\\n            from langchain_text_splitters import RecursiveCharacterTextSplitter\\n            from langchain.storage import InMemoryStore\\n\\n            # This text splitter is used to create the parent documents\\n            parent_splitter = RecursiveCharacterTextSplitter(chunk_size=2000, add_start_index=True)\\n            # This text splitter is used to create the child documents\\n            # It should create documents smaller than the parent\\n            child_splitter = RecursiveCharacterTextSplitter(chunk_size=400, add_start_index=True)\\n            # The vectorstore to use to index the child chunks\\n            vectorstore = Chroma(embedding_function=OpenAIEmbeddings())\\n            # The storage layer for the parent documents\\n            store = InMemoryStore()', metadata={'source': 'test_repo/libs/langchain/langchain/retrievers/parent_document_retriever.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='# Initialize the retriever\\n            retriever = ParentDocumentRetriever(\\n                vectorstore=vectorstore,\\n                docstore=store,\\n                child_splitter=child_splitter,\\n                parent_splitter=parent_splitter,\\n            )\\n    \"\"\"  # noqa: E501\\n\\n    child_splitter: TextSplitter\\n    \"\"\"The text splitter to use to create child documents.\"\"\"\\n\\n    \"\"\"The key to use to track the parent id. This will be stored in the\\n    metadata of child documents.\"\"\"\\n    parent_splitter: Optional[TextSplitter] = None\\n    \"\"\"The text splitter to use to create parent documents.\\n    If none, then the parent documents will be the raw documents passed in.\"\"\"\\n\\n    child_metadata_fields: Optional[Sequence[str]] = None\\n    \"\"\"Metadata fields to leave in child documents. If None, leave all parent document \\n        metadata.\\n    \"\"\"', metadata={'source': 'test_repo/libs/langchain/langchain/retrievers/parent_document_retriever.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='def add_documents(\\n        self,\\n        documents: List[Document],\\n        ids: Optional[List[str]] = None,\\n        add_to_docstore: bool = True,\\n    ) -> None:\\n        \"\"\"Adds documents to the docstore and vectorstores.', metadata={'source': 'test_repo/libs/langchain/langchain/retrievers/parent_document_retriever.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='Args:\\n            documents: List of documents to add\\n            ids: Optional list of ids for documents. If provided should be the same\\n                length as the list of documents. Can be provided if parent documents\\n                are already in the document store and you don\\'t want to re-add\\n                to the docstore. If not provided, random UUIDs will be used as\\n                ids.\\n            add_to_docstore: Boolean of whether to add documents to docstore.\\n                This can be false if and only if `ids` are provided. You may want\\n                to set this to False if the documents are already in the docstore\\n                and you don\\'t want to re-add them.\\n        \"\"\"\\n        if self.parent_splitter is not None:\\n            documents = self.parent_splitter.split_documents(documents)\\n        if ids is None:\\n            doc_ids = [str(uuid.uuid4()) for _ in documents]\\n            if not add_to_docstore:\\n                raise ValueError(', metadata={'source': 'test_repo/libs/langchain/langchain/retrievers/parent_document_retriever.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='if not add_to_docstore:\\n                raise ValueError(\\n                    \"If ids are not passed in, `add_to_docstore` MUST be True\"\\n                )\\n        else:\\n            if len(documents) != len(ids):\\n                raise ValueError(\\n                    \"Got uneven list of documents and ids. \"\\n                    \"If `ids` is provided, should be same length as `documents`.\"\\n                )\\n            doc_ids = ids', metadata={'source': 'test_repo/libs/langchain/langchain/retrievers/parent_document_retriever.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='docs = []\\n        full_docs = []\\n        for i, doc in enumerate(documents):\\n            _id = doc_ids[i]\\n            sub_docs = self.child_splitter.split_documents([doc])\\n            if self.child_metadata_fields is not None:\\n                for _doc in sub_docs:\\n                    _doc.metadata = {\\n                        k: _doc.metadata[k] for k in self.child_metadata_fields\\n                    }\\n            for _doc in sub_docs:\\n                _doc.metadata[self.id_key] = _id\\n            docs.extend(sub_docs)\\n            full_docs.append((_id, doc))\\n        self.vectorstore.add_documents(docs)\\n        if add_to_docstore:\\n            self.docstore.mset(full_docs)', metadata={'source': 'test_repo/libs/langchain/langchain/retrievers/parent_document_retriever.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from langchain_community.retrievers.llama_index import (\\n    LlamaIndexGraphRetriever,\\n    LlamaIndexRetriever,\\n)\\n\\n__all__ = [\"LlamaIndexRetriever\", \"LlamaIndexGraphRetriever\"]', metadata={'source': 'test_repo/libs/langchain/langchain/retrievers/llama_index.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from langchain_community.retrievers.svm import SVMRetriever\\n\\n__all__ = [\"SVMRetriever\"]', metadata={'source': 'test_repo/libs/langchain/langchain/retrievers/svm.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='import datetime\\nfrom copy import deepcopy\\nfrom typing import Any, Dict, List, Optional, Tuple\\n\\nfrom langchain_core.callbacks import CallbackManagerForRetrieverRun\\nfrom langchain_core.documents import Document\\nfrom langchain_core.pydantic_v1 import Field\\nfrom langchain_core.retrievers import BaseRetriever\\nfrom langchain_core.vectorstores import VectorStore\\n\\n\\ndef _get_hours_passed(time: datetime.datetime, ref_time: datetime.datetime) -> float:\\n    \"\"\"Get the hours passed between two datetimes.\"\"\"\\n    return (time - ref_time).total_seconds() / 3600', metadata={'source': 'test_repo/libs/langchain/langchain/retrievers/time_weighted_retriever.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='class TimeWeightedVectorStoreRetriever(BaseRetriever):\\n    \"\"\"Retriever that combines embedding similarity with\\n    recency in retrieving values.\"\"\"\\n\\n    vectorstore: VectorStore\\n    \"\"\"The vectorstore to store documents and determine salience.\"\"\"\\n\\n    search_kwargs: dict = Field(default_factory=lambda: dict(k=100))\\n    \"\"\"Keyword arguments to pass to the vectorstore similarity search.\"\"\"\\n\\n    # TODO: abstract as a queue\\n    memory_stream: List[Document] = Field(default_factory=list)\\n    \"\"\"The memory_stream of documents to search through.\"\"\"\\n\\n    decay_rate: float = Field(default=0.01)\\n    \"\"\"The exponential decay factor used as (1.0-decay_rate)**(hrs_passed).\"\"\"\\n\\n    k: int = 4\\n    \"\"\"The maximum number of documents to retrieve in a given call.\"\"\"\\n\\n    other_score_keys: List[str] = []\\n    \"\"\"Other keys in the metadata to factor into the score, e.g. \\'importance\\'.\"\"\"', metadata={'source': 'test_repo/libs/langchain/langchain/retrievers/time_weighted_retriever.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='default_salience: Optional[float] = None\\n    \"\"\"The salience to assign memories not retrieved from the vector store.\\n\\n    None assigns no salience to documents not fetched from the vector store.\\n    \"\"\"\\n\\n    class Config:\\n        \"\"\"Configuration for this pydantic object.\"\"\"\\n\\n        arbitrary_types_allowed = True\\n\\n    def _document_get_date(self, field: str, document: Document) -> datetime.datetime:\\n        \"\"\"Return the value of the date field of a document.\"\"\"\\n        if field in document.metadata:\\n            if isinstance(document.metadata[field], float):\\n                return datetime.datetime.fromtimestamp(document.metadata[field])\\n            return document.metadata[field]\\n        return datetime.datetime.now()', metadata={'source': 'test_repo/libs/langchain/langchain/retrievers/time_weighted_retriever.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='def _get_combined_score(\\n        self,\\n        document: Document,\\n        vector_relevance: Optional[float],\\n        current_time: datetime.datetime,\\n    ) -> float:\\n        \"\"\"Return the combined score for a document.\"\"\"\\n        hours_passed = _get_hours_passed(\\n            current_time,\\n            self._document_get_date(\"last_accessed_at\", document),\\n        )\\n        score = (1.0 - self.decay_rate) ** hours_passed\\n        for key in self.other_score_keys:\\n            if key in document.metadata:\\n                score += document.metadata[key]\\n        if vector_relevance is not None:\\n            score += vector_relevance\\n        return score', metadata={'source': 'test_repo/libs/langchain/langchain/retrievers/time_weighted_retriever.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='def get_salient_docs(self, query: str) -> Dict[int, Tuple[Document, float]]:\\n        \"\"\"Return documents that are salient to the query.\"\"\"\\n        docs_and_scores: List[Tuple[Document, float]]\\n        docs_and_scores = self.vectorstore.similarity_search_with_relevance_scores(\\n            query, **self.search_kwargs\\n        )\\n        results = {}\\n        for fetched_doc, relevance in docs_and_scores:\\n            if \"buffer_idx\" in fetched_doc.metadata:\\n                buffer_idx = fetched_doc.metadata[\"buffer_idx\"]\\n                doc = self.memory_stream[buffer_idx]\\n                results[buffer_idx] = (doc, relevance)\\n        return results', metadata={'source': 'test_repo/libs/langchain/langchain/retrievers/time_weighted_retriever.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='def _get_relevant_documents(\\n        self, query: str, *, run_manager: CallbackManagerForRetrieverRun\\n    ) -> List[Document]:\\n        \"\"\"Return documents that are relevant to the query.\"\"\"\\n        current_time = datetime.datetime.now()\\n        docs_and_scores = {\\n            doc.metadata[\"buffer_idx\"]: (doc, self.default_salience)\\n            for doc in self.memory_stream[-self.k :]\\n        }\\n        # If a doc is considered salient, update the salience score\\n        docs_and_scores.update(self.get_salient_docs(query))\\n        rescored_docs = [\\n            (doc, self._get_combined_score(doc, relevance, current_time))\\n            for doc, relevance in docs_and_scores.values()\\n        ]\\n        rescored_docs.sort(key=lambda x: x[1], reverse=True)\\n        result = []\\n        # Ensure frequently accessed memories aren\\'t forgotten\\n        for doc, _ in rescored_docs[: self.k]:\\n            # TODO: Update vector store doc once `update` method is exposed.', metadata={'source': 'test_repo/libs/langchain/langchain/retrievers/time_weighted_retriever.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='# TODO: Update vector store doc once `update` method is exposed.\\n            buffered_doc = self.memory_stream[doc.metadata[\"buffer_idx\"]]\\n            buffered_doc.metadata[\"last_accessed_at\"] = current_time\\n            result.append(buffered_doc)\\n        return result', metadata={'source': 'test_repo/libs/langchain/langchain/retrievers/time_weighted_retriever.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='def add_documents(self, documents: List[Document], **kwargs: Any) -> List[str]:\\n        \"\"\"Add documents to vectorstore.\"\"\"\\n        current_time = kwargs.get(\"current_time\")\\n        if current_time is None:\\n            current_time = datetime.datetime.now()\\n        # Avoid mutating input documents\\n        dup_docs = [deepcopy(d) for d in documents]\\n        for i, doc in enumerate(dup_docs):\\n            if \"last_accessed_at\" not in doc.metadata:\\n                doc.metadata[\"last_accessed_at\"] = current_time\\n            if \"created_at\" not in doc.metadata:\\n                doc.metadata[\"created_at\"] = current_time\\n            doc.metadata[\"buffer_idx\"] = len(self.memory_stream) + i\\n        self.memory_stream.extend(dup_docs)\\n        return self.vectorstore.add_documents(dup_docs, **kwargs)', metadata={'source': 'test_repo/libs/langchain/langchain/retrievers/time_weighted_retriever.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='async def aadd_documents(\\n        self, documents: List[Document], **kwargs: Any\\n    ) -> List[str]:\\n        \"\"\"Add documents to vectorstore.\"\"\"\\n        current_time = kwargs.get(\"current_time\")\\n        if current_time is None:\\n            current_time = datetime.datetime.now()\\n        # Avoid mutating input documents\\n        dup_docs = [deepcopy(d) for d in documents]\\n        for i, doc in enumerate(dup_docs):\\n            if \"last_accessed_at\" not in doc.metadata:\\n                doc.metadata[\"last_accessed_at\"] = current_time\\n            if \"created_at\" not in doc.metadata:\\n                doc.metadata[\"created_at\"] = current_time\\n            doc.metadata[\"buffer_idx\"] = len(self.memory_stream) + i\\n        self.memory_stream.extend(dup_docs)\\n        return await self.vectorstore.aadd_documents(dup_docs, **kwargs)', metadata={'source': 'test_repo/libs/langchain/langchain/retrievers/time_weighted_retriever.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from langchain_community.retrievers.outline import OutlineRetriever\\n\\n__all__ = [\"OutlineRetriever\"]', metadata={'source': 'test_repo/libs/langchain/langchain/retrievers/outline.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from langchain_community.retrievers.google_cloud_documentai_warehouse import (\\n    GoogleDocumentAIWarehouseRetriever,\\n)\\n\\n__all__ = [\"GoogleDocumentAIWarehouseRetriever\"]', metadata={'source': 'test_repo/libs/langchain/langchain/retrievers/google_cloud_documentai_warehouse.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='import logging\\nfrom typing import List\\n\\nfrom langchain_core.callbacks import (\\n    AsyncCallbackManagerForRetrieverRun,\\n    CallbackManagerForRetrieverRun,\\n)\\nfrom langchain_core.documents import Document\\nfrom langchain_core.language_models import BaseLLM\\nfrom langchain_core.prompts.prompt import PromptTemplate\\nfrom langchain_core.retrievers import BaseRetriever\\n\\nfrom langchain.chains.llm import LLMChain\\n\\nlogger = logging.getLogger(__name__)\\n\\n# Default template\\nDEFAULT_TEMPLATE = \"\"\"You are an assistant tasked with taking a natural language \\\\\\nquery from a user and converting it into a query for a vectorstore. \\\\\\nIn this process, you strip out information that is not relevant for \\\\\\nthe retrieval task. Here is the user query: {question}\"\"\"\\n\\n# Default prompt\\nDEFAULT_QUERY_PROMPT = PromptTemplate.from_template(DEFAULT_TEMPLATE)', metadata={'source': 'test_repo/libs/langchain/langchain/retrievers/re_phraser.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='class RePhraseQueryRetriever(BaseRetriever):\\n    \"\"\"Given a query, use an LLM to re-phrase it.\\n    Then, retrieve docs for the re-phrased query.\"\"\"\\n\\n    retriever: BaseRetriever\\n    llm_chain: LLMChain\\n\\n    @classmethod\\n    def from_llm(\\n        cls,\\n        retriever: BaseRetriever,\\n        llm: BaseLLM,\\n        prompt: PromptTemplate = DEFAULT_QUERY_PROMPT,\\n    ) -> \"RePhraseQueryRetriever\":\\n        \"\"\"Initialize from llm using default template.\\n\\n        The prompt used here expects a single input: `question`\\n\\n        Args:\\n            retriever: retriever to query documents from\\n            llm: llm for query generation using DEFAULT_QUERY_PROMPT\\n            prompt: prompt template for query generation\\n\\n        Returns:\\n            RePhraseQueryRetriever\\n        \"\"\"\\n\\n        llm_chain = LLMChain(llm=llm, prompt=prompt)\\n        return cls(\\n            retriever=retriever,\\n            llm_chain=llm_chain,\\n        )', metadata={'source': 'test_repo/libs/langchain/langchain/retrievers/re_phraser.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='def _get_relevant_documents(\\n        self,\\n        query: str,\\n        *,\\n        run_manager: CallbackManagerForRetrieverRun,\\n    ) -> List[Document]:\\n        \"\"\"Get relevated documents given a user question.\\n\\n        Args:\\n            query: user question\\n\\n        Returns:\\n            Relevant documents for re-phrased question\\n        \"\"\"\\n        response = self.llm_chain(query, callbacks=run_manager.get_child())\\n        re_phrased_question = response[\"text\"]\\n        logger.info(f\"Re-phrased question: {re_phrased_question}\")\\n        docs = self.retriever.get_relevant_documents(\\n            re_phrased_question, callbacks=run_manager.get_child()\\n        )\\n        return docs\\n\\n    async def _aget_relevant_documents(\\n        self,\\n        query: str,\\n        *,\\n        run_manager: AsyncCallbackManagerForRetrieverRun,\\n    ) -> List[Document]:\\n        raise NotImplementedError', metadata={'source': 'test_repo/libs/langchain/langchain/retrievers/re_phraser.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from langchain_community.retrievers.arcee import ArceeRetriever\\n\\n__all__ = [\"ArceeRetriever\"]', metadata={'source': 'test_repo/libs/langchain/langchain/retrievers/arcee.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from langchain_community.retrievers.docarray import DocArrayRetriever, SearchType\\n\\n__all__ = [\"SearchType\", \"DocArrayRetriever\"]', metadata={'source': 'test_repo/libs/langchain/langchain/retrievers/docarray.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='\"\"\"DocumentFilter that uses an LLM chain to extract the relevant parts of documents.\"\"\"\\nfrom __future__ import annotations\\n\\nimport asyncio\\nfrom typing import Any, Callable, Dict, Optional, Sequence, cast\\n\\nfrom langchain_core.documents import Document\\nfrom langchain_core.language_models import BaseLanguageModel\\nfrom langchain_core.output_parsers import BaseOutputParser\\nfrom langchain_core.prompts import PromptTemplate\\n\\nfrom langchain.callbacks.manager import Callbacks\\nfrom langchain.chains.llm import LLMChain\\nfrom langchain.retrievers.document_compressors.base import BaseDocumentCompressor\\nfrom langchain.retrievers.document_compressors.chain_extract_prompt import (\\n    prompt_template,\\n)\\n\\n\\ndef default_get_input(query: str, doc: Document) -> Dict[str, Any]:\\n    \"\"\"Return the compression chain input.\"\"\"\\n    return {\"question\": query, \"context\": doc.page_content}', metadata={'source': 'test_repo/libs/langchain/langchain/retrievers/document_compressors/chain_extract.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='class NoOutputParser(BaseOutputParser[str]):\\n    \"\"\"Parse outputs that could return a null string of some sort.\"\"\"\\n\\n    no_output_str: str = \"NO_OUTPUT\"\\n\\n    def parse(self, text: str) -> str:\\n        cleaned_text = text.strip()\\n        if cleaned_text == self.no_output_str:\\n            return \"\"\\n        return cleaned_text\\n\\n\\ndef _get_default_chain_prompt() -> PromptTemplate:\\n    output_parser = NoOutputParser()\\n    template = prompt_template.format(no_output_str=output_parser.no_output_str)\\n    return PromptTemplate(\\n        template=template,\\n        input_variables=[\"question\", \"context\"],\\n        output_parser=output_parser,\\n    )', metadata={'source': 'test_repo/libs/langchain/langchain/retrievers/document_compressors/chain_extract.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='class LLMChainExtractor(BaseDocumentCompressor):\\n    \"\"\"Document compressor that uses an LLM chain to extract\\n    the relevant parts of documents.\"\"\"\\n\\n    llm_chain: LLMChain\\n    \"\"\"LLM wrapper to use for compressing documents.\"\"\"\\n\\n    get_input: Callable[[str, Document], dict] = default_get_input\\n    \"\"\"Callable for constructing the chain input from the query and a Document.\"\"\"', metadata={'source': 'test_repo/libs/langchain/langchain/retrievers/document_compressors/chain_extract.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='def compress_documents(\\n        self,\\n        documents: Sequence[Document],\\n        query: str,\\n        callbacks: Optional[Callbacks] = None,\\n    ) -> Sequence[Document]:\\n        \"\"\"Compress page content of raw documents.\"\"\"\\n        compressed_docs = []\\n        for doc in documents:\\n            _input = self.get_input(query, doc)\\n            output = self.llm_chain.predict_and_parse(**_input, callbacks=callbacks)\\n            if len(output) == 0:\\n                continue\\n            compressed_docs.append(\\n                Document(page_content=cast(str, output), metadata=doc.metadata)\\n            )\\n        return compressed_docs', metadata={'source': 'test_repo/libs/langchain/langchain/retrievers/document_compressors/chain_extract.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='async def acompress_documents(\\n        self,\\n        documents: Sequence[Document],\\n        query: str,\\n        callbacks: Optional[Callbacks] = None,\\n    ) -> Sequence[Document]:\\n        \"\"\"Compress page content of raw documents asynchronously.\"\"\"\\n        outputs = await asyncio.gather(\\n            *[\\n                self.llm_chain.apredict_and_parse(\\n                    **self.get_input(query, doc), callbacks=callbacks\\n                )\\n                for doc in documents\\n            ]\\n        )\\n        compressed_docs = []\\n        for i, doc in enumerate(documents):\\n            if len(outputs[i]) == 0:\\n                continue\\n            compressed_docs.append(\\n                Document(page_content=outputs[i], metadata=doc.metadata)\\n            )\\n        return compressed_docs', metadata={'source': 'test_repo/libs/langchain/langchain/retrievers/document_compressors/chain_extract.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='@classmethod\\n    def from_llm(\\n        cls,\\n        llm: BaseLanguageModel,\\n        prompt: Optional[PromptTemplate] = None,\\n        get_input: Optional[Callable[[str, Document], str]] = None,\\n        llm_chain_kwargs: Optional[dict] = None,\\n    ) -> LLMChainExtractor:\\n        \"\"\"Initialize from LLM.\"\"\"\\n        _prompt = prompt if prompt is not None else _get_default_chain_prompt()\\n        _get_input = get_input if get_input is not None else default_get_input\\n        llm_chain = LLMChain(llm=llm, prompt=_prompt, **(llm_chain_kwargs or {}))\\n        return cls(llm_chain=llm_chain, get_input=_get_input)', metadata={'source': 'test_repo/libs/langchain/langchain/retrievers/document_compressors/chain_extract.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='\"\"\"Filter that uses an LLM to drop documents that aren\\'t relevant to the query.\"\"\"\\nfrom typing import Any, Callable, Dict, Optional, Sequence\\n\\nfrom langchain_core.documents import Document\\nfrom langchain_core.language_models import BaseLanguageModel\\nfrom langchain_core.prompts import BasePromptTemplate, PromptTemplate\\n\\nfrom langchain.callbacks.manager import Callbacks\\nfrom langchain.chains import LLMChain\\nfrom langchain.output_parsers.boolean import BooleanOutputParser\\nfrom langchain.retrievers.document_compressors.base import BaseDocumentCompressor\\nfrom langchain.retrievers.document_compressors.chain_filter_prompt import (\\n    prompt_template,\\n)\\n\\n\\ndef _get_default_chain_prompt() -> PromptTemplate:\\n    return PromptTemplate(\\n        template=prompt_template,\\n        input_variables=[\"question\", \"context\"],\\n        output_parser=BooleanOutputParser(),\\n    )', metadata={'source': 'test_repo/libs/langchain/langchain/retrievers/document_compressors/chain_filter.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='def default_get_input(query: str, doc: Document) -> Dict[str, Any]:\\n    \"\"\"Return the compression chain input.\"\"\"\\n    return {\"question\": query, \"context\": doc.page_content}', metadata={'source': 'test_repo/libs/langchain/langchain/retrievers/document_compressors/chain_filter.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='class LLMChainFilter(BaseDocumentCompressor):\\n    \"\"\"Filter that drops documents that aren\\'t relevant to the query.\"\"\"\\n\\n    llm_chain: LLMChain\\n    \"\"\"LLM wrapper to use for filtering documents. \\n    The chain prompt is expected to have a BooleanOutputParser.\"\"\"\\n\\n    get_input: Callable[[str, Document], dict] = default_get_input\\n    \"\"\"Callable for constructing the chain input from the query and a Document.\"\"\"\\n\\n    def compress_documents(\\n        self,\\n        documents: Sequence[Document],\\n        query: str,\\n        callbacks: Optional[Callbacks] = None,\\n    ) -> Sequence[Document]:\\n        \"\"\"Filter down documents based on their relevance to the query.\"\"\"\\n        filtered_docs = []\\n        for doc in documents:\\n            _input = self.get_input(query, doc)\\n            include_doc = self.llm_chain.predict_and_parse(\\n                **_input, callbacks=callbacks\\n            )\\n            if include_doc:\\n                filtered_docs.append(doc)\\n        return filtered_docs', metadata={'source': 'test_repo/libs/langchain/langchain/retrievers/document_compressors/chain_filter.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='@classmethod\\n    def from_llm(\\n        cls,\\n        llm: BaseLanguageModel,\\n        prompt: Optional[BasePromptTemplate] = None,\\n        **kwargs: Any,\\n    ) -> \"LLMChainFilter\":\\n        \"\"\"Create a LLMChainFilter from a language model.\\n\\n        Args:\\n            llm: The language model to use for filtering.\\n            prompt: The prompt to use for the filter.\\n            **kwargs: Additional arguments to pass to the constructor.\\n\\n        Returns:\\n            A LLMChainFilter that uses the given language model.\\n        \"\"\"\\n        _prompt = prompt if prompt is not None else _get_default_chain_prompt()\\n        llm_chain = LLMChain(llm=llm, prompt=_prompt)\\n        return cls(llm_chain=llm_chain, **kwargs)', metadata={'source': 'test_repo/libs/langchain/langchain/retrievers/document_compressors/chain_filter.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from langchain.retrievers.document_compressors.base import DocumentCompressorPipeline\\nfrom langchain.retrievers.document_compressors.chain_extract import (\\n    LLMChainExtractor,\\n)\\nfrom langchain.retrievers.document_compressors.chain_filter import (\\n    LLMChainFilter,\\n)\\nfrom langchain.retrievers.document_compressors.cohere_rerank import CohereRerank\\nfrom langchain.retrievers.document_compressors.embeddings_filter import (\\n    EmbeddingsFilter,\\n)\\nfrom langchain.retrievers.document_compressors.flashrank_rerank import FlashrankRerank\\n\\n__all__ = [\\n    \"DocumentCompressorPipeline\",\\n    \"EmbeddingsFilter\",\\n    \"LLMChainExtractor\",\\n    \"LLMChainFilter\",\\n    \"CohereRerank\",\\n    \"FlashrankRerank\",\\n]', metadata={'source': 'test_repo/libs/langchain/langchain/retrievers/document_compressors/__init__.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from __future__ import annotations\\n\\nfrom copy import deepcopy\\nfrom typing import Any, Dict, List, Optional, Sequence, Union\\n\\nfrom langchain_core.documents import Document\\nfrom langchain_core.pydantic_v1 import Extra, root_validator\\n\\nfrom langchain.callbacks.manager import Callbacks\\nfrom langchain.retrievers.document_compressors.base import BaseDocumentCompressor\\nfrom langchain.utils import get_from_dict_or_env', metadata={'source': 'test_repo/libs/langchain/langchain/retrievers/document_compressors/cohere_rerank.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='class CohereRerank(BaseDocumentCompressor):\\n    \"\"\"Document compressor that uses `Cohere Rerank API`.\"\"\"\\n\\n    client: Any = None\\n    \"\"\"Cohere client to use for compressing documents.\"\"\"\\n    top_n: Optional[int] = 3\\n    \"\"\"Number of documents to return.\"\"\"\\n    model: str = \"rerank-english-v2.0\"\\n    \"\"\"Model to use for reranking.\"\"\"\\n    cohere_api_key: Optional[str] = None\\n    \"\"\"Cohere API key. Must be specified directly or via environment variable \\n        COHERE_API_KEY.\"\"\"\\n    user_agent: str = \"langchain\"\\n    \"\"\"Identifier for the application making the request.\"\"\"\\n\\n    class Config:\\n        \"\"\"Configuration for this pydantic object.\"\"\"\\n\\n        extra = Extra.forbid\\n        arbitrary_types_allowed = True', metadata={'source': 'test_repo/libs/langchain/langchain/retrievers/document_compressors/cohere_rerank.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='extra = Extra.forbid\\n        arbitrary_types_allowed = True\\n\\n    @root_validator(pre=True)\\n    def validate_environment(cls, values: Dict) -> Dict:\\n        \"\"\"Validate that api key and python package exists in environment.\"\"\"\\n        if not values.get(\"client\"):\\n            try:\\n                import cohere\\n            except ImportError:\\n                raise ImportError(\\n                    \"Could not import cohere python package. \"\\n                    \"Please install it with `pip install cohere`.\"\\n                )\\n            cohere_api_key = get_from_dict_or_env(\\n                values, \"cohere_api_key\", \"COHERE_API_KEY\"\\n            )\\n            client_name = values.get(\"user_agent\", \"langchain\")\\n            values[\"client\"] = cohere.Client(cohere_api_key, client_name=client_name)\\n        return values', metadata={'source': 'test_repo/libs/langchain/langchain/retrievers/document_compressors/cohere_rerank.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='def rerank(\\n        self,\\n        documents: Sequence[Union[str, Document, dict]],\\n        query: str,\\n        *,\\n        model: Optional[str] = None,\\n        top_n: Optional[int] = -1,\\n        max_chunks_per_doc: Optional[int] = None,\\n    ) -> List[Dict[str, Any]]:\\n        \"\"\"Returns an ordered list of documents ordered by their relevance to the provided query.', metadata={'source': 'test_repo/libs/langchain/langchain/retrievers/document_compressors/cohere_rerank.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='Args:\\n            query: The query to use for reranking.\\n            documents: A sequence of documents to rerank.\\n            model: The model to use for re-ranking. Default to self.model.\\n            top_n : The number of results to return. If None returns all results.\\n                Defaults to self.top_n.\\n            max_chunks_per_doc : The maximum number of chunks derived from a document.\\n        \"\"\"  # noqa: E501\\n        if len(documents) == 0:  # to avoid empty api call\\n            return []\\n        docs = [\\n            doc.page_content if isinstance(doc, Document) else doc for doc in documents\\n        ]\\n        model = model or self.model\\n        top_n = top_n if (top_n is None or top_n > 0) else self.top_n\\n        results = self.client.rerank(\\n            query, docs, model, top_n=top_n, max_chunks_per_doc=max_chunks_per_doc\\n        )\\n        result_dicts = []\\n        for res in results:\\n            result_dicts.append(', metadata={'source': 'test_repo/libs/langchain/langchain/retrievers/document_compressors/cohere_rerank.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content=')\\n        result_dicts = []\\n        for res in results:\\n            result_dicts.append(\\n                {\"index\": res.index, \"relevance_score\": res.relevance_score}\\n            )\\n        return result_dicts', metadata={'source': 'test_repo/libs/langchain/langchain/retrievers/document_compressors/cohere_rerank.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='def compress_documents(\\n        self,\\n        documents: Sequence[Document],\\n        query: str,\\n        callbacks: Optional[Callbacks] = None,\\n    ) -> Sequence[Document]:\\n        \"\"\"\\n        Compress documents using Cohere\\'s rerank API.\\n\\n        Args:\\n            documents: A sequence of documents to compress.\\n            query: The query to use for compressing the documents.\\n            callbacks: Callbacks to run during the compression process.\\n\\n        Returns:\\n            A sequence of compressed documents.\\n        \"\"\"\\n        compressed = []\\n        for res in self.rerank(documents, query):\\n            doc = documents[res[\"index\"]]\\n            doc_copy = Document(doc.page_content, metadata=deepcopy(doc.metadata))\\n            doc_copy.metadata[\"relevance_score\"] = res[\"relevance_score\"]\\n            compressed.append(doc_copy)\\n        return compressed', metadata={'source': 'test_repo/libs/langchain/langchain/retrievers/document_compressors/cohere_rerank.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from __future__ import annotations\\n\\nfrom typing import TYPE_CHECKING, Dict, Optional, Sequence\\n\\nfrom langchain_core.documents import Document\\nfrom langchain_core.pydantic_v1 import Extra, root_validator\\n\\nfrom langchain.callbacks.manager import Callbacks\\nfrom langchain.retrievers.document_compressors.base import BaseDocumentCompressor\\n\\nif TYPE_CHECKING:\\n    from flashrank import Ranker, RerankRequest\\nelse:\\n    # Avoid pydantic annotation issues when actually instantiating\\n    # while keeping this import optional\\n    try:\\n        from flashrank import Ranker, RerankRequest\\n    except ImportError:\\n        pass\\n\\nDEFAULT_MODEL_NAME = \"ms-marco-MultiBERT-L-12\"', metadata={'source': 'test_repo/libs/langchain/langchain/retrievers/document_compressors/flashrank_rerank.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='class FlashrankRerank(BaseDocumentCompressor):\\n    \"\"\"Document compressor using Flashrank interface.\"\"\"\\n\\n    client: Ranker\\n    \"\"\"Flashrank client to use for compressing documents\"\"\"\\n    top_n: int = 3\\n    \"\"\"Number of documents to return.\"\"\"\\n    model: Optional[str] = None\\n    \"\"\"Model to use for reranking.\"\"\"\\n\\n    class Config:\\n        \"\"\"Configuration for this pydantic object.\"\"\"\\n\\n        extra = Extra.forbid\\n        arbitrary_types_allowed = True\\n\\n    @root_validator(pre=True)\\n    def validate_environment(cls, values: Dict) -> Dict:\\n        \"\"\"Validate that api key and python package exists in environment.\"\"\"\\n        try:\\n            from flashrank import Ranker\\n        except ImportError:\\n            raise ImportError(\\n                \"Could not import flashrank python package. \"\\n                \"Please install it with `pip install flashrank`.\"\\n            )', metadata={'source': 'test_repo/libs/langchain/langchain/retrievers/document_compressors/flashrank_rerank.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='values[\"model\"] = values.get(\"model\", DEFAULT_MODEL_NAME)\\n        values[\"client\"] = Ranker(model_name=values[\"model\"])\\n        return values\\n\\n    def compress_documents(\\n        self,\\n        documents: Sequence[Document],\\n        query: str,\\n        callbacks: Optional[Callbacks] = None,\\n    ) -> Sequence[Document]:\\n        passages = [\\n            {\"id\": i, \"text\": doc.page_content} for i, doc in enumerate(documents)\\n        ]\\n\\n        rerank_request = RerankRequest(query=query, passages=passages)\\n        rerank_response = self.client.rerank(rerank_request)[: self.top_n]\\n        final_results = []\\n        for r in rerank_response:\\n            doc = Document(\\n                page_content=r[\"text\"],\\n                metadata={\"id\": r[\"id\"], \"relevance_score\": r[\"score\"]},\\n            )\\n            final_results.append(doc)\\n        return final_results', metadata={'source': 'test_repo/libs/langchain/langchain/retrievers/document_compressors/flashrank_rerank.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from typing import Callable, Dict, Optional, Sequence\\n\\nimport numpy as np\\nfrom langchain_community.document_transformers.embeddings_redundant_filter import (\\n    _get_embeddings_from_stateful_docs,\\n    get_stateful_documents,\\n)\\nfrom langchain_core.documents import Document\\nfrom langchain_core.embeddings import Embeddings\\nfrom langchain_core.pydantic_v1 import root_validator\\n\\nfrom langchain.callbacks.manager import Callbacks\\nfrom langchain.retrievers.document_compressors.base import (\\n    BaseDocumentCompressor,\\n)\\nfrom langchain.utils.math import cosine_similarity', metadata={'source': 'test_repo/libs/langchain/langchain/retrievers/document_compressors/embeddings_filter.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='class EmbeddingsFilter(BaseDocumentCompressor):\\n    \"\"\"Document compressor that uses embeddings to drop documents\\n    unrelated to the query.\"\"\"\\n\\n    embeddings: Embeddings\\n    \"\"\"Embeddings to use for embedding document contents and queries.\"\"\"\\n    similarity_fn: Callable = cosine_similarity\\n    \"\"\"Similarity function for comparing documents. Function expected to take as input\\n    two matrices (List[List[float]]) and return a matrix of scores where higher values\\n    indicate greater similarity.\"\"\"\\n    k: Optional[int] = 20\\n    \"\"\"The number of relevant documents to return. Can be set to None, in which case\\n    `similarity_threshold` must be specified. Defaults to 20.\"\"\"\\n    similarity_threshold: Optional[float]\\n    \"\"\"Threshold for determining when two documents are similar enough\\n    to be considered redundant. Defaults to None, must be specified if `k` is set\\n    to None.\"\"\"\\n\\n    class Config:\\n        \"\"\"Configuration for this pydantic object.\"\"\"', metadata={'source': 'test_repo/libs/langchain/langchain/retrievers/document_compressors/embeddings_filter.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='class Config:\\n        \"\"\"Configuration for this pydantic object.\"\"\"\\n\\n        arbitrary_types_allowed = True\\n\\n    @root_validator()\\n    def validate_params(cls, values: Dict) -> Dict:\\n        \"\"\"Validate similarity parameters.\"\"\"\\n        if values[\"k\"] is None and values[\"similarity_threshold\"] is None:\\n            raise ValueError(\"Must specify one of `k` or `similarity_threshold`.\")\\n        return values', metadata={'source': 'test_repo/libs/langchain/langchain/retrievers/document_compressors/embeddings_filter.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='def compress_documents(\\n        self,\\n        documents: Sequence[Document],\\n        query: str,\\n        callbacks: Optional[Callbacks] = None,\\n    ) -> Sequence[Document]:\\n        \"\"\"Filter documents based on similarity of their embeddings to the query.\"\"\"\\n        stateful_documents = get_stateful_documents(documents)\\n        embedded_documents = _get_embeddings_from_stateful_docs(\\n            self.embeddings, stateful_documents\\n        )\\n        embedded_query = self.embeddings.embed_query(query)\\n        similarity = self.similarity_fn([embedded_query], embedded_documents)[0]\\n        included_idxs = np.arange(len(embedded_documents))\\n        if self.k is not None:\\n            included_idxs = np.argsort(similarity)[::-1][: self.k]\\n        if self.similarity_threshold is not None:\\n            similar_enough = np.where(\\n                similarity[included_idxs] > self.similarity_threshold\\n            )\\n            included_idxs = included_idxs[similar_enough]', metadata={'source': 'test_repo/libs/langchain/langchain/retrievers/document_compressors/embeddings_filter.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content=')\\n            included_idxs = included_idxs[similar_enough]\\n        for i in included_idxs:\\n            stateful_documents[i].state[\"query_similarity_score\"] = similarity[i]\\n        return [stateful_documents[i] for i in included_idxs]', metadata={'source': 'test_repo/libs/langchain/langchain/retrievers/document_compressors/embeddings_filter.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from inspect import signature\\nfrom typing import List, Optional, Sequence, Union\\n\\nfrom langchain_core.documents import (\\n    BaseDocumentCompressor,\\n    BaseDocumentTransformer,\\n    Document,\\n)\\n\\nfrom langchain.callbacks.manager import Callbacks', metadata={'source': 'test_repo/libs/langchain/langchain/retrievers/document_compressors/base.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='class DocumentCompressorPipeline(BaseDocumentCompressor):\\n    \"\"\"Document compressor that uses a pipeline of Transformers.\"\"\"\\n\\n    transformers: List[Union[BaseDocumentTransformer, BaseDocumentCompressor]]\\n    \"\"\"List of document filters that are chained together and run in sequence.\"\"\"\\n\\n    class Config:\\n        \"\"\"Configuration for this pydantic object.\"\"\"\\n\\n        arbitrary_types_allowed = True', metadata={'source': 'test_repo/libs/langchain/langchain/retrievers/document_compressors/base.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='def compress_documents(\\n        self,\\n        documents: Sequence[Document],\\n        query: str,\\n        callbacks: Optional[Callbacks] = None,\\n    ) -> Sequence[Document]:\\n        \"\"\"Transform a list of documents.\"\"\"\\n        for _transformer in self.transformers:\\n            if isinstance(_transformer, BaseDocumentCompressor):\\n                accepts_callbacks = (\\n                    signature(_transformer.compress_documents).parameters.get(\\n                        \"callbacks\"\\n                    )\\n                    is not None\\n                )\\n                if accepts_callbacks:\\n                    documents = _transformer.compress_documents(\\n                        documents, query, callbacks=callbacks\\n                    )\\n                else:\\n                    documents = _transformer.compress_documents(documents, query)\\n            elif isinstance(_transformer, BaseDocumentTransformer):\\n                documents = _transformer.transform_documents(documents)', metadata={'source': 'test_repo/libs/langchain/langchain/retrievers/document_compressors/base.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='documents = _transformer.transform_documents(documents)\\n            else:\\n                raise ValueError(f\"Got unexpected transformer type: {_transformer}\")\\n        return documents', metadata={'source': 'test_repo/libs/langchain/langchain/retrievers/document_compressors/base.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='async def acompress_documents(\\n        self,\\n        documents: Sequence[Document],\\n        query: str,\\n        callbacks: Optional[Callbacks] = None,\\n    ) -> Sequence[Document]:\\n        \"\"\"Compress retrieved documents given the query context.\"\"\"\\n        for _transformer in self.transformers:\\n            if isinstance(_transformer, BaseDocumentCompressor):\\n                accepts_callbacks = (\\n                    signature(_transformer.acompress_documents).parameters.get(\\n                        \"callbacks\"\\n                    )\\n                    is not None\\n                )\\n                if accepts_callbacks:\\n                    documents = await _transformer.acompress_documents(\\n                        documents, query, callbacks=callbacks\\n                    )\\n                else:\\n                    documents = await _transformer.acompress_documents(documents, query)\\n            elif isinstance(_transformer, BaseDocumentTransformer):', metadata={'source': 'test_repo/libs/langchain/langchain/retrievers/document_compressors/base.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='elif isinstance(_transformer, BaseDocumentTransformer):\\n                documents = await _transformer.atransform_documents(documents)\\n            else:\\n                raise ValueError(f\"Got unexpected transformer type: {_transformer}\")\\n        return documents', metadata={'source': 'test_repo/libs/langchain/langchain/retrievers/document_compressors/base.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='# flake8: noqa\\nprompt_template = \"\"\"Given the following question and context, return YES if the context is relevant to the question and NO if it isn\\'t.\\n\\n> Question: {question}\\n> Context:\\n>>>\\n{context}\\n>>>\\n> Relevant (YES / NO):\"\"\"', metadata={'source': 'test_repo/libs/langchain/langchain/retrievers/document_compressors/chain_filter_prompt.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='# flake8: noqa\\nprompt_template = \"\"\"Given the following question and context, extract any part of the context *AS IS* that is relevant to answer the question. If none of the context is relevant return {no_output_str}. \\n\\nRemember, *DO NOT* edit the extracted parts of the context.\\n\\n> Question: {{question}}\\n> Context:\\n>>>\\n{{context}}\\n>>>\\nExtracted relevant parts:\"\"\"', metadata={'source': 'test_repo/libs/langchain/langchain/retrievers/document_compressors/chain_extract_prompt.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='import re\\nfrom typing import Any, Callable, Dict, Tuple\\n\\nfrom langchain.chains.query_constructor.ir import (\\n    Comparator,\\n    Comparison,\\n    Operation,\\n    Operator,\\n    StructuredQuery,\\n    Visitor,\\n)\\n\\n\\ndef _DEFAULT_COMPOSER(op_name: str) -> Callable:\\n    \"\"\"\\n    Default composer for logical operators.\\n\\n    Args:\\n        op_name: Name of the operator.\\n\\n    Returns:\\n        Callable that takes a list of arguments and returns a string.\\n    \"\"\"\\n\\n    def f(*args: Any) -> str:\\n        args_: map[str] = map(str, args)\\n        return f\" {op_name} \".join(args_)\\n\\n    return f\\n\\n\\ndef _FUNCTION_COMPOSER(op_name: str) -> Callable:\\n    \"\"\"\\n    Composer for functions.\\n\\n    Args:\\n        op_name: Name of the function.\\n\\n    Returns:\\n        Callable that takes a list of arguments and returns a string.\\n    \"\"\"\\n\\n    def f(*args: Any) -> str:\\n        args_: map[str] = map(str, args)\\n        return f\"{op_name}({\\',\\'.join(args_)})\"\\n\\n    return f', metadata={'source': 'test_repo/libs/langchain/langchain/retrievers/self_query/myscale.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='class MyScaleTranslator(Visitor):\\n    \"\"\"Translate `MyScale` internal query language elements to valid filters.\"\"\"\\n\\n    allowed_operators = [Operator.AND, Operator.OR, Operator.NOT]\\n    \"\"\"Subset of allowed logical operators.\"\"\"\\n\\n    allowed_comparators = [\\n        Comparator.EQ,\\n        Comparator.GT,\\n        Comparator.GTE,\\n        Comparator.LT,\\n        Comparator.LTE,\\n        Comparator.CONTAIN,\\n        Comparator.LIKE,\\n    ]\\n\\n    map_dict = {\\n        Operator.AND: _DEFAULT_COMPOSER(\"AND\"),\\n        Operator.OR: _DEFAULT_COMPOSER(\"OR\"),\\n        Operator.NOT: _DEFAULT_COMPOSER(\"NOT\"),\\n        Comparator.EQ: _DEFAULT_COMPOSER(\"=\"),\\n        Comparator.GT: _DEFAULT_COMPOSER(\">\"),\\n        Comparator.GTE: _DEFAULT_COMPOSER(\">=\"),\\n        Comparator.LT: _DEFAULT_COMPOSER(\"<\"),\\n        Comparator.LTE: _DEFAULT_COMPOSER(\"<=\"),\\n        Comparator.CONTAIN: _FUNCTION_COMPOSER(\"has\"),\\n        Comparator.LIKE: _DEFAULT_COMPOSER(\"ILIKE\"),\\n    }', metadata={'source': 'test_repo/libs/langchain/langchain/retrievers/self_query/myscale.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='def __init__(self, metadata_key: str = \"metadata\") -> None:\\n        super().__init__()\\n        self.metadata_key = metadata_key\\n\\n    def visit_operation(self, operation: Operation) -> Dict:\\n        args = [arg.accept(self) for arg in operation.arguments]\\n        func = operation.operator\\n        self._validate_func(func)\\n        return self.map_dict[func](*args)\\n\\n    def visit_comparison(self, comparison: Comparison) -> Dict:\\n        regex = r\"\\\\((.*?)\\\\)\"\\n        matched = re.search(r\"\\\\(\\\\w+\\\\)\", comparison.attribute)\\n\\n        # If arbitrary function is applied to an attribute\\n        if matched:\\n            attr = re.sub(\\n                regex,\\n                f\"({self.metadata_key}.{matched.group(0)[1:-1]})\",\\n                comparison.attribute,\\n            )\\n        else:\\n            attr = f\"{self.metadata_key}.{comparison.attribute}\"\\n        value = comparison.value\\n        comp = comparison.comparator\\n\\n        value = f\"\\'{value}\\'\" if isinstance(value, str) else value', metadata={'source': 'test_repo/libs/langchain/langchain/retrievers/self_query/myscale.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='value = f\"\\'{value}\\'\" if isinstance(value, str) else value\\n\\n        # convert timestamp for datetime objects\\n        if isinstance(value, dict) and value.get(\"type\") == \"date\":\\n            attr = f\"parseDateTime32BestEffort({attr})\"\\n            value = f\"parseDateTime32BestEffort(\\'{value[\\'date\\']}\\')\"\\n\\n        # string pattern match\\n        if comp is Comparator.LIKE:\\n            value = f\"\\'%{value[1:-1]}%\\'\"\\n        return self.map_dict[comp](attr, value)\\n\\n    def visit_structured_query(\\n        self, structured_query: StructuredQuery\\n    ) -> Tuple[str, dict]:\\n        print(structured_query)  # noqa: T201\\n        if structured_query.filter is None:\\n            kwargs = {}\\n        else:\\n            kwargs = {\"where_str\": structured_query.filter.accept(self)}\\n        return structured_query.query, kwargs', metadata={'source': 'test_repo/libs/langchain/langchain/retrievers/self_query/myscale.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from __future__ import annotations\\n\\nfrom typing import Any, Tuple\\n\\nfrom langchain_community.vectorstores.redis import Redis\\nfrom langchain_community.vectorstores.redis.filters import (\\n    RedisFilterExpression,\\n    RedisFilterField,\\n    RedisFilterOperator,\\n    RedisNum,\\n    RedisTag,\\n    RedisText,\\n)\\nfrom langchain_community.vectorstores.redis.schema import RedisModel\\n\\nfrom langchain.chains.query_constructor.ir import (\\n    Comparator,\\n    Comparison,\\n    Operation,\\n    Operator,\\n    StructuredQuery,\\n    Visitor,\\n)\\n\\n_COMPARATOR_TO_BUILTIN_METHOD = {\\n    Comparator.EQ: \"__eq__\",\\n    Comparator.NE: \"__ne__\",\\n    Comparator.LT: \"__lt__\",\\n    Comparator.GT: \"__gt__\",\\n    Comparator.LTE: \"__le__\",\\n    Comparator.GTE: \"__ge__\",\\n    Comparator.CONTAIN: \"__eq__\",\\n    Comparator.LIKE: \"__mod__\",\\n}', metadata={'source': 'test_repo/libs/langchain/langchain/retrievers/self_query/redis.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='class RedisTranslator(Visitor):\\n    \"\"\"Visitor for translating structured queries to Redis filter expressions.\"\"\"\\n\\n    allowed_comparators = (\\n        Comparator.EQ,\\n        Comparator.NE,\\n        Comparator.LT,\\n        Comparator.LTE,\\n        Comparator.GT,\\n        Comparator.GTE,\\n        Comparator.CONTAIN,\\n        Comparator.LIKE,\\n    )\\n    \"\"\"Subset of allowed logical comparators.\"\"\"\\n    allowed_operators = (Operator.AND, Operator.OR)\\n    \"\"\"Subset of allowed logical operators.\"\"\"\\n\\n    def __init__(self, schema: RedisModel) -> None:\\n        self._schema = schema', metadata={'source': 'test_repo/libs/langchain/langchain/retrievers/self_query/redis.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='def __init__(self, schema: RedisModel) -> None:\\n        self._schema = schema\\n\\n    def _attribute_to_filter_field(self, attribute: str) -> RedisFilterField:\\n        if attribute in [tf.name for tf in self._schema.text]:\\n            return RedisText(attribute)\\n        elif attribute in [tf.name for tf in self._schema.tag or []]:\\n            return RedisTag(attribute)\\n        elif attribute in [tf.name for tf in self._schema.numeric or []]:\\n            return RedisNum(attribute)\\n        else:\\n            raise ValueError(\\n                f\"Invalid attribute {attribute} not in vector store schema. Schema is:\"\\n                f\"\\\\n{self._schema.as_dict()}\"\\n            )\\n\\n    def visit_comparison(self, comparison: Comparison) -> RedisFilterExpression:\\n        filter_field = self._attribute_to_filter_field(comparison.attribute)\\n        comparison_method = _COMPARATOR_TO_BUILTIN_METHOD[comparison.comparator]\\n        return getattr(filter_field, comparison_method)(comparison.value)', metadata={'source': 'test_repo/libs/langchain/langchain/retrievers/self_query/redis.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='def visit_operation(self, operation: Operation) -> Any:\\n        left = operation.arguments[0].accept(self)\\n        if len(operation.arguments) > 2:\\n            right = self.visit_operation(\\n                Operation(\\n                    operator=operation.operator, arguments=operation.arguments[1:]\\n                )\\n            )\\n        else:\\n            right = operation.arguments[1].accept(self)\\n        redis_operator = (\\n            RedisFilterOperator.OR\\n            if operation.operator == Operator.OR\\n            else RedisFilterOperator.AND\\n        )\\n        return RedisFilterExpression(operator=redis_operator, left=left, right=right)\\n\\n    def visit_structured_query(\\n        self, structured_query: StructuredQuery\\n    ) -> Tuple[str, dict]:\\n        if structured_query.filter is None:\\n            kwargs = {}\\n        else:\\n            kwargs = {\"filter\": structured_query.filter.accept(self)}\\n        return structured_query.query, kwargs', metadata={'source': 'test_repo/libs/langchain/langchain/retrievers/self_query/redis.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='@classmethod\\n    def from_vectorstore(cls, vectorstore: Redis) -> RedisTranslator:\\n        return cls(vectorstore._schema)', metadata={'source': 'test_repo/libs/langchain/langchain/retrievers/self_query/redis.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from typing import Dict, Tuple, Union\\n\\nfrom langchain.chains.query_constructor.ir import (\\n    Comparator,\\n    Comparison,\\n    Operation,\\n    Operator,\\n    StructuredQuery,\\n    Visitor,\\n)', metadata={'source': 'test_repo/libs/langchain/langchain/retrievers/self_query/elasticsearch.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='class ElasticsearchTranslator(Visitor):\\n    \"\"\"Translate `Elasticsearch` internal query language elements to valid filters.\"\"\"\\n\\n    allowed_comparators = [\\n        Comparator.EQ,\\n        Comparator.GT,\\n        Comparator.GTE,\\n        Comparator.LT,\\n        Comparator.LTE,\\n        Comparator.CONTAIN,\\n        Comparator.LIKE,\\n    ]\\n    \"\"\"Subset of allowed logical comparators.\"\"\"\\n\\n    allowed_operators = [Operator.AND, Operator.OR, Operator.NOT]\\n    \"\"\"Subset of allowed logical operators.\"\"\"', metadata={'source': 'test_repo/libs/langchain/langchain/retrievers/self_query/elasticsearch.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='def _format_func(self, func: Union[Operator, Comparator]) -> str:\\n        self._validate_func(func)\\n        map_dict = {\\n            Operator.OR: \"should\",\\n            Operator.NOT: \"must_not\",\\n            Operator.AND: \"must\",\\n            Comparator.EQ: \"term\",\\n            Comparator.GT: \"gt\",\\n            Comparator.GTE: \"gte\",\\n            Comparator.LT: \"lt\",\\n            Comparator.LTE: \"lte\",\\n            Comparator.CONTAIN: \"match\",\\n            Comparator.LIKE: \"match\",\\n        }\\n        return map_dict[func]\\n\\n    def visit_operation(self, operation: Operation) -> Dict:\\n        args = [arg.accept(self) for arg in operation.arguments]\\n\\n        return {\"bool\": {self._format_func(operation.operator): args}}\\n\\n    def visit_comparison(self, comparison: Comparison) -> Dict:\\n        # ElasticsearchStore filters require to target\\n        # the metadata object field\\n        field = f\"metadata.{comparison.attribute}\"', metadata={'source': 'test_repo/libs/langchain/langchain/retrievers/self_query/elasticsearch.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='is_range_comparator = comparison.comparator in [\\n            Comparator.GT,\\n            Comparator.GTE,\\n            Comparator.LT,\\n            Comparator.LTE,\\n        ]\\n\\n        if is_range_comparator:\\n            value = comparison.value\\n            if isinstance(comparison.value, dict) and \"date\" in comparison.value:\\n                value = comparison.value[\"date\"]\\n            return {\"range\": {field: {self._format_func(comparison.comparator): value}}}\\n\\n        if comparison.comparator == Comparator.CONTAIN:\\n            return {\\n                self._format_func(comparison.comparator): {\\n                    field: {\"query\": comparison.value}\\n                }\\n            }\\n\\n        if comparison.comparator == Comparator.LIKE:\\n            return {\\n                self._format_func(comparison.comparator): {\\n                    field: {\"query\": comparison.value, \"fuzziness\": \"AUTO\"}\\n                }\\n            }', metadata={'source': 'test_repo/libs/langchain/langchain/retrievers/self_query/elasticsearch.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='# we assume that if the value is a string,\\n        # we want to use the keyword field\\n        field = f\"{field}.keyword\" if isinstance(comparison.value, str) else field\\n\\n        if isinstance(comparison.value, dict):\\n            if \"date\" in comparison.value:\\n                comparison.value = comparison.value[\"date\"]\\n\\n        return {self._format_func(comparison.comparator): {field: comparison.value}}\\n\\n    def visit_structured_query(\\n        self, structured_query: StructuredQuery\\n    ) -> Tuple[str, dict]:\\n        if structured_query.filter is None:\\n            kwargs = {}\\n        else:\\n            kwargs = {\"filter\": [structured_query.filter.accept(self)]}\\n        return structured_query.query, kwargs', metadata={'source': 'test_repo/libs/langchain/langchain/retrievers/self_query/elasticsearch.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from typing import Tuple, Union\\n\\nfrom langchain.chains.query_constructor.ir import (\\n    Comparator,\\n    Comparison,\\n    Operation,\\n    Operator,\\n    StructuredQuery,\\n    Visitor,\\n)', metadata={'source': 'test_repo/libs/langchain/langchain/retrievers/self_query/dingo.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='class DingoDBTranslator(Visitor):\\n    \"\"\"Translate `DingoDB` internal query language elements to valid filters.\"\"\"\\n\\n    allowed_comparators = (\\n        Comparator.EQ,\\n        Comparator.NE,\\n        Comparator.LT,\\n        Comparator.LTE,\\n        Comparator.GT,\\n        Comparator.GTE,\\n    )\\n    \"\"\"Subset of allowed logical comparators.\"\"\"\\n    allowed_operators = (Operator.AND, Operator.OR)\\n    \"\"\"Subset of allowed logical operators.\"\"\"\\n\\n    def _format_func(self, func: Union[Operator, Comparator]) -> str:\\n        self._validate_func(func)\\n        return f\"${func.value}\"\\n\\n    def visit_operation(self, operation: Operation) -> Operation:\\n        return operation\\n\\n    def visit_comparison(self, comparison: Comparison) -> Comparison:\\n        return comparison', metadata={'source': 'test_repo/libs/langchain/langchain/retrievers/self_query/dingo.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='def visit_comparison(self, comparison: Comparison) -> Comparison:\\n        return comparison\\n\\n    def visit_structured_query(\\n        self, structured_query: StructuredQuery\\n    ) -> Tuple[str, dict]:\\n        if structured_query.filter is None:\\n            kwargs = {}\\n        else:\\n            kwargs = {\\n                \"search_params\": {\\n                    \"langchain_expr\": structured_query.filter.accept(self)\\n                }\\n            }\\n        return structured_query.query, kwargs', metadata={'source': 'test_repo/libs/langchain/langchain/retrievers/self_query/dingo.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from __future__ import annotations\\n\\nfrom typing import TYPE_CHECKING, Tuple\\n\\nfrom langchain.chains.query_constructor.ir import (\\n    Comparator,\\n    Comparison,\\n    Operation,\\n    Operator,\\n    StructuredQuery,\\n    Visitor,\\n)\\n\\nif TYPE_CHECKING:\\n    from qdrant_client.http import models as rest', metadata={'source': 'test_repo/libs/langchain/langchain/retrievers/self_query/qdrant.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='class QdrantTranslator(Visitor):\\n    \"\"\"Translate `Qdrant` internal query language elements to valid filters.\"\"\"\\n\\n    allowed_operators = (\\n        Operator.AND,\\n        Operator.OR,\\n        Operator.NOT,\\n    )\\n    \"\"\"Subset of allowed logical operators.\"\"\"\\n\\n    allowed_comparators = (\\n        Comparator.EQ,\\n        Comparator.LT,\\n        Comparator.LTE,\\n        Comparator.GT,\\n        Comparator.GTE,\\n        Comparator.LIKE,\\n    )\\n    \"\"\"Subset of allowed logical comparators.\"\"\"\\n\\n    def __init__(self, metadata_key: str):\\n        self.metadata_key = metadata_key\\n\\n    def visit_operation(self, operation: Operation) -> rest.Filter:\\n        try:\\n            from qdrant_client.http import models as rest\\n        except ImportError as e:\\n            raise ImportError(\\n                \"Cannot import qdrant_client. Please install with `pip install \"\\n                \"qdrant-client`.\"\\n            ) from e', metadata={'source': 'test_repo/libs/langchain/langchain/retrievers/self_query/qdrant.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='args = [arg.accept(self) for arg in operation.arguments]\\n        operator = {\\n            Operator.AND: \"must\",\\n            Operator.OR: \"should\",\\n            Operator.NOT: \"must_not\",\\n        }[operation.operator]\\n        return rest.Filter(**{operator: args})\\n\\n    def visit_comparison(self, comparison: Comparison) -> rest.FieldCondition:\\n        try:\\n            from qdrant_client.http import models as rest\\n        except ImportError as e:\\n            raise ImportError(\\n                \"Cannot import qdrant_client. Please install with `pip install \"\\n                \"qdrant-client`.\"\\n            ) from e', metadata={'source': 'test_repo/libs/langchain/langchain/retrievers/self_query/qdrant.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='self._validate_func(comparison.comparator)\\n        attribute = self.metadata_key + \".\" + comparison.attribute\\n        if comparison.comparator == Comparator.EQ:\\n            return rest.FieldCondition(\\n                key=attribute, match=rest.MatchValue(value=comparison.value)\\n            )\\n        if comparison.comparator == Comparator.LIKE:\\n            return rest.FieldCondition(\\n                key=attribute, match=rest.MatchText(text=comparison.value)\\n            )\\n        kwargs = {comparison.comparator.value: comparison.value}\\n        return rest.FieldCondition(key=attribute, range=rest.Range(**kwargs))', metadata={'source': 'test_repo/libs/langchain/langchain/retrievers/self_query/qdrant.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='def visit_structured_query(\\n        self, structured_query: StructuredQuery\\n    ) -> Tuple[str, dict]:\\n        try:\\n            from qdrant_client.http import models as rest\\n        except ImportError as e:\\n            raise ImportError(\\n                \"Cannot import qdrant_client. Please install with `pip install \"\\n                \"qdrant-client`.\"\\n            ) from e\\n\\n        if structured_query.filter is None:\\n            kwargs = {}\\n        else:\\n            filter = structured_query.filter.accept(self)\\n            if isinstance(filter, rest.FieldCondition):\\n                filter = rest.Filter(must=[filter])\\n            kwargs = {\"filter\": filter}\\n        return structured_query.query, kwargs', metadata={'source': 'test_repo/libs/langchain/langchain/retrievers/self_query/qdrant.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from typing import Dict, Tuple, Union\\n\\nfrom langchain.chains.query_constructor.ir import (\\n    Comparator,\\n    Comparison,\\n    Operation,\\n    Operator,\\n    StructuredQuery,\\n    Visitor,\\n)', metadata={'source': 'test_repo/libs/langchain/langchain/retrievers/self_query/chroma.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='class ChromaTranslator(Visitor):\\n    \"\"\"Translate `Chroma` internal query language elements to valid filters.\"\"\"\\n\\n    allowed_operators = [Operator.AND, Operator.OR]\\n    \"\"\"Subset of allowed logical operators.\"\"\"\\n    allowed_comparators = [\\n        Comparator.EQ,\\n        Comparator.NE,\\n        Comparator.GT,\\n        Comparator.GTE,\\n        Comparator.LT,\\n        Comparator.LTE,\\n    ]\\n    \"\"\"Subset of allowed logical comparators.\"\"\"\\n\\n    def _format_func(self, func: Union[Operator, Comparator]) -> str:\\n        self._validate_func(func)\\n        return f\"${func.value}\"\\n\\n    def visit_operation(self, operation: Operation) -> Dict:\\n        args = [arg.accept(self) for arg in operation.arguments]\\n        return {self._format_func(operation.operator): args}\\n\\n    def visit_comparison(self, comparison: Comparison) -> Dict:\\n        return {\\n            comparison.attribute: {\\n                self._format_func(comparison.comparator): comparison.value\\n            }\\n        }', metadata={'source': 'test_repo/libs/langchain/langchain/retrievers/self_query/chroma.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='def visit_structured_query(\\n        self, structured_query: StructuredQuery\\n    ) -> Tuple[str, dict]:\\n        if structured_query.filter is None:\\n            kwargs = {}\\n        else:\\n            kwargs = {\"filter\": structured_query.filter.accept(self)}\\n        return structured_query.query, kwargs', metadata={'source': 'test_repo/libs/langchain/langchain/retrievers/self_query/chroma.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='\"\"\"Logic for converting internal query language to a valid DashVector query.\"\"\"\\nfrom typing import Tuple, Union\\n\\nfrom langchain.chains.query_constructor.ir import (\\n    Comparator,\\n    Comparison,\\n    Operation,\\n    Operator,\\n    StructuredQuery,\\n    Visitor,\\n)', metadata={'source': 'test_repo/libs/langchain/langchain/retrievers/self_query/dashvector.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='class DashvectorTranslator(Visitor):\\n    \"\"\"Logic for converting internal query language elements to valid filters.\"\"\"\\n\\n    allowed_operators = [Operator.AND, Operator.OR]\\n    allowed_comparators = [\\n        Comparator.EQ,\\n        Comparator.GT,\\n        Comparator.GTE,\\n        Comparator.LT,\\n        Comparator.LTE,\\n        Comparator.LIKE,\\n    ]\\n\\n    map_dict = {\\n        Operator.AND: \" AND \",\\n        Operator.OR: \" OR \",\\n        Comparator.EQ: \" = \",\\n        Comparator.GT: \" > \",\\n        Comparator.GTE: \" >= \",\\n        Comparator.LT: \" < \",\\n        Comparator.LTE: \" <= \",\\n        Comparator.LIKE: \" LIKE \",\\n    }\\n\\n    def _format_func(self, func: Union[Operator, Comparator]) -> str:\\n        self._validate_func(func)\\n        return self.map_dict[func]\\n\\n    def visit_operation(self, operation: Operation) -> str:\\n        args = [arg.accept(self) for arg in operation.arguments]\\n        return self._format_func(operation.operator).join(args)', metadata={'source': 'test_repo/libs/langchain/langchain/retrievers/self_query/dashvector.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='def visit_comparison(self, comparison: Comparison) -> str:\\n        value = comparison.value\\n        if isinstance(value, str):\\n            if comparison.comparator == Comparator.LIKE:\\n                value = f\"\\'%{value}%\\'\"\\n            else:\\n                value = f\"\\'{value}\\'\"\\n        return (\\n            f\"{comparison.attribute}{self._format_func(comparison.comparator)}{value}\"\\n        )\\n\\n    def visit_structured_query(\\n        self, structured_query: StructuredQuery\\n    ) -> Tuple[str, dict]:\\n        if structured_query.filter is None:\\n            kwargs = {}\\n        else:\\n            kwargs = {\"filter\": structured_query.filter.accept(self)}\\n        return structured_query.query, kwargs', metadata={'source': 'test_repo/libs/langchain/langchain/retrievers/self_query/dashvector.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='\"\"\"Logic for converting internal query language to a valid MongoDB Atlas query.\"\"\"\\nfrom typing import Dict, Tuple, Union\\n\\nfrom langchain.chains.query_constructor.ir import (\\n    Comparator,\\n    Comparison,\\n    Operation,\\n    Operator,\\n    StructuredQuery,\\n    Visitor,\\n)\\n\\nMULTIPLE_ARITY_COMPARATORS = [Comparator.IN, Comparator.NIN]', metadata={'source': 'test_repo/libs/langchain/langchain/retrievers/self_query/mongodb_atlas.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='class MongoDBAtlasTranslator(Visitor):\\n    \"\"\"Translate Mongo internal query language elements to valid filters.\"\"\"\\n\\n    \"\"\"Subset of allowed logical comparators.\"\"\"\\n    allowed_comparators = [\\n        Comparator.EQ,\\n        Comparator.NE,\\n        Comparator.GT,\\n        Comparator.GTE,\\n        Comparator.LT,\\n        Comparator.LTE,\\n        Comparator.IN,\\n        Comparator.NIN,\\n    ]\\n\\n    \"\"\"Subset of allowed logical operators.\"\"\"\\n    allowed_operators = [Operator.AND, Operator.OR]', metadata={'source': 'test_repo/libs/langchain/langchain/retrievers/self_query/mongodb_atlas.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='\"\"\"Subset of allowed logical operators.\"\"\"\\n    allowed_operators = [Operator.AND, Operator.OR]\\n\\n    ## Convert a operator or a comparator to Mongo Query Format\\n    def _format_func(self, func: Union[Operator, Comparator]) -> str:\\n        self._validate_func(func)\\n        map_dict = {\\n            Operator.AND: \"$and\",\\n            Operator.OR: \"$or\",\\n            Comparator.EQ: \"$eq\",\\n            Comparator.NE: \"$ne\",\\n            Comparator.GTE: \"$gte\",\\n            Comparator.LTE: \"$lte\",\\n            Comparator.LT: \"$lt\",\\n            Comparator.GT: \"$gt\",\\n            Comparator.IN: \"$in\",\\n            Comparator.NIN: \"$nin\",\\n        }\\n        return map_dict[func]\\n\\n    def visit_operation(self, operation: Operation) -> Dict:\\n        args = [arg.accept(self) for arg in operation.arguments]\\n        return {self._format_func(operation.operator): args}', metadata={'source': 'test_repo/libs/langchain/langchain/retrievers/self_query/mongodb_atlas.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='def visit_comparison(self, comparison: Comparison) -> Dict:\\n        if comparison.comparator in MULTIPLE_ARITY_COMPARATORS and not isinstance(\\n            comparison.value, list\\n        ):\\n            comparison.value = [comparison.value]\\n\\n        comparator = self._format_func(comparison.comparator)\\n\\n        attribute = comparison.attribute\\n\\n        return {attribute: {comparator: comparison.value}}\\n\\n    def visit_structured_query(\\n        self, structured_query: StructuredQuery\\n    ) -> Tuple[str, dict]:\\n        if structured_query.filter is None:\\n            kwargs = {}\\n        else:\\n            kwargs = {\"pre_filter\": structured_query.filter.accept(self)}\\n        return structured_query.query, kwargs', metadata={'source': 'test_repo/libs/langchain/langchain/retrievers/self_query/mongodb_atlas.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from __future__ import annotations\\n\\nfrom typing import TYPE_CHECKING, Tuple, Union\\n\\nfrom langchain.chains.query_constructor.ir import (\\n    Comparator,\\n    Comparison,\\n    Operation,\\n    Operator,\\n    StructuredQuery,\\n    Visitor,\\n)\\n\\nif TYPE_CHECKING:\\n    from timescale_vector import client', metadata={'source': 'test_repo/libs/langchain/langchain/retrievers/self_query/timescalevector.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='class TimescaleVectorTranslator(Visitor):\\n    \"\"\"Translate the internal query language elements to valid filters.\"\"\"\\n\\n    allowed_operators = [Operator.AND, Operator.OR, Operator.NOT]\\n    \"\"\"Subset of allowed logical operators.\"\"\"\\n\\n    allowed_comparators = [\\n        Comparator.EQ,\\n        Comparator.GT,\\n        Comparator.GTE,\\n        Comparator.LT,\\n        Comparator.LTE,\\n    ]\\n\\n    COMPARATOR_MAP = {\\n        Comparator.EQ: \"==\",\\n        Comparator.GT: \">\",\\n        Comparator.GTE: \">=\",\\n        Comparator.LT: \"<\",\\n        Comparator.LTE: \"<=\",\\n    }\\n\\n    OPERATOR_MAP = {Operator.AND: \"AND\", Operator.OR: \"OR\", Operator.NOT: \"NOT\"}\\n\\n    def _format_func(self, func: Union[Operator, Comparator]) -> str:\\n        self._validate_func(func)\\n        if isinstance(func, Operator):\\n            value = self.OPERATOR_MAP[func.value]  # type: ignore\\n        elif isinstance(func, Comparator):\\n            value = self.COMPARATOR_MAP[func.value]  # type: ignore\\n        return f\"{value}\"', metadata={'source': 'test_repo/libs/langchain/langchain/retrievers/self_query/timescalevector.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='def visit_operation(self, operation: Operation) -> client.Predicates:\\n        try:\\n            from timescale_vector import client\\n        except ImportError as e:\\n            raise ImportError(\\n                \"Cannot import timescale-vector. Please install with `pip install \"\\n                \"timescale-vector`.\"\\n            ) from e\\n        args = [arg.accept(self) for arg in operation.arguments]\\n        return client.Predicates(*args, operator=self._format_func(operation.operator))', metadata={'source': 'test_repo/libs/langchain/langchain/retrievers/self_query/timescalevector.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='def visit_comparison(self, comparison: Comparison) -> client.Predicates:\\n        try:\\n            from timescale_vector import client\\n        except ImportError as e:\\n            raise ImportError(\\n                \"Cannot import timescale-vector. Please install with `pip install \"\\n                \"timescale-vector`.\"\\n            ) from e\\n        return client.Predicates(\\n            (\\n                comparison.attribute,\\n                self._format_func(comparison.comparator),\\n                comparison.value,\\n            )\\n        )\\n\\n    def visit_structured_query(\\n        self, structured_query: StructuredQuery\\n    ) -> Tuple[str, dict]:\\n        if structured_query.filter is None:\\n            kwargs = {}\\n        else:\\n            kwargs = {\"predicates\": structured_query.filter.accept(self)}\\n        return structured_query.query, kwargs', metadata={'source': 'test_repo/libs/langchain/langchain/retrievers/self_query/timescalevector.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from typing import Dict, Tuple, Union\\n\\nfrom langchain.chains.query_constructor.ir import (\\n    Comparator,\\n    Comparison,\\n    Operation,\\n    Operator,\\n    StructuredQuery,\\n    Visitor,\\n)', metadata={'source': 'test_repo/libs/langchain/langchain/retrievers/self_query/pgvector.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='class PGVectorTranslator(Visitor):\\n    \"\"\"Translate `PGVector` internal query language elements to valid filters.\"\"\"\\n\\n    allowed_operators = [Operator.AND, Operator.OR]\\n    \"\"\"Subset of allowed logical operators.\"\"\"\\n    allowed_comparators = [\\n        Comparator.EQ,\\n        Comparator.NE,\\n        Comparator.GT,\\n        Comparator.LT,\\n        Comparator.IN,\\n        Comparator.NIN,\\n        Comparator.CONTAIN,\\n        Comparator.LIKE,\\n    ]\\n    \"\"\"Subset of allowed logical comparators.\"\"\"\\n\\n    def _format_func(self, func: Union[Operator, Comparator]) -> str:\\n        self._validate_func(func)\\n        return f\"{func.value}\"\\n\\n    def visit_operation(self, operation: Operation) -> Dict:\\n        args = [arg.accept(self) for arg in operation.arguments]\\n        return {self._format_func(operation.operator): args}', metadata={'source': 'test_repo/libs/langchain/langchain/retrievers/self_query/pgvector.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='def visit_comparison(self, comparison: Comparison) -> Dict:\\n        return {\\n            comparison.attribute: {\\n                self._format_func(comparison.comparator): comparison.value\\n            }\\n        }\\n\\n    def visit_structured_query(\\n        self, structured_query: StructuredQuery\\n    ) -> Tuple[str, dict]:\\n        if structured_query.filter is None:\\n            kwargs = {}\\n        else:\\n            kwargs = {\"filter\": structured_query.filter.accept(self)}\\n        return structured_query.query, kwargs', metadata={'source': 'test_repo/libs/langchain/langchain/retrievers/self_query/pgvector.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='\"\"\"Logic for converting internal query language to a valid AstraDB query.\"\"\"\\nfrom typing import Dict, Tuple, Union\\n\\nfrom langchain.chains.query_constructor.ir import (\\n    Comparator,\\n    Comparison,\\n    Operation,\\n    Operator,\\n    StructuredQuery,\\n    Visitor,\\n)\\n\\nMULTIPLE_ARITY_COMPARATORS = [Comparator.IN, Comparator.NIN]', metadata={'source': 'test_repo/libs/langchain/langchain/retrievers/self_query/astradb.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='class AstraDBTranslator(Visitor):\\n    \"\"\"Translate AstraDB internal query language elements to valid filters.\"\"\"\\n\\n    \"\"\"Subset of allowed logical comparators.\"\"\"\\n    allowed_comparators = [\\n        Comparator.EQ,\\n        Comparator.NE,\\n        Comparator.GT,\\n        Comparator.GTE,\\n        Comparator.LT,\\n        Comparator.LTE,\\n        Comparator.IN,\\n        Comparator.NIN,\\n    ]\\n\\n    \"\"\"Subset of allowed logical operators.\"\"\"\\n    allowed_operators = [Operator.AND, Operator.OR]\\n\\n    def _format_func(self, func: Union[Operator, Comparator]) -> str:\\n        self._validate_func(func)\\n        map_dict = {\\n            Operator.AND: \"$and\",\\n            Operator.OR: \"$or\",\\n            Comparator.EQ: \"$eq\",\\n            Comparator.NE: \"$ne\",\\n            Comparator.GTE: \"$gte\",\\n            Comparator.LTE: \"$lte\",\\n            Comparator.LT: \"$lt\",\\n            Comparator.GT: \"$gt\",\\n            Comparator.IN: \"$in\",\\n            Comparator.NIN: \"$nin\",\\n        }\\n        return map_dict[func]', metadata={'source': 'test_repo/libs/langchain/langchain/retrievers/self_query/astradb.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='def visit_operation(self, operation: Operation) -> Dict:\\n        args = [arg.accept(self) for arg in operation.arguments]\\n        return {self._format_func(operation.operator): args}\\n\\n    def visit_comparison(self, comparison: Comparison) -> Dict:\\n        if comparison.comparator in MULTIPLE_ARITY_COMPARATORS and not isinstance(\\n            comparison.value, list\\n        ):\\n            comparison.value = [comparison.value]\\n\\n        comparator = self._format_func(comparison.comparator)\\n        return {comparison.attribute: {comparator: comparison.value}}\\n\\n    def visit_structured_query(\\n        self, structured_query: StructuredQuery\\n    ) -> Tuple[str, dict]:\\n        if structured_query.filter is None:\\n            kwargs = {}\\n        else:\\n            kwargs = {\"filter\": structured_query.filter.accept(self)}\\n        return structured_query.query, kwargs', metadata={'source': 'test_repo/libs/langchain/langchain/retrievers/self_query/astradb.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from typing import Dict, Tuple, Union\\n\\nfrom langchain.chains.query_constructor.ir import (\\n    Comparator,\\n    Comparison,\\n    Operation,\\n    Operator,\\n    StructuredQuery,\\n    Visitor,\\n)', metadata={'source': 'test_repo/libs/langchain/langchain/retrievers/self_query/opensearch.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='class OpenSearchTranslator(Visitor):\\n    \"\"\"Translate `OpenSearch` internal query domain-specific\\n    language elements to valid filters.\"\"\"\\n\\n    allowed_comparators = [\\n        Comparator.EQ,\\n        Comparator.LT,\\n        Comparator.LTE,\\n        Comparator.GT,\\n        Comparator.GTE,\\n        Comparator.CONTAIN,\\n        Comparator.LIKE,\\n    ]\\n    \"\"\"Subset of allowed logical comparators.\"\"\"\\n\\n    allowed_operators = [Operator.AND, Operator.OR, Operator.NOT]\\n    \"\"\"Subset of allowed logical operators.\"\"\"', metadata={'source': 'test_repo/libs/langchain/langchain/retrievers/self_query/opensearch.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='def _format_func(self, func: Union[Operator, Comparator]) -> str:\\n        self._validate_func(func)\\n        comp_operator_map = {\\n            Comparator.EQ: \"term\",\\n            Comparator.LT: \"lt\",\\n            Comparator.LTE: \"lte\",\\n            Comparator.GT: \"gt\",\\n            Comparator.GTE: \"gte\",\\n            Comparator.CONTAIN: \"match\",\\n            Comparator.LIKE: \"fuzzy\",\\n            Operator.AND: \"must\",\\n            Operator.OR: \"should\",\\n            Operator.NOT: \"must_not\",\\n        }\\n        return comp_operator_map[func]\\n\\n    def visit_operation(self, operation: Operation) -> Dict:\\n        args = [arg.accept(self) for arg in operation.arguments]\\n\\n        return {\"bool\": {self._format_func(operation.operator): args}}\\n\\n    def visit_comparison(self, comparison: Comparison) -> Dict:\\n        field = f\"metadata.{comparison.attribute}\"', metadata={'source': 'test_repo/libs/langchain/langchain/retrievers/self_query/opensearch.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='if comparison.comparator in [\\n            Comparator.LT,\\n            Comparator.LTE,\\n            Comparator.GT,\\n            Comparator.GTE,\\n        ]:\\n            if isinstance(comparison.value, dict):\\n                if \"date\" in comparison.value:\\n                    return {\\n                        \"range\": {\\n                            field: {\\n                                self._format_func(\\n                                    comparison.comparator\\n                                ): comparison.value[\"date\"]\\n                            }\\n                        }\\n                    }\\n            else:\\n                return {\\n                    \"range\": {\\n                        field: {\\n                            self._format_func(comparison.comparator): comparison.value\\n                        }\\n                    }\\n                }', metadata={'source': 'test_repo/libs/langchain/langchain/retrievers/self_query/opensearch.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='if comparison.comparator == Comparator.LIKE:\\n            return {\\n                self._format_func(comparison.comparator): {\\n                    field: {\"value\": comparison.value}\\n                }\\n            }\\n\\n        field = f\"{field}.keyword\" if isinstance(comparison.value, str) else field\\n\\n        if isinstance(comparison.value, dict):\\n            if \"date\" in comparison.value:\\n                comparison.value = comparison.value[\"date\"]\\n\\n        return {self._format_func(comparison.comparator): {field: comparison.value}}\\n\\n    def visit_structured_query(\\n        self, structured_query: StructuredQuery\\n    ) -> Tuple[str, dict]:\\n        if structured_query.filter is None:\\n            kwargs = {}\\n        else:\\n            kwargs = {\"filter\": structured_query.filter.accept(self)}\\n\\n        return structured_query.query, kwargs', metadata={'source': 'test_repo/libs/langchain/langchain/retrievers/self_query/opensearch.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from typing import Tuple, Union\\n\\nfrom langchain.chains.query_constructor.ir import (\\n    Comparator,\\n    Comparison,\\n    Operation,\\n    Operator,\\n    StructuredQuery,\\n    Visitor,\\n)\\n\\n\\ndef process_value(value: Union[int, float, str]) -> str:\\n    \"\"\"Convert a value to a string and add single quotes if it is a string.\"\"\"\\n    if isinstance(value, str):\\n        return f\"\\'{value}\\'\"\\n    else:\\n        return str(value)', metadata={'source': 'test_repo/libs/langchain/langchain/retrievers/self_query/vectara.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='class VectaraTranslator(Visitor):\\n    \"\"\"Translate `Vectara` internal query language elements to valid filters.\"\"\"\\n\\n    allowed_operators = [Operator.AND, Operator.OR]\\n    \"\"\"Subset of allowed logical operators.\"\"\"\\n    allowed_comparators = [\\n        Comparator.EQ,\\n        Comparator.NE,\\n        Comparator.GT,\\n        Comparator.GTE,\\n        Comparator.LT,\\n        Comparator.LTE,\\n    ]\\n    \"\"\"Subset of allowed logical comparators.\"\"\"\\n\\n    def _format_func(self, func: Union[Operator, Comparator]) -> str:\\n        map_dict = {\\n            Operator.AND: \" and \",\\n            Operator.OR: \" or \",\\n            Comparator.EQ: \"=\",\\n            Comparator.NE: \"!=\",\\n            Comparator.GT: \">\",\\n            Comparator.GTE: \">=\",\\n            Comparator.LT: \"<\",\\n            Comparator.LTE: \"<=\",\\n        }\\n        self._validate_func(func)\\n        return map_dict[func]', metadata={'source': 'test_repo/libs/langchain/langchain/retrievers/self_query/vectara.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='def visit_operation(self, operation: Operation) -> str:\\n        args = [arg.accept(self) for arg in operation.arguments]\\n        operator = self._format_func(operation.operator)\\n        return \"( \" + operator.join(args) + \" )\"\\n\\n    def visit_comparison(self, comparison: Comparison) -> str:\\n        comparator = self._format_func(comparison.comparator)\\n        processed_value = process_value(comparison.value)\\n        attribute = comparison.attribute\\n        return (\\n            \"( \" + \"doc.\" + attribute + \" \" + comparator + \" \" + processed_value + \" )\"\\n        )\\n\\n    def visit_structured_query(\\n        self, structured_query: StructuredQuery\\n    ) -> Tuple[str, dict]:\\n        if structured_query.filter is None:\\n            kwargs = {}\\n        else:\\n            kwargs = {\"filter\": structured_query.filter.accept(self)}\\n        return structured_query.query, kwargs', metadata={'source': 'test_repo/libs/langchain/langchain/retrievers/self_query/vectara.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='\"\"\"Retriever that generates and executes structured queries over its own data source.\"\"\"\\nimport logging\\nfrom typing import Any, Dict, List, Optional, Sequence, Tuple, Type, Union\\n\\nfrom langchain_community.vectorstores import (\\n    AstraDB,\\n    Chroma,\\n    DashVector,\\n    DeepLake,\\n    Dingo,\\n    ElasticsearchStore,\\n    Milvus,\\n    MongoDBAtlasVectorSearch,\\n    MyScale,\\n    OpenSearchVectorSearch,\\n    PGVector,\\n    Pinecone,\\n    Qdrant,\\n    Redis,\\n    SupabaseVectorStore,\\n    TimescaleVector,\\n    Vectara,\\n    Weaviate,\\n)\\nfrom langchain_core.documents import Document\\nfrom langchain_core.language_models import BaseLanguageModel\\nfrom langchain_core.pydantic_v1 import Field, root_validator\\nfrom langchain_core.retrievers import BaseRetriever\\nfrom langchain_core.runnables import Runnable\\nfrom langchain_core.vectorstores import VectorStore', metadata={'source': 'test_repo/libs/langchain/langchain/retrievers/self_query/base.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from langchain.callbacks.manager import (\\n    AsyncCallbackManagerForRetrieverRun,\\n    CallbackManagerForRetrieverRun,\\n)\\nfrom langchain.chains.query_constructor.base import load_query_constructor_runnable\\nfrom langchain.chains.query_constructor.ir import StructuredQuery, Visitor\\nfrom langchain.chains.query_constructor.schema import AttributeInfo\\nfrom langchain.retrievers.self_query.astradb import AstraDBTranslator\\nfrom langchain.retrievers.self_query.chroma import ChromaTranslator\\nfrom langchain.retrievers.self_query.dashvector import DashvectorTranslator\\nfrom langchain.retrievers.self_query.deeplake import DeepLakeTranslator\\nfrom langchain.retrievers.self_query.dingo import DingoDBTranslator\\nfrom langchain.retrievers.self_query.elasticsearch import ElasticsearchTranslator\\nfrom langchain.retrievers.self_query.milvus import MilvusTranslator\\nfrom langchain.retrievers.self_query.mongodb_atlas import MongoDBAtlasTranslator', metadata={'source': 'test_repo/libs/langchain/langchain/retrievers/self_query/base.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from langchain.retrievers.self_query.mongodb_atlas import MongoDBAtlasTranslator\\nfrom langchain.retrievers.self_query.myscale import MyScaleTranslator\\nfrom langchain.retrievers.self_query.opensearch import OpenSearchTranslator\\nfrom langchain.retrievers.self_query.pgvector import PGVectorTranslator\\nfrom langchain.retrievers.self_query.pinecone import PineconeTranslator\\nfrom langchain.retrievers.self_query.qdrant import QdrantTranslator\\nfrom langchain.retrievers.self_query.redis import RedisTranslator\\nfrom langchain.retrievers.self_query.supabase import SupabaseVectorTranslator\\nfrom langchain.retrievers.self_query.timescalevector import TimescaleVectorTranslator\\nfrom langchain.retrievers.self_query.vectara import VectaraTranslator\\nfrom langchain.retrievers.self_query.weaviate import WeaviateTranslator', metadata={'source': 'test_repo/libs/langchain/langchain/retrievers/self_query/base.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='logger = logging.getLogger(__name__)\\nQUERY_CONSTRUCTOR_RUN_NAME = \"query_constructor\"', metadata={'source': 'test_repo/libs/langchain/langchain/retrievers/self_query/base.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='def _get_builtin_translator(vectorstore: VectorStore) -> Visitor:\\n    \"\"\"Get the translator class corresponding to the vector store class.\"\"\"\\n    BUILTIN_TRANSLATORS: Dict[Type[VectorStore], Type[Visitor]] = {\\n        AstraDB: AstraDBTranslator,\\n        PGVector: PGVectorTranslator,\\n        Pinecone: PineconeTranslator,\\n        Chroma: ChromaTranslator,\\n        DashVector: DashvectorTranslator,\\n        Dingo: DingoDBTranslator,\\n        Weaviate: WeaviateTranslator,\\n        Vectara: VectaraTranslator,\\n        Qdrant: QdrantTranslator,\\n        MyScale: MyScaleTranslator,\\n        DeepLake: DeepLakeTranslator,\\n        ElasticsearchStore: ElasticsearchTranslator,\\n        Milvus: MilvusTranslator,\\n        SupabaseVectorStore: SupabaseVectorTranslator,\\n        TimescaleVector: TimescaleVectorTranslator,\\n        OpenSearchVectorSearch: OpenSearchTranslator,\\n        MongoDBAtlasVectorSearch: MongoDBAtlasTranslator,\\n    }', metadata={'source': 'test_repo/libs/langchain/langchain/retrievers/self_query/base.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='if isinstance(vectorstore, Qdrant):\\n        return QdrantTranslator(metadata_key=vectorstore.metadata_payload_key)\\n    elif isinstance(vectorstore, MyScale):\\n        return MyScaleTranslator(metadata_key=vectorstore.metadata_column)\\n    elif isinstance(vectorstore, Redis):\\n        return RedisTranslator.from_vectorstore(vectorstore)\\n    elif vectorstore.__class__ in BUILTIN_TRANSLATORS:\\n        return BUILTIN_TRANSLATORS[vectorstore.__class__]()\\n    else:\\n        try:\\n            from langchain_astradb.vectorstores import AstraDBVectorStore\\n\\n            if isinstance(vectorstore, AstraDBVectorStore):\\n                return AstraDBTranslator()\\n        except ImportError:\\n            pass\\n\\n        raise ValueError(\\n            f\"Self query retriever with Vector Store type {vectorstore.__class__}\"\\n            f\" not supported.\"\\n        )', metadata={'source': 'test_repo/libs/langchain/langchain/retrievers/self_query/base.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='class SelfQueryRetriever(BaseRetriever):\\n    \"\"\"Retriever that uses a vector store and an LLM to generate\\n    the vector store queries.\"\"\"\\n\\n    vectorstore: VectorStore\\n    \"\"\"The underlying vector store from which documents will be retrieved.\"\"\"\\n    query_constructor: Runnable[dict, StructuredQuery] = Field(alias=\"llm_chain\")\\n    \"\"\"The query constructor chain for generating the vector store queries.\\n    \\n    llm_chain is legacy name kept for backwards compatibility.\"\"\"\\n    search_type: str = \"similarity\"\\n    \"\"\"The search type to perform on the vector store.\"\"\"\\n    search_kwargs: dict = Field(default_factory=dict)\\n    \"\"\"Keyword arguments to pass in to the vector store search.\"\"\"\\n    structured_query_translator: Visitor\\n    \"\"\"Translator for turning internal query language into vectorstore search params.\"\"\"\\n    verbose: bool = False\\n\\n    use_original_query: bool = False\\n    \"\"\"Use original query instead of the revised new query from LLM\"\"\"', metadata={'source': 'test_repo/libs/langchain/langchain/retrievers/self_query/base.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='class Config:\\n        \"\"\"Configuration for this pydantic object.\"\"\"\\n\\n        arbitrary_types_allowed = True\\n        allow_population_by_field_name = True\\n\\n    @root_validator(pre=True)\\n    def validate_translator(cls, values: Dict) -> Dict:\\n        \"\"\"Validate translator.\"\"\"\\n        if \"structured_query_translator\" not in values:\\n            values[\"structured_query_translator\"] = _get_builtin_translator(\\n                values[\"vectorstore\"]\\n            )\\n        return values\\n\\n    @property\\n    def llm_chain(self) -> Runnable:\\n        \"\"\"llm_chain is legacy name kept for backwards compatibility.\"\"\"\\n        return self.query_constructor', metadata={'source': 'test_repo/libs/langchain/langchain/retrievers/self_query/base.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='def _prepare_query(\\n        self, query: str, structured_query: StructuredQuery\\n    ) -> Tuple[str, Dict[str, Any]]:\\n        new_query, new_kwargs = self.structured_query_translator.visit_structured_query(\\n            structured_query\\n        )\\n        if structured_query.limit is not None:\\n            new_kwargs[\"k\"] = structured_query.limit\\n        if self.use_original_query:\\n            new_query = query\\n        search_kwargs = {**self.search_kwargs, **new_kwargs}\\n        return new_query, search_kwargs\\n\\n    def _get_docs_with_query(\\n        self, query: str, search_kwargs: Dict[str, Any]\\n    ) -> List[Document]:\\n        docs = self.vectorstore.search(query, self.search_type, **search_kwargs)\\n        return docs\\n\\n    async def _aget_docs_with_query(\\n        self, query: str, search_kwargs: Dict[str, Any]\\n    ) -> List[Document]:\\n        docs = await self.vectorstore.asearch(query, self.search_type, **search_kwargs)\\n        return docs', metadata={'source': 'test_repo/libs/langchain/langchain/retrievers/self_query/base.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='def _get_relevant_documents(\\n        self, query: str, *, run_manager: CallbackManagerForRetrieverRun\\n    ) -> List[Document]:\\n        \"\"\"Get documents relevant for a query.\\n\\n        Args:\\n            query: string to find relevant documents for\\n\\n        Returns:\\n            List of relevant documents\\n        \"\"\"\\n        structured_query = self.query_constructor.invoke(\\n            {\"query\": query}, config={\"callbacks\": run_manager.get_child()}\\n        )\\n        if self.verbose:\\n            logger.info(f\"Generated Query: {structured_query}\")\\n        new_query, search_kwargs = self._prepare_query(query, structured_query)\\n        docs = self._get_docs_with_query(new_query, search_kwargs)\\n        return docs\\n\\n    async def _aget_relevant_documents(\\n        self, query: str, *, run_manager: AsyncCallbackManagerForRetrieverRun\\n    ) -> List[Document]:\\n        \"\"\"Get documents relevant for a query.\\n\\n        Args:\\n            query: string to find relevant documents for', metadata={'source': 'test_repo/libs/langchain/langchain/retrievers/self_query/base.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='Args:\\n            query: string to find relevant documents for\\n\\n        Returns:\\n            List of relevant documents\\n        \"\"\"\\n        structured_query = await self.query_constructor.ainvoke(\\n            {\"query\": query}, config={\"callbacks\": run_manager.get_child()}\\n        )\\n        if self.verbose:\\n            logger.info(f\"Generated Query: {structured_query}\")\\n        new_query, search_kwargs = self._prepare_query(query, structured_query)\\n        docs = await self._aget_docs_with_query(new_query, search_kwargs)\\n        return docs', metadata={'source': 'test_repo/libs/langchain/langchain/retrievers/self_query/base.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='@classmethod\\n    def from_llm(\\n        cls,\\n        llm: BaseLanguageModel,\\n        vectorstore: VectorStore,\\n        document_contents: str,\\n        metadata_field_info: Sequence[Union[AttributeInfo, dict]],\\n        structured_query_translator: Optional[Visitor] = None,\\n        chain_kwargs: Optional[Dict] = None,\\n        enable_limit: bool = False,\\n        use_original_query: bool = False,\\n        **kwargs: Any,\\n    ) -> \"SelfQueryRetriever\":\\n        if structured_query_translator is None:\\n            structured_query_translator = _get_builtin_translator(vectorstore)\\n        chain_kwargs = chain_kwargs or {}', metadata={'source': 'test_repo/libs/langchain/langchain/retrievers/self_query/base.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='if (\\n            \"allowed_comparators\" not in chain_kwargs\\n            and structured_query_translator.allowed_comparators is not None\\n        ):\\n            chain_kwargs[\\n                \"allowed_comparators\"\\n            ] = structured_query_translator.allowed_comparators\\n        if (\\n            \"allowed_operators\" not in chain_kwargs\\n            and structured_query_translator.allowed_operators is not None\\n        ):\\n            chain_kwargs[\\n                \"allowed_operators\"\\n            ] = structured_query_translator.allowed_operators\\n        query_constructor = load_query_constructor_runnable(\\n            llm,\\n            document_contents,\\n            metadata_field_info,\\n            enable_limit=enable_limit,\\n            **chain_kwargs,\\n        )\\n        query_constructor = query_constructor.with_config(\\n            run_name=QUERY_CONSTRUCTOR_RUN_NAME\\n        )\\n        return cls(\\n            query_constructor=query_constructor,\\n            vectorstore=vectorstore,', metadata={'source': 'test_repo/libs/langchain/langchain/retrievers/self_query/base.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='query_constructor=query_constructor,\\n            vectorstore=vectorstore,\\n            use_original_query=use_original_query,\\n            structured_query_translator=structured_query_translator,\\n            **kwargs,\\n        )', metadata={'source': 'test_repo/libs/langchain/langchain/retrievers/self_query/base.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from datetime import datetime\\nfrom typing import Dict, Tuple, Union\\n\\nfrom langchain.chains.query_constructor.ir import (\\n    Comparator,\\n    Comparison,\\n    Operation,\\n    Operator,\\n    StructuredQuery,\\n    Visitor,\\n)', metadata={'source': 'test_repo/libs/langchain/langchain/retrievers/self_query/weaviate.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='class WeaviateTranslator(Visitor):\\n    \"\"\"Translate `Weaviate` internal query language elements to valid filters.\"\"\"\\n\\n    allowed_operators = [Operator.AND, Operator.OR]\\n    \"\"\"Subset of allowed logical operators.\"\"\"\\n\\n    allowed_comparators = [\\n        Comparator.EQ,\\n        Comparator.NE,\\n        Comparator.GTE,\\n        Comparator.LTE,\\n        Comparator.LT,\\n        Comparator.GT,\\n    ]\\n\\n    def _format_func(self, func: Union[Operator, Comparator]) -> str:\\n        self._validate_func(func)\\n        # https://weaviate.io/developers/weaviate/api/graphql/filters\\n        map_dict = {\\n            Operator.AND: \"And\",\\n            Operator.OR: \"Or\",\\n            Comparator.EQ: \"Equal\",\\n            Comparator.NE: \"NotEqual\",\\n            Comparator.GTE: \"GreaterThanEqual\",\\n            Comparator.LTE: \"LessThanEqual\",\\n            Comparator.LT: \"LessThan\",\\n            Comparator.GT: \"GreaterThan\",\\n        }\\n        return map_dict[func]', metadata={'source': 'test_repo/libs/langchain/langchain/retrievers/self_query/weaviate.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='def visit_operation(self, operation: Operation) -> Dict:\\n        args = [arg.accept(self) for arg in operation.arguments]\\n        return {\"operator\": self._format_func(operation.operator), \"operands\": args}', metadata={'source': 'test_repo/libs/langchain/langchain/retrievers/self_query/weaviate.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='def visit_comparison(self, comparison: Comparison) -> Dict:\\n        value_type = \"valueText\"\\n        value = comparison.value\\n        if isinstance(comparison.value, bool):\\n            value_type = \"valueBoolean\"\\n        elif isinstance(comparison.value, float):\\n            value_type = \"valueNumber\"\\n        elif isinstance(comparison.value, int):\\n            value_type = \"valueInt\"\\n        elif (\\n            isinstance(comparison.value, dict)\\n            and comparison.value.get(\"type\") == \"date\"\\n        ):\\n            value_type = \"valueDate\"\\n            # ISO 8601 timestamp, formatted as RFC3339\\n            date = datetime.strptime(comparison.value[\"date\"], \"%Y-%m-%d\")\\n            value = date.strftime(\"%Y-%m-%dT%H:%M:%SZ\")\\n        filter = {\\n            \"path\": [comparison.attribute],\\n            \"operator\": self._format_func(comparison.comparator),\\n            value_type: value,\\n        }\\n        return filter', metadata={'source': 'test_repo/libs/langchain/langchain/retrievers/self_query/weaviate.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='def visit_structured_query(\\n        self, structured_query: StructuredQuery\\n    ) -> Tuple[str, dict]:\\n        if structured_query.filter is None:\\n            kwargs = {}\\n        else:\\n            kwargs = {\"where_filter\": structured_query.filter.accept(self)}\\n        return structured_query.query, kwargs', metadata={'source': 'test_repo/libs/langchain/langchain/retrievers/self_query/weaviate.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='\"\"\"Logic for converting internal query language to a valid Milvus query.\"\"\"\\nfrom typing import Tuple, Union\\n\\nfrom langchain.chains.query_constructor.ir import (\\n    Comparator,\\n    Comparison,\\n    Operation,\\n    Operator,\\n    StructuredQuery,\\n    Visitor,\\n)\\n\\nCOMPARATOR_TO_BER = {\\n    Comparator.EQ: \"==\",\\n    Comparator.GT: \">\",\\n    Comparator.GTE: \">=\",\\n    Comparator.LT: \"<\",\\n    Comparator.LTE: \"<=\",\\n    Comparator.IN: \"in\",\\n    Comparator.LIKE: \"like\",\\n}\\n\\nUNARY_OPERATORS = [Operator.NOT]', metadata={'source': 'test_repo/libs/langchain/langchain/retrievers/self_query/milvus.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='def process_value(value: Union[int, float, str], comparator: Comparator) -> str:\\n    \"\"\"Convert a value to a string and add double quotes if it is a string.\\n\\n    It required for comparators involving strings.\\n\\n    Args:\\n        value: The value to convert.\\n        comparator: The comparator.\\n\\n    Returns:\\n        The converted value as a string.\\n    \"\"\"\\n    #\\n    if isinstance(value, str):\\n        if comparator is Comparator.LIKE:\\n            # If the comparator is LIKE, add a percent sign after it for prefix matching\\n            # and add double quotes\\n            return f\\'\"{value}%\"\\'\\n        else:\\n            # If the value is already a string, add double quotes\\n            return f\\'\"{value}\"\\'\\n    else:\\n        # If the value is not a string, convert it to a string without double quotes\\n        return str(value)', metadata={'source': 'test_repo/libs/langchain/langchain/retrievers/self_query/milvus.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='class MilvusTranslator(Visitor):\\n    \"\"\"Translate Milvus internal query language elements to valid filters.\"\"\"\\n\\n    \"\"\"Subset of allowed logical operators.\"\"\"\\n    allowed_operators = [Operator.AND, Operator.NOT, Operator.OR]\\n\\n    \"\"\"Subset of allowed logical comparators.\"\"\"\\n    allowed_comparators = [\\n        Comparator.EQ,\\n        Comparator.GT,\\n        Comparator.GTE,\\n        Comparator.LT,\\n        Comparator.LTE,\\n        Comparator.IN,\\n        Comparator.LIKE,\\n    ]\\n\\n    def _format_func(self, func: Union[Operator, Comparator]) -> str:\\n        self._validate_func(func)\\n        value = func.value\\n        if isinstance(func, Comparator):\\n            value = COMPARATOR_TO_BER[func]\\n        return f\"{value}\"', metadata={'source': 'test_repo/libs/langchain/langchain/retrievers/self_query/milvus.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='def visit_operation(self, operation: Operation) -> str:\\n        if operation.operator in UNARY_OPERATORS and len(operation.arguments) == 1:\\n            operator = self._format_func(operation.operator)\\n            return operator + \"(\" + operation.arguments[0].accept(self) + \")\"\\n        elif operation.operator in UNARY_OPERATORS:\\n            raise ValueError(\\n                f\\'\"{operation.operator.value}\" can have only one argument in Milvus\\'\\n            )\\n        else:\\n            args = [arg.accept(self) for arg in operation.arguments]\\n            operator = self._format_func(operation.operator)\\n            return \"(\" + (\" \" + operator + \" \").join(args) + \")\"\\n\\n    def visit_comparison(self, comparison: Comparison) -> str:\\n        comparator = self._format_func(comparison.comparator)\\n        processed_value = process_value(comparison.value, comparison.comparator)\\n        attribute = comparison.attribute', metadata={'source': 'test_repo/libs/langchain/langchain/retrievers/self_query/milvus.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='return \"( \" + attribute + \" \" + comparator + \" \" + processed_value + \" )\"\\n\\n    def visit_structured_query(\\n        self, structured_query: StructuredQuery\\n    ) -> Tuple[str, dict]:\\n        if structured_query.filter is None:\\n            kwargs = {}\\n        else:\\n            kwargs = {\"expr\": structured_query.filter.accept(self)}\\n        return structured_query.query, kwargs', metadata={'source': 'test_repo/libs/langchain/langchain/retrievers/self_query/milvus.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='\"\"\"Logic for converting internal query language to a valid Chroma query.\"\"\"\\nfrom typing import Tuple, Union\\n\\nfrom langchain.chains.query_constructor.ir import (\\n    Comparator,\\n    Comparison,\\n    Operation,\\n    Operator,\\n    StructuredQuery,\\n    Visitor,\\n)\\n\\nCOMPARATOR_TO_TQL = {\\n    Comparator.EQ: \"==\",\\n    Comparator.GT: \">\",\\n    Comparator.GTE: \">=\",\\n    Comparator.LT: \"<\",\\n    Comparator.LTE: \"<=\",\\n}\\n\\n\\nOPERATOR_TO_TQL = {\\n    Operator.AND: \"and\",\\n    Operator.OR: \"or\",\\n    Operator.NOT: \"NOT\",\\n}\\n\\n\\ndef can_cast_to_float(string: str) -> bool:\\n    \"\"\"Check if a string can be cast to a float.\"\"\"\\n    try:\\n        float(string)\\n        return True\\n    except ValueError:\\n        return False', metadata={'source': 'test_repo/libs/langchain/langchain/retrievers/self_query/deeplake.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='class DeepLakeTranslator(Visitor):\\n    \"\"\"Translate `DeepLake` internal query language elements to valid filters.\"\"\"\\n\\n    allowed_operators = [Operator.AND, Operator.OR, Operator.NOT]\\n    \"\"\"Subset of allowed logical operators.\"\"\"\\n    allowed_comparators = [\\n        Comparator.EQ,\\n        Comparator.GT,\\n        Comparator.GTE,\\n        Comparator.LT,\\n        Comparator.LTE,\\n    ]\\n    \"\"\"Subset of allowed logical comparators.\"\"\"\\n\\n    def _format_func(self, func: Union[Operator, Comparator]) -> str:\\n        self._validate_func(func)\\n        if isinstance(func, Operator):\\n            value = OPERATOR_TO_TQL[func.value]  # type: ignore\\n        elif isinstance(func, Comparator):\\n            value = COMPARATOR_TO_TQL[func.value]  # type: ignore\\n        return f\"{value}\"', metadata={'source': 'test_repo/libs/langchain/langchain/retrievers/self_query/deeplake.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='def visit_operation(self, operation: Operation) -> str:\\n        args = [arg.accept(self) for arg in operation.arguments]\\n        operator = self._format_func(operation.operator)\\n        return \"(\" + (\" \" + operator + \" \").join(args) + \")\"\\n\\n    def visit_comparison(self, comparison: Comparison) -> str:\\n        comparator = self._format_func(comparison.comparator)\\n        values = comparison.value\\n        if isinstance(values, list):\\n            tql = []\\n            for value in values:\\n                comparison.value = value\\n                tql.append(self.visit_comparison(comparison))\\n\\n            return \"(\" + (\" or \").join(tql) + \")\"\\n\\n        if not can_cast_to_float(comparison.value):\\n            values = f\"\\'{values}\\'\"\\n        return f\"metadata[\\'{comparison.attribute}\\'] {comparator} {values}\"', metadata={'source': 'test_repo/libs/langchain/langchain/retrievers/self_query/deeplake.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='def visit_structured_query(\\n        self, structured_query: StructuredQuery\\n    ) -> Tuple[str, dict]:\\n        if structured_query.filter is None:\\n            kwargs = {}\\n        else:\\n            tqL = f\"SELECT * WHERE {structured_query.filter.accept(self)}\"\\n            kwargs = {\"tql\": tqL}\\n        return structured_query.query, kwargs', metadata={'source': 'test_repo/libs/langchain/langchain/retrievers/self_query/deeplake.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from typing import Any, Dict, Tuple\\n\\nfrom langchain.chains.query_constructor.ir import (\\n    Comparator,\\n    Comparison,\\n    Operation,\\n    Operator,\\n    StructuredQuery,\\n    Visitor,\\n)', metadata={'source': 'test_repo/libs/langchain/langchain/retrievers/self_query/supabase.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='class SupabaseVectorTranslator(Visitor):\\n    \"\"\"Translate Langchain filters to Supabase PostgREST filters.\"\"\"\\n\\n    allowed_operators = [Operator.AND, Operator.OR]\\n    \"\"\"Subset of allowed logical operators.\"\"\"\\n\\n    allowed_comparators = [\\n        Comparator.EQ,\\n        Comparator.NE,\\n        Comparator.GT,\\n        Comparator.GTE,\\n        Comparator.LT,\\n        Comparator.LTE,\\n        Comparator.LIKE,\\n    ]\\n    \"\"\"Subset of allowed logical comparators.\"\"\"\\n\\n    metadata_column = \"metadata\"\\n\\n    def _map_comparator(self, comparator: Comparator) -> str:\\n        \"\"\"\\n        Maps Langchain comparator to PostgREST comparator:', metadata={'source': 'test_repo/libs/langchain/langchain/retrievers/self_query/supabase.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='https://postgrest.org/en/stable/references/api/tables_views.html#operators\\n        \"\"\"\\n        postgrest_comparator = {\\n            Comparator.EQ: \"eq\",\\n            Comparator.NE: \"neq\",\\n            Comparator.GT: \"gt\",\\n            Comparator.GTE: \"gte\",\\n            Comparator.LT: \"lt\",\\n            Comparator.LTE: \"lte\",\\n            Comparator.LIKE: \"like\",\\n        }.get(comparator)\\n\\n        if postgrest_comparator is None:\\n            raise Exception(\\n                f\"Comparator \\'{comparator}\\' is not currently \"\\n                \"supported in Supabase Vector\"\\n            )\\n\\n        return postgrest_comparator\\n\\n    def _get_json_operator(self, value: Any) -> str:\\n        if isinstance(value, str):\\n            return \"->>\"\\n        else:\\n            return \"->\"\\n\\n    def visit_operation(self, operation: Operation) -> str:\\n        args = [arg.accept(self) for arg in operation.arguments]\\n        return f\"{operation.operator.value}({\\',\\'.join(args)})\"', metadata={'source': 'test_repo/libs/langchain/langchain/retrievers/self_query/supabase.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='def visit_comparison(self, comparison: Comparison) -> str:\\n        if isinstance(comparison.value, list):\\n            return self.visit_operation(\\n                Operation(\\n                    operator=Operator.AND,\\n                    arguments=[\\n                        Comparison(\\n                            comparator=comparison.comparator,\\n                            attribute=comparison.attribute,\\n                            value=value,\\n                        )\\n                        for value in comparison.value\\n                    ],\\n                )\\n            )\\n\\n        return \".\".join(\\n            [\\n                f\"{self.metadata_column}{self._get_json_operator(comparison.value)}{comparison.attribute}\",\\n                f\"{self._map_comparator(comparison.comparator)}\",\\n                f\"{comparison.value}\",\\n            ]\\n        )', metadata={'source': 'test_repo/libs/langchain/langchain/retrievers/self_query/supabase.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='def visit_structured_query(\\n        self, structured_query: StructuredQuery\\n    ) -> Tuple[str, Dict[str, str]]:\\n        if structured_query.filter is None:\\n            kwargs = {}\\n        else:\\n            kwargs = {\"postgrest_filter\": structured_query.filter.accept(self)}\\n        return structured_query.query, kwargs', metadata={'source': 'test_repo/libs/langchain/langchain/retrievers/self_query/supabase.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from typing import Dict, Tuple, Union\\n\\nfrom langchain.chains.query_constructor.ir import (\\n    Comparator,\\n    Comparison,\\n    Operation,\\n    Operator,\\n    StructuredQuery,\\n    Visitor,\\n)', metadata={'source': 'test_repo/libs/langchain/langchain/retrievers/self_query/pinecone.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='class PineconeTranslator(Visitor):\\n    \"\"\"Translate `Pinecone` internal query language elements to valid filters.\"\"\"\\n\\n    allowed_comparators = (\\n        Comparator.EQ,\\n        Comparator.NE,\\n        Comparator.LT,\\n        Comparator.LTE,\\n        Comparator.GT,\\n        Comparator.GTE,\\n        Comparator.IN,\\n        Comparator.NIN,\\n    )\\n    \"\"\"Subset of allowed logical comparators.\"\"\"\\n    allowed_operators = (Operator.AND, Operator.OR)\\n    \"\"\"Subset of allowed logical operators.\"\"\"\\n\\n    def _format_func(self, func: Union[Operator, Comparator]) -> str:\\n        self._validate_func(func)\\n        return f\"${func.value}\"\\n\\n    def visit_operation(self, operation: Operation) -> Dict:\\n        args = [arg.accept(self) for arg in operation.arguments]\\n        return {self._format_func(operation.operator): args}', metadata={'source': 'test_repo/libs/langchain/langchain/retrievers/self_query/pinecone.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='def visit_comparison(self, comparison: Comparison) -> Dict:\\n        if comparison.comparator in (Comparator.IN, Comparator.NIN) and not isinstance(\\n            comparison.value, list\\n        ):\\n            comparison.value = [comparison.value]\\n\\n        return {\\n            comparison.attribute: {\\n                self._format_func(comparison.comparator): comparison.value\\n            }\\n        }\\n\\n    def visit_structured_query(\\n        self, structured_query: StructuredQuery\\n    ) -> Tuple[str, dict]:\\n        if structured_query.filter is None:\\n            kwargs = {}\\n        else:\\n            kwargs = {\"filter\": structured_query.filter.accept(self)}\\n        return structured_query.query, kwargs', metadata={'source': 'test_repo/libs/langchain/langchain/retrievers/self_query/pinecone.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from langchain_community.embeddings.nlpcloud import NLPCloudEmbeddings\\n\\n__all__ = [\"NLPCloudEmbeddings\"]', metadata={'source': 'test_repo/libs/langchain/langchain/embeddings/nlpcloud.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from langchain_community.embeddings.elasticsearch import ElasticsearchEmbeddings\\n\\n__all__ = [\"ElasticsearchEmbeddings\"]', metadata={'source': 'test_repo/libs/langchain/langchain/embeddings/elasticsearch.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from langchain_community.embeddings.baidu_qianfan_endpoint import (\\n    QianfanEmbeddingsEndpoint,\\n)\\n\\n__all__ = [\"QianfanEmbeddingsEndpoint\"]', metadata={'source': 'test_repo/libs/langchain/langchain/embeddings/baidu_qianfan_endpoint.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from langchain_community.embeddings.gradient_ai import GradientEmbeddings\\n\\n__all__ = [\"GradientEmbeddings\"]', metadata={'source': 'test_repo/libs/langchain/langchain/embeddings/gradient_ai.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from langchain_community.embeddings.aleph_alpha import (\\n    AlephAlphaAsymmetricSemanticEmbedding,\\n    AlephAlphaSymmetricSemanticEmbedding,\\n)\\n\\n__all__ = [\\n    \"AlephAlphaAsymmetricSemanticEmbedding\",\\n    \"AlephAlphaSymmetricSemanticEmbedding\",\\n]', metadata={'source': 'test_repo/libs/langchain/langchain/embeddings/aleph_alpha.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from langchain_community.embeddings.octoai_embeddings import (\\n    OctoAIEmbeddings,\\n)\\n\\n__all__ = [\"OctoAIEmbeddings\"]', metadata={'source': 'test_repo/libs/langchain/langchain/embeddings/octoai_embeddings.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from langchain_community.embeddings.modelscope_hub import ModelScopeEmbeddings\\n\\n__all__ = [\"ModelScopeEmbeddings\"]', metadata={'source': 'test_repo/libs/langchain/langchain/embeddings/modelscope_hub.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from langchain_community.embeddings.azure_openai import AzureOpenAIEmbeddings\\n\\n__all__ = [\"AzureOpenAIEmbeddings\"]', metadata={'source': 'test_repo/libs/langchain/langchain/embeddings/azure_openai.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from langchain_community.embeddings.fastembed import FastEmbedEmbeddings\\n\\n__all__ = [\"FastEmbedEmbeddings\"]', metadata={'source': 'test_repo/libs/langchain/langchain/embeddings/fastembed.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from langchain_community.embeddings.johnsnowlabs import JohnSnowLabsEmbeddings\\n\\n__all__ = [\"JohnSnowLabsEmbeddings\"]', metadata={'source': 'test_repo/libs/langchain/langchain/embeddings/johnsnowlabs.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from langchain_community.embeddings.mlflow import MlflowEmbeddings\\n\\n__all__ = [\"MlflowEmbeddings\"]', metadata={'source': 'test_repo/libs/langchain/langchain/embeddings/mlflow.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from langchain_community.embeddings.dashscope import (\\n    DashScopeEmbeddings,\\n)\\n\\n__all__ = [\"DashScopeEmbeddings\"]', metadata={'source': 'test_repo/libs/langchain/langchain/embeddings/dashscope.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='\"\"\"**Embedding models**  are wrappers around embedding models\\nfrom different APIs and services.\\n\\n**Embedding models** can be LLMs or not.\\n\\n**Class hierarchy:**\\n\\n.. code-block::\\n\\n    Embeddings --> <name>Embeddings  # Examples: OpenAIEmbeddings, HuggingFaceEmbeddings\\n\"\"\"\\n\\n\\nimport logging\\nimport warnings\\nfrom typing import Any\\n\\nfrom langchain_core._api import LangChainDeprecationWarning\\n\\nfrom langchain.embeddings.cache import CacheBackedEmbeddings\\nfrom langchain.utils.interactive_env import is_interactive_env', metadata={'source': 'test_repo/libs/langchain/langchain/embeddings/__init__.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='def __getattr__(name: str) -> Any:\\n    from langchain_community import embeddings\\n\\n    # If not in interactive env, raise warning.\\n    if not is_interactive_env():\\n        warnings.warn(\\n            \"Importing embeddings from langchain is deprecated. Importing from \"\\n            \"langchain will no longer be supported as of langchain==0.2.0. \"\\n            \"Please import from langchain-community instead:\\\\n\\\\n\"\\n            f\"`from langchain_community.embeddings import {name}`.\\\\n\\\\n\"\\n            \"To install langchain-community run `pip install -U langchain-community`.\",\\n            category=LangChainDeprecationWarning,\\n        )\\n\\n    return getattr(embeddings, name)\\n\\n\\nlogger = logging.getLogger(__name__)', metadata={'source': 'test_repo/libs/langchain/langchain/embeddings/__init__.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='__all__ = [\\n    \"OpenAIEmbeddings\",\\n    \"AzureOpenAIEmbeddings\",\\n    \"CacheBackedEmbeddings\",\\n    \"ClarifaiEmbeddings\",\\n    \"CohereEmbeddings\",\\n    \"DatabricksEmbeddings\",\\n    \"ElasticsearchEmbeddings\",\\n    \"FastEmbedEmbeddings\",\\n    \"HuggingFaceEmbeddings\",\\n    \"HuggingFaceInferenceAPIEmbeddings\",\\n    \"InfinityEmbeddings\",\\n    \"GradientEmbeddings\",\\n    \"JinaEmbeddings\",\\n    \"LlamaCppEmbeddings\",\\n    \"HuggingFaceHubEmbeddings\",\\n    \"MlflowEmbeddings\",\\n    \"MlflowAIGatewayEmbeddings\",\\n    \"ModelScopeEmbeddings\",\\n    \"TensorflowHubEmbeddings\",\\n    \"SagemakerEndpointEmbeddings\",\\n    \"HuggingFaceInstructEmbeddings\",\\n    \"MosaicMLInstructorEmbeddings\",\\n    \"SelfHostedEmbeddings\",\\n    \"SelfHostedHuggingFaceEmbeddings\",\\n    \"SelfHostedHuggingFaceInstructEmbeddings\",\\n    \"FakeEmbeddings\",\\n    \"DeterministicFakeEmbedding\",\\n    \"AlephAlphaAsymmetricSemanticEmbedding\",\\n    \"AlephAlphaSymmetricSemanticEmbedding\",\\n    \"SentenceTransformerEmbeddings\",\\n    \"GooglePalmEmbeddings\",', metadata={'source': 'test_repo/libs/langchain/langchain/embeddings/__init__.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='\"SentenceTransformerEmbeddings\",\\n    \"GooglePalmEmbeddings\",\\n    \"MiniMaxEmbeddings\",\\n    \"VertexAIEmbeddings\",\\n    \"BedrockEmbeddings\",\\n    \"DeepInfraEmbeddings\",\\n    \"EdenAiEmbeddings\",\\n    \"DashScopeEmbeddings\",\\n    \"EmbaasEmbeddings\",\\n    \"OctoAIEmbeddings\",\\n    \"SpacyEmbeddings\",\\n    \"NLPCloudEmbeddings\",\\n    \"GPT4AllEmbeddings\",\\n    \"XinferenceEmbeddings\",\\n    \"LocalAIEmbeddings\",\\n    \"AwaEmbeddings\",\\n    \"HuggingFaceBgeEmbeddings\",\\n    \"ErnieEmbeddings\",\\n    \"JavelinAIGatewayEmbeddings\",\\n    \"OllamaEmbeddings\",\\n    \"QianfanEmbeddingsEndpoint\",\\n    \"JohnSnowLabsEmbeddings\",\\n    \"VoyageEmbeddings\",\\n    \"BookendEmbeddings\",\\n]', metadata={'source': 'test_repo/libs/langchain/langchain/embeddings/__init__.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='# TODO: this is in here to maintain backwards compatibility', metadata={'source': 'test_repo/libs/langchain/langchain/embeddings/__init__.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='class HypotheticalDocumentEmbedder:\\n    def __init__(self, *args: Any, **kwargs: Any):\\n        logger.warning(\\n            \"Using a deprecated class. Please use \"\\n            \"`from langchain.chains import HypotheticalDocumentEmbedder` instead\"\\n        )\\n        from langchain.chains.hyde.base import HypotheticalDocumentEmbedder as H\\n\\n        return H(*args, **kwargs)  # type: ignore\\n\\n    @classmethod\\n    def from_llm(cls, *args: Any, **kwargs: Any) -> Any:\\n        logger.warning(\\n            \"Using a deprecated class. Please use \"\\n            \"`from langchain.chains import HypotheticalDocumentEmbedder` instead\"\\n        )\\n        from langchain.chains.hyde.base import HypotheticalDocumentEmbedder as H\\n\\n        return H.from_llm(*args, **kwargs)', metadata={'source': 'test_repo/libs/langchain/langchain/embeddings/__init__.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from langchain_community.embeddings.localai import (\\n    LocalAIEmbeddings,\\n)\\n\\n__all__ = [\\n    \"LocalAIEmbeddings\",\\n]', metadata={'source': 'test_repo/libs/langchain/langchain/embeddings/localai.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from langchain_community.embeddings.tensorflow_hub import (\\n    TensorflowHubEmbeddings,\\n)\\n\\n__all__ = [\"TensorflowHubEmbeddings\"]', metadata={'source': 'test_repo/libs/langchain/langchain/embeddings/tensorflow_hub.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from langchain_community.embeddings.fake import (\\n    DeterministicFakeEmbedding,\\n    FakeEmbeddings,\\n)\\n\\n__all__ = [\"FakeEmbeddings\", \"DeterministicFakeEmbedding\"]', metadata={'source': 'test_repo/libs/langchain/langchain/embeddings/fake.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from langchain_community.embeddings.vertexai import VertexAIEmbeddings\\n\\n__all__ = [\"VertexAIEmbeddings\"]', metadata={'source': 'test_repo/libs/langchain/langchain/embeddings/vertexai.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from langchain_community.embeddings.spacy_embeddings import SpacyEmbeddings\\n\\n__all__ = [\"SpacyEmbeddings\"]', metadata={'source': 'test_repo/libs/langchain/langchain/embeddings/spacy_embeddings.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from langchain_community.embeddings.mosaicml import MosaicMLInstructorEmbeddings\\n\\n__all__ = [\"MosaicMLInstructorEmbeddings\"]', metadata={'source': 'test_repo/libs/langchain/langchain/embeddings/mosaicml.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from langchain_community.embeddings.cloudflare_workersai import (\\n    CloudflareWorkersAIEmbeddings,\\n)\\n\\n__all__ = [\"CloudflareWorkersAIEmbeddings\"]', metadata={'source': 'test_repo/libs/langchain/langchain/embeddings/cloudflare_workersai.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from langchain_community.embeddings.javelin_ai_gateway import (\\n    JavelinAIGatewayEmbeddings,\\n)\\n\\n__all__ = [\"JavelinAIGatewayEmbeddings\"]', metadata={'source': 'test_repo/libs/langchain/langchain/embeddings/javelin_ai_gateway.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from langchain_community.embeddings.llm_rails import LLMRailsEmbeddings\\n\\n__all__ = [\"LLMRailsEmbeddings\"]', metadata={'source': 'test_repo/libs/langchain/langchain/embeddings/llm_rails.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from langchain_community.embeddings.awa import AwaEmbeddings\\n\\n__all__ = [\"AwaEmbeddings\"]', metadata={'source': 'test_repo/libs/langchain/langchain/embeddings/awa.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from langchain_community.embeddings.minimax import (\\n    MiniMaxEmbeddings,\\n)\\n\\n__all__ = [\"MiniMaxEmbeddings\"]', metadata={'source': 'test_repo/libs/langchain/langchain/embeddings/minimax.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from langchain_community.embeddings.mlflow_gateway import (\\n    MlflowAIGatewayEmbeddings,\\n)\\n\\n__all__ = [\"MlflowAIGatewayEmbeddings\"]', metadata={'source': 'test_repo/libs/langchain/langchain/embeddings/mlflow_gateway.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from langchain_community.embeddings.huggingface_hub import (\\n    HuggingFaceHubEmbeddings,\\n)\\n\\n__all__ = [\"HuggingFaceHubEmbeddings\"]', metadata={'source': 'test_repo/libs/langchain/langchain/embeddings/huggingface_hub.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from langchain_community.embeddings.cohere import CohereEmbeddings\\n\\n__all__ = [\"CohereEmbeddings\"]', metadata={'source': 'test_repo/libs/langchain/langchain/embeddings/cohere.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from langchain_community.embeddings.self_hosted import (\\n    SelfHostedEmbeddings,\\n)\\n\\n__all__ = [\"SelfHostedEmbeddings\"]', metadata={'source': 'test_repo/libs/langchain/langchain/embeddings/self_hosted.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from langchain_community.embeddings.infinity import (\\n    InfinityEmbeddings,\\n    TinyAsyncOpenAIInfinityEmbeddingClient,\\n)\\n\\n__all__ = [\"InfinityEmbeddings\", \"TinyAsyncOpenAIInfinityEmbeddingClient\"]', metadata={'source': 'test_repo/libs/langchain/langchain/embeddings/infinity.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from langchain_community.embeddings.openai import (\\n    OpenAIEmbeddings,\\n)\\n\\n__all__ = [\\n    \"OpenAIEmbeddings\",\\n]', metadata={'source': 'test_repo/libs/langchain/langchain/embeddings/openai.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from langchain_community.embeddings.google_palm import (\\n    GooglePalmEmbeddings,\\n)\\n\\n__all__ = [\"GooglePalmEmbeddings\"]', metadata={'source': 'test_repo/libs/langchain/langchain/embeddings/google_palm.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from langchain_community.embeddings.databricks import DatabricksEmbeddings\\n\\n__all__ = [\"DatabricksEmbeddings\"]', metadata={'source': 'test_repo/libs/langchain/langchain/embeddings/databricks.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from langchain_community.embeddings.llamacpp import LlamaCppEmbeddings\\n\\n__all__ = [\"LlamaCppEmbeddings\"]', metadata={'source': 'test_repo/libs/langchain/langchain/embeddings/llamacpp.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from langchain_core.embeddings import Embeddings\\n\\n# This is for backwards compatibility\\n__all__ = [\"Embeddings\"]', metadata={'source': 'test_repo/libs/langchain/langchain/embeddings/base.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from langchain_community.embeddings.bedrock import BedrockEmbeddings\\n\\n__all__ = [\"BedrockEmbeddings\"]', metadata={'source': 'test_repo/libs/langchain/langchain/embeddings/bedrock.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from langchain_community.embeddings.edenai import EdenAiEmbeddings\\n\\n__all__ = [\"EdenAiEmbeddings\"]', metadata={'source': 'test_repo/libs/langchain/langchain/embeddings/edenai.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from langchain_community.embeddings.xinference import XinferenceEmbeddings\\n\\n__all__ = [\"XinferenceEmbeddings\"]', metadata={'source': 'test_repo/libs/langchain/langchain/embeddings/xinference.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from langchain_community.embeddings.jina import JinaEmbeddings\\n\\n__all__ = [\"JinaEmbeddings\"]', metadata={'source': 'test_repo/libs/langchain/langchain/embeddings/jina.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from langchain_community.embeddings.gpt4all import GPT4AllEmbeddings\\n\\n__all__ = [\"GPT4AllEmbeddings\"]', metadata={'source': 'test_repo/libs/langchain/langchain/embeddings/gpt4all.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from langchain_community.embeddings.sentence_transformer import (\\n    SentenceTransformerEmbeddings,\\n)\\n\\n__all__ = [\"SentenceTransformerEmbeddings\"]', metadata={'source': 'test_repo/libs/langchain/langchain/embeddings/sentence_transformer.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from langchain_community.embeddings.self_hosted_hugging_face import (\\n    SelfHostedHuggingFaceEmbeddings,\\n    SelfHostedHuggingFaceInstructEmbeddings,\\n)\\n\\n__all__ = [\\n    \"SelfHostedHuggingFaceEmbeddings\",\\n    \"SelfHostedHuggingFaceInstructEmbeddings\",\\n]', metadata={'source': 'test_repo/libs/langchain/langchain/embeddings/self_hosted_hugging_face.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from langchain_community.embeddings.ernie import ErnieEmbeddings\\n\\n__all__ = [\"ErnieEmbeddings\"]', metadata={'source': 'test_repo/libs/langchain/langchain/embeddings/ernie.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from langchain_community.embeddings.sagemaker_endpoint import (\\n    EmbeddingsContentHandler,\\n    SagemakerEndpointEmbeddings,\\n)\\n\\n__all__ = [\"EmbeddingsContentHandler\", \"SagemakerEndpointEmbeddings\"]', metadata={'source': 'test_repo/libs/langchain/langchain/embeddings/sagemaker_endpoint.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from langchain_community.embeddings.ollama import OllamaEmbeddings\\n\\n__all__ = [\"OllamaEmbeddings\"]', metadata={'source': 'test_repo/libs/langchain/langchain/embeddings/ollama.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from langchain_community.embeddings.embaas import (\\n    EmbaasEmbeddings,\\n)\\n\\n__all__ = [\\n    \"EmbaasEmbeddings\",\\n]', metadata={'source': 'test_repo/libs/langchain/langchain/embeddings/embaas.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='\"\"\"Module contains code for a cache backed embedder.\\n\\nThe cache backed embedder is a wrapper around an embedder that caches\\nembeddings in a key-value store. The cache is used to avoid recomputing\\nembeddings for the same text.\\n\\nThe text is hashed and the hash is used as the key in the cache.\\n\"\"\"\\nfrom __future__ import annotations\\n\\nimport hashlib\\nimport json\\nimport uuid\\nfrom functools import partial\\nfrom typing import Callable, List, Sequence, Union, cast\\n\\nfrom langchain_core.embeddings import Embeddings\\nfrom langchain_core.stores import BaseStore, ByteStore\\n\\nfrom langchain.storage.encoder_backed import EncoderBackedStore\\n\\nNAMESPACE_UUID = uuid.UUID(int=1985)\\n\\n\\ndef _hash_string_to_uuid(input_string: str) -> uuid.UUID:\\n    \"\"\"Hash a string and returns the corresponding UUID.\"\"\"\\n    hash_value = hashlib.sha1(input_string.encode(\"utf-8\")).hexdigest()\\n    return uuid.uuid5(NAMESPACE_UUID, hash_value)', metadata={'source': 'test_repo/libs/langchain/langchain/embeddings/cache.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='def _key_encoder(key: str, namespace: str) -> str:\\n    \"\"\"Encode a key.\"\"\"\\n    return namespace + str(_hash_string_to_uuid(key))\\n\\n\\ndef _create_key_encoder(namespace: str) -> Callable[[str], str]:\\n    \"\"\"Create an encoder for a key.\"\"\"\\n    return partial(_key_encoder, namespace=namespace)\\n\\n\\ndef _value_serializer(value: Sequence[float]) -> bytes:\\n    \"\"\"Serialize a value.\"\"\"\\n    return json.dumps(value).encode()\\n\\n\\ndef _value_deserializer(serialized_value: bytes) -> List[float]:\\n    \"\"\"Deserialize a value.\"\"\"\\n    return cast(List[float], json.loads(serialized_value.decode()))', metadata={'source': 'test_repo/libs/langchain/langchain/embeddings/cache.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='class CacheBackedEmbeddings(Embeddings):\\n    \"\"\"Interface for caching results from embedding models.\\n\\n    The interface allows works with any store that implements\\n    the abstract store interface accepting keys of type str and values of list of\\n    floats.\\n\\n    If need be, the interface can be extended to accept other implementations\\n    of the value serializer and deserializer, as well as the key encoder.\\n\\n    Examples:\\n\\n        .. code-block: python\\n\\n            from langchain.embeddings import CacheBackedEmbeddings\\n            from langchain.storage import LocalFileStore\\n            from langchain_community.embeddings import OpenAIEmbeddings\\n\\n            store = LocalFileStore(\\'./my_cache\\')\\n\\n            underlying_embedder = OpenAIEmbeddings()\\n            embedder = CacheBackedEmbeddings.from_bytes_store(\\n                underlying_embedder, store, namespace=underlying_embedder.model\\n            )', metadata={'source': 'test_repo/libs/langchain/langchain/embeddings/cache.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='# Embedding is computed and cached\\n            embeddings = embedder.embed_documents([\"hello\", \"goodbye\"])\\n\\n            # Embeddings are retrieved from the cache, no computation is done\\n            embeddings = embedder.embed_documents([\"hello\", \"goodbye\"])\\n    \"\"\"\\n\\n    def __init__(\\n        self,\\n        underlying_embeddings: Embeddings,\\n        document_embedding_store: BaseStore[str, List[float]],\\n    ) -> None:\\n        \"\"\"Initialize the embedder.\\n\\n        Args:\\n            underlying_embeddings: the embedder to use for computing embeddings.\\n            document_embedding_store: The store to use for caching document embeddings.\\n        \"\"\"\\n        super().__init__()\\n        self.document_embedding_store = document_embedding_store\\n        self.underlying_embeddings = underlying_embeddings\\n\\n    def embed_documents(self, texts: List[str]) -> List[List[float]]:\\n        \"\"\"Embed a list of texts.', metadata={'source': 'test_repo/libs/langchain/langchain/embeddings/cache.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='The method first checks the cache for the embeddings.\\n        If the embeddings are not found, the method uses the underlying embedder\\n        to embed the documents and stores the results in the cache.\\n\\n        Args:\\n            texts: A list of texts to embed.\\n\\n        Returns:\\n            A list of embeddings for the given texts.\\n        \"\"\"\\n        vectors: List[Union[List[float], None]] = self.document_embedding_store.mget(\\n            texts\\n        )\\n        missing_indices: List[int] = [\\n            i for i, vector in enumerate(vectors) if vector is None\\n        ]\\n        missing_texts = [texts[i] for i in missing_indices]', metadata={'source': 'test_repo/libs/langchain/langchain/embeddings/cache.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='if missing_texts:\\n            missing_vectors = self.underlying_embeddings.embed_documents(missing_texts)\\n            self.document_embedding_store.mset(\\n                list(zip(missing_texts, missing_vectors))\\n            )\\n            for index, updated_vector in zip(missing_indices, missing_vectors):\\n                vectors[index] = updated_vector\\n\\n        return cast(\\n            List[List[float]], vectors\\n        )  # Nones should have been resolved by now\\n\\n    async def aembed_documents(self, texts: List[str]) -> List[List[float]]:\\n        \"\"\"Embed a list of texts.\\n\\n        The method first checks the cache for the embeddings.\\n        If the embeddings are not found, the method uses the underlying embedder\\n        to embed the documents and stores the results in the cache.\\n\\n        Args:\\n            texts: A list of texts to embed.', metadata={'source': 'test_repo/libs/langchain/langchain/embeddings/cache.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='Args:\\n            texts: A list of texts to embed.\\n\\n        Returns:\\n            A list of embeddings for the given texts.\\n        \"\"\"\\n        vectors: List[\\n            Union[List[float], None]\\n        ] = await self.document_embedding_store.amget(texts)\\n        missing_indices: List[int] = [\\n            i for i, vector in enumerate(vectors) if vector is None\\n        ]\\n        missing_texts = [texts[i] for i in missing_indices]\\n\\n        if missing_texts:\\n            missing_vectors = await self.underlying_embeddings.aembed_documents(\\n                missing_texts\\n            )\\n            await self.document_embedding_store.amset(\\n                list(zip(missing_texts, missing_vectors))\\n            )\\n            for index, updated_vector in zip(missing_indices, missing_vectors):\\n                vectors[index] = updated_vector\\n\\n        return cast(\\n            List[List[float]], vectors\\n        )  # Nones should have been resolved by now', metadata={'source': 'test_repo/libs/langchain/langchain/embeddings/cache.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='def embed_query(self, text: str) -> List[float]:\\n        \"\"\"Embed query text.\\n\\n        This method does not support caching at the moment.\\n\\n        Support for caching queries is easily to implement, but might make\\n        sense to hold off to see the most common patterns.\\n\\n        If the cache has an eviction policy, we may need to be a bit more careful\\n        about sharing the cache between documents and queries. Generally,\\n        one is OK evicting query caches, but document caches should be kept.\\n\\n        Args:\\n            text: The text to embed.\\n\\n        Returns:\\n            The embedding for the given text.\\n        \"\"\"\\n        return self.underlying_embeddings.embed_query(text)\\n\\n    async def aembed_query(self, text: str) -> List[float]:\\n        \"\"\"Embed query text.\\n\\n        This method does not support caching at the moment.\\n\\n        Support for caching queries is easily to implement, but might make\\n        sense to hold off to see the most common patterns.', metadata={'source': 'test_repo/libs/langchain/langchain/embeddings/cache.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='If the cache has an eviction policy, we may need to be a bit more careful\\n        about sharing the cache between documents and queries. Generally,\\n        one is OK evicting query caches, but document caches should be kept.\\n\\n        Args:\\n            text: The text to embed.\\n\\n        Returns:\\n            The embedding for the given text.\\n        \"\"\"\\n        return await self.underlying_embeddings.aembed_query(text)\\n\\n    @classmethod\\n    def from_bytes_store(\\n        cls,\\n        underlying_embeddings: Embeddings,\\n        document_embedding_cache: ByteStore,\\n        *,\\n        namespace: str = \"\",\\n    ) -> CacheBackedEmbeddings:\\n        \"\"\"On-ramp that adds the necessary serialization and encoding to the store.', metadata={'source': 'test_repo/libs/langchain/langchain/embeddings/cache.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='Args:\\n            underlying_embeddings: The embedder to use for embedding.\\n            document_embedding_cache: The cache to use for storing document embeddings.\\n            *,\\n            namespace: The namespace to use for document cache.\\n                       This namespace is used to avoid collisions with other caches.\\n                       For example, set it to the name of the embedding model used.\\n        \"\"\"\\n        namespace = namespace\\n        key_encoder = _create_key_encoder(namespace)\\n        encoder_backed_store = EncoderBackedStore[str, List[float]](\\n            document_embedding_cache,\\n            key_encoder,\\n            _value_serializer,\\n            _value_deserializer,\\n        )\\n        return cls(underlying_embeddings, encoder_backed_store)', metadata={'source': 'test_repo/libs/langchain/langchain/embeddings/cache.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from langchain_community.embeddings.huggingface import (\\n    HuggingFaceBgeEmbeddings,\\n    HuggingFaceEmbeddings,\\n    HuggingFaceInferenceAPIEmbeddings,\\n    HuggingFaceInstructEmbeddings,\\n)\\n\\n__all__ = [\\n    \"HuggingFaceEmbeddings\",\\n    \"HuggingFaceInstructEmbeddings\",\\n    \"HuggingFaceBgeEmbeddings\",\\n    \"HuggingFaceInferenceAPIEmbeddings\",\\n]', metadata={'source': 'test_repo/libs/langchain/langchain/embeddings/huggingface.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from langchain_community.embeddings.deepinfra import (\\n    DeepInfraEmbeddings,\\n)\\n\\n__all__ = [\"DeepInfraEmbeddings\"]', metadata={'source': 'test_repo/libs/langchain/langchain/embeddings/deepinfra.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from langchain_community.embeddings.bookend import (\\n    BookendEmbeddings,\\n)\\n\\n__all__ = [\"BookendEmbeddings\"]', metadata={'source': 'test_repo/libs/langchain/langchain/embeddings/bookend.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from langchain_community.embeddings.clarifai import ClarifaiEmbeddings\\n\\n__all__ = [\"ClarifaiEmbeddings\"]', metadata={'source': 'test_repo/libs/langchain/langchain/embeddings/clarifai.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from langchain_community.embeddings.voyageai import (\\n    VoyageEmbeddings,\\n)\\n\\n__all__ = [\\n    \"VoyageEmbeddings\",\\n]', metadata={'source': 'test_repo/libs/langchain/langchain/embeddings/voyageai.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from typing import Any, Dict, List\\n\\nfrom langchain_core.messages import BaseMessage, get_buffer_string\\nfrom langchain_core.pydantic_v1 import root_validator\\n\\nfrom langchain.memory.chat_memory import BaseChatMemory\\nfrom langchain.memory.summary import SummarizerMixin', metadata={'source': 'test_repo/libs/langchain/langchain/memory/summary_buffer.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='class ConversationSummaryBufferMemory(BaseChatMemory, SummarizerMixin):\\n    \"\"\"Buffer with summarizer for storing conversation memory.\"\"\"\\n\\n    max_token_limit: int = 2000\\n    moving_summary_buffer: str = \"\"\\n    memory_key: str = \"history\"\\n\\n    @property\\n    def buffer(self) -> List[BaseMessage]:\\n        return self.chat_memory.messages\\n\\n    @property\\n    def memory_variables(self) -> List[str]:\\n        \"\"\"Will always return list of memory variables.\\n\\n        :meta private:\\n        \"\"\"\\n        return [self.memory_key]', metadata={'source': 'test_repo/libs/langchain/langchain/memory/summary_buffer.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content=':meta private:\\n        \"\"\"\\n        return [self.memory_key]\\n\\n    def load_memory_variables(self, inputs: Dict[str, Any]) -> Dict[str, Any]:\\n        \"\"\"Return history buffer.\"\"\"\\n        buffer = self.buffer\\n        if self.moving_summary_buffer != \"\":\\n            first_messages: List[BaseMessage] = [\\n                self.summary_message_cls(content=self.moving_summary_buffer)\\n            ]\\n            buffer = first_messages + buffer\\n        if self.return_messages:\\n            final_buffer: Any = buffer\\n        else:\\n            final_buffer = get_buffer_string(\\n                buffer, human_prefix=self.human_prefix, ai_prefix=self.ai_prefix\\n            )\\n        return {self.memory_key: final_buffer}', metadata={'source': 'test_repo/libs/langchain/langchain/memory/summary_buffer.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='@root_validator()\\n    def validate_prompt_input_variables(cls, values: Dict) -> Dict:\\n        \"\"\"Validate that prompt input variables are consistent.\"\"\"\\n        prompt_variables = values[\"prompt\"].input_variables\\n        expected_keys = {\"summary\", \"new_lines\"}\\n        if expected_keys != set(prompt_variables):\\n            raise ValueError(\\n                \"Got unexpected prompt input variables. The prompt expects \"\\n                f\"{prompt_variables}, but it should have {expected_keys}.\"\\n            )\\n        return values\\n\\n    def save_context(self, inputs: Dict[str, Any], outputs: Dict[str, str]) -> None:\\n        \"\"\"Save context from this conversation to buffer.\"\"\"\\n        super().save_context(inputs, outputs)\\n        self.prune()', metadata={'source': 'test_repo/libs/langchain/langchain/memory/summary_buffer.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='def prune(self) -> None:\\n        \"\"\"Prune buffer if it exceeds max token limit\"\"\"\\n        buffer = self.chat_memory.messages\\n        curr_buffer_length = self.llm.get_num_tokens_from_messages(buffer)\\n        if curr_buffer_length > self.max_token_limit:\\n            pruned_memory = []\\n            while curr_buffer_length > self.max_token_limit:\\n                pruned_memory.append(buffer.pop(0))\\n                curr_buffer_length = self.llm.get_num_tokens_from_messages(buffer)\\n            self.moving_summary_buffer = self.predict_new_summary(\\n                pruned_memory, self.moving_summary_buffer\\n            )\\n\\n    def clear(self) -> None:\\n        \"\"\"Clear memory contents.\"\"\"\\n        super().clear()\\n        self.moving_summary_buffer = \"\"', metadata={'source': 'test_repo/libs/langchain/langchain/memory/summary_buffer.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='\"\"\"**Memory** maintains Chain state, incorporating context from past runs.\\n\\n**Class hierarchy for Memory:**\\n\\n.. code-block::\\n\\n    BaseMemory --> BaseChatMemory --> <name>Memory  # Examples: ZepMemory, MotorheadMemory\\n\\n**Main helpers:**\\n\\n.. code-block::\\n\\n    BaseChatMessageHistory\\n\\n**Chat Message History** stores the chat message history in different stores.\\n\\n**Class hierarchy for ChatMessageHistory:**\\n\\n.. code-block::\\n\\n    BaseChatMessageHistory --> <name>ChatMessageHistory  # Example: ZepChatMessageHistory\\n\\n**Main helpers:**\\n\\n.. code-block::', metadata={'source': 'test_repo/libs/langchain/langchain/memory/__init__.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='**Main helpers:**\\n\\n.. code-block::\\n\\n    AIMessage, BaseMessage, HumanMessage\\n\"\"\"  # noqa: E501\\nfrom langchain_community.chat_message_histories import (\\n    AstraDBChatMessageHistory,\\n    CassandraChatMessageHistory,\\n    ChatMessageHistory,\\n    CosmosDBChatMessageHistory,\\n    DynamoDBChatMessageHistory,\\n    ElasticsearchChatMessageHistory,\\n    FileChatMessageHistory,\\n    MomentoChatMessageHistory,\\n    MongoDBChatMessageHistory,\\n    PostgresChatMessageHistory,\\n    RedisChatMessageHistory,\\n    SingleStoreDBChatMessageHistory,\\n    SQLChatMessageHistory,\\n    StreamlitChatMessageHistory,\\n    UpstashRedisChatMessageHistory,\\n    XataChatMessageHistory,\\n    ZepChatMessageHistory,\\n)', metadata={'source': 'test_repo/libs/langchain/langchain/memory/__init__.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from langchain.memory.buffer import (\\n    ConversationBufferMemory,\\n    ConversationStringBufferMemory,\\n)\\nfrom langchain.memory.buffer_window import ConversationBufferWindowMemory\\nfrom langchain.memory.combined import CombinedMemory\\nfrom langchain.memory.entity import (\\n    ConversationEntityMemory,\\n    InMemoryEntityStore,\\n    RedisEntityStore,\\n    SQLiteEntityStore,\\n    UpstashRedisEntityStore,\\n)\\nfrom langchain.memory.kg import ConversationKGMemory\\nfrom langchain.memory.motorhead_memory import MotorheadMemory\\nfrom langchain.memory.readonly import ReadOnlySharedMemory\\nfrom langchain.memory.simple import SimpleMemory\\nfrom langchain.memory.summary import ConversationSummaryMemory\\nfrom langchain.memory.summary_buffer import ConversationSummaryBufferMemory\\nfrom langchain.memory.token_buffer import ConversationTokenBufferMemory\\nfrom langchain.memory.vectorstore import VectorStoreRetrieverMemory\\nfrom langchain.memory.zep_memory import ZepMemory', metadata={'source': 'test_repo/libs/langchain/langchain/memory/__init__.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='__all__ = [\\n    \"AstraDBChatMessageHistory\",\\n    \"CassandraChatMessageHistory\",\\n    \"ChatMessageHistory\",\\n    \"CombinedMemory\",\\n    \"ConversationBufferMemory\",\\n    \"ConversationBufferWindowMemory\",\\n    \"ConversationEntityMemory\",\\n    \"ConversationKGMemory\",\\n    \"ConversationStringBufferMemory\",\\n    \"ConversationSummaryBufferMemory\",\\n    \"ConversationSummaryMemory\",\\n    \"ConversationTokenBufferMemory\",\\n    \"CosmosDBChatMessageHistory\",\\n    \"DynamoDBChatMessageHistory\",\\n    \"ElasticsearchChatMessageHistory\",\\n    \"FileChatMessageHistory\",\\n    \"InMemoryEntityStore\",\\n    \"MomentoChatMessageHistory\",\\n    \"MongoDBChatMessageHistory\",\\n    \"MotorheadMemory\",\\n    \"PostgresChatMessageHistory\",\\n    \"ReadOnlySharedMemory\",\\n    \"RedisChatMessageHistory\",\\n    \"RedisEntityStore\",\\n    \"SingleStoreDBChatMessageHistory\",\\n    \"SQLChatMessageHistory\",\\n    \"SQLiteEntityStore\",\\n    \"SimpleMemory\",\\n    \"StreamlitChatMessageHistory\",\\n    \"VectorStoreRetrieverMemory\",\\n    \"XataChatMessageHistory\",', metadata={'source': 'test_repo/libs/langchain/langchain/memory/__init__.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='\"StreamlitChatMessageHistory\",\\n    \"VectorStoreRetrieverMemory\",\\n    \"XataChatMessageHistory\",\\n    \"ZepChatMessageHistory\",\\n    \"ZepMemory\",\\n    \"UpstashRedisEntityStore\",\\n    \"UpstashRedisChatMessageHistory\",\\n]', metadata={'source': 'test_repo/libs/langchain/langchain/memory/__init__.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='import logging\\nfrom abc import ABC, abstractmethod\\nfrom itertools import islice\\nfrom typing import Any, Dict, Iterable, List, Optional\\n\\nfrom langchain_community.utilities.redis import get_client\\nfrom langchain_core.language_models import BaseLanguageModel\\nfrom langchain_core.messages import BaseMessage, get_buffer_string\\nfrom langchain_core.prompts import BasePromptTemplate\\nfrom langchain_core.pydantic_v1 import BaseModel, Field\\n\\nfrom langchain.chains.llm import LLMChain\\nfrom langchain.memory.chat_memory import BaseChatMemory\\nfrom langchain.memory.prompt import (\\n    ENTITY_EXTRACTION_PROMPT,\\n    ENTITY_SUMMARIZATION_PROMPT,\\n)\\nfrom langchain.memory.utils import get_prompt_input_key\\n\\nlogger = logging.getLogger(__name__)', metadata={'source': 'test_repo/libs/langchain/langchain/memory/entity.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='class BaseEntityStore(BaseModel, ABC):\\n    \"\"\"Abstract base class for Entity store.\"\"\"\\n\\n    @abstractmethod\\n    def get(self, key: str, default: Optional[str] = None) -> Optional[str]:\\n        \"\"\"Get entity value from store.\"\"\"\\n        pass\\n\\n    @abstractmethod\\n    def set(self, key: str, value: Optional[str]) -> None:\\n        \"\"\"Set entity value in store.\"\"\"\\n        pass\\n\\n    @abstractmethod\\n    def delete(self, key: str) -> None:\\n        \"\"\"Delete entity value from store.\"\"\"\\n        pass\\n\\n    @abstractmethod\\n    def exists(self, key: str) -> bool:\\n        \"\"\"Check if entity exists in store.\"\"\"\\n        pass\\n\\n    @abstractmethod\\n    def clear(self) -> None:\\n        \"\"\"Delete all entities from store.\"\"\"\\n        pass', metadata={'source': 'test_repo/libs/langchain/langchain/memory/entity.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='class InMemoryEntityStore(BaseEntityStore):\\n    \"\"\"In-memory Entity store.\"\"\"\\n\\n    store: Dict[str, Optional[str]] = {}\\n\\n    def get(self, key: str, default: Optional[str] = None) -> Optional[str]:\\n        return self.store.get(key, default)\\n\\n    def set(self, key: str, value: Optional[str]) -> None:\\n        self.store[key] = value\\n\\n    def delete(self, key: str) -> None:\\n        del self.store[key]\\n\\n    def exists(self, key: str) -> bool:\\n        return key in self.store\\n\\n    def clear(self) -> None:\\n        return self.store.clear()', metadata={'source': 'test_repo/libs/langchain/langchain/memory/entity.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='class UpstashRedisEntityStore(BaseEntityStore):\\n    \"\"\"Upstash Redis backed Entity store.\\n\\n    Entities get a TTL of 1 day by default, and\\n    that TTL is extended by 3 days every time the entity is read back.\\n    \"\"\"\\n\\n    def __init__(\\n        self,\\n        session_id: str = \"default\",\\n        url: str = \"\",\\n        token: str = \"\",\\n        key_prefix: str = \"memory_store\",\\n        ttl: Optional[int] = 60 * 60 * 24,\\n        recall_ttl: Optional[int] = 60 * 60 * 24 * 3,\\n        *args: Any,\\n        **kwargs: Any,\\n    ):\\n        try:\\n            from upstash_redis import Redis\\n        except ImportError:\\n            raise ImportError(\\n                \"Could not import upstash_redis python package. \"\\n                \"Please install it with `pip install upstash_redis`.\"\\n            )\\n\\n        super().__init__(*args, **kwargs)', metadata={'source': 'test_repo/libs/langchain/langchain/memory/entity.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='super().__init__(*args, **kwargs)\\n\\n        try:\\n            self.redis_client = Redis(url=url, token=token)\\n        except Exception:\\n            logger.error(\"Upstash Redis instance could not be initiated.\")\\n\\n        self.session_id = session_id\\n        self.key_prefix = key_prefix\\n        self.ttl = ttl\\n        self.recall_ttl = recall_ttl or ttl\\n\\n    @property\\n    def full_key_prefix(self) -> str:\\n        return f\"{self.key_prefix}:{self.session_id}\"\\n\\n    def get(self, key: str, default: Optional[str] = None) -> Optional[str]:\\n        res = (\\n            self.redis_client.getex(f\"{self.full_key_prefix}:{key}\", ex=self.recall_ttl)\\n            or default\\n            or \"\"\\n        )\\n        logger.debug(f\"Upstash Redis MEM get \\'{self.full_key_prefix}:{key}\\': \\'{res}\\'\")\\n        return res', metadata={'source': 'test_repo/libs/langchain/langchain/memory/entity.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='def set(self, key: str, value: Optional[str]) -> None:\\n        if not value:\\n            return self.delete(key)\\n        self.redis_client.set(f\"{self.full_key_prefix}:{key}\", value, ex=self.ttl)\\n        logger.debug(\\n            f\"Redis MEM set \\'{self.full_key_prefix}:{key}\\': \\'{value}\\' EX {self.ttl}\"\\n        )\\n\\n    def delete(self, key: str) -> None:\\n        self.redis_client.delete(f\"{self.full_key_prefix}:{key}\")\\n\\n    def exists(self, key: str) -> bool:\\n        return self.redis_client.exists(f\"{self.full_key_prefix}:{key}\") == 1\\n\\n    def clear(self) -> None:\\n        def scan_and_delete(cursor: int) -> int:\\n            cursor, keys_to_delete = self.redis_client.scan(\\n                cursor, f\"{self.full_key_prefix}:*\"\\n            )\\n            self.redis_client.delete(*keys_to_delete)\\n            return cursor\\n\\n        cursor = scan_and_delete(0)\\n        while cursor != 0:\\n            scan_and_delete(cursor)', metadata={'source': 'test_repo/libs/langchain/langchain/memory/entity.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='class RedisEntityStore(BaseEntityStore):\\n    \"\"\"Redis-backed Entity store.\\n\\n    Entities get a TTL of 1 day by default, and\\n    that TTL is extended by 3 days every time the entity is read back.\\n    \"\"\"\\n\\n    redis_client: Any\\n    session_id: str = \"default\"\\n    key_prefix: str = \"memory_store\"\\n    ttl: Optional[int] = 60 * 60 * 24\\n    recall_ttl: Optional[int] = 60 * 60 * 24 * 3\\n\\n    def __init__(\\n        self,\\n        session_id: str = \"default\",\\n        url: str = \"redis://localhost:6379/0\",\\n        key_prefix: str = \"memory_store\",\\n        ttl: Optional[int] = 60 * 60 * 24,\\n        recall_ttl: Optional[int] = 60 * 60 * 24 * 3,\\n        *args: Any,\\n        **kwargs: Any,\\n    ):\\n        try:\\n            import redis\\n        except ImportError:\\n            raise ImportError(\\n                \"Could not import redis python package. \"\\n                \"Please install it with `pip install redis`.\"\\n            )\\n\\n        super().__init__(*args, **kwargs)', metadata={'source': 'test_repo/libs/langchain/langchain/memory/entity.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='super().__init__(*args, **kwargs)\\n\\n        try:\\n            self.redis_client = get_client(redis_url=url, decode_responses=True)\\n        except redis.exceptions.ConnectionError as error:\\n            logger.error(error)\\n\\n        self.session_id = session_id\\n        self.key_prefix = key_prefix\\n        self.ttl = ttl\\n        self.recall_ttl = recall_ttl or ttl\\n\\n    @property\\n    def full_key_prefix(self) -> str:\\n        return f\"{self.key_prefix}:{self.session_id}\"\\n\\n    def get(self, key: str, default: Optional[str] = None) -> Optional[str]:\\n        res = (\\n            self.redis_client.getex(f\"{self.full_key_prefix}:{key}\", ex=self.recall_ttl)\\n            or default\\n            or \"\"\\n        )\\n        logger.debug(f\"REDIS MEM get \\'{self.full_key_prefix}:{key}\\': \\'{res}\\'\")\\n        return res', metadata={'source': 'test_repo/libs/langchain/langchain/memory/entity.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='def set(self, key: str, value: Optional[str]) -> None:\\n        if not value:\\n            return self.delete(key)\\n        self.redis_client.set(f\"{self.full_key_prefix}:{key}\", value, ex=self.ttl)\\n        logger.debug(\\n            f\"REDIS MEM set \\'{self.full_key_prefix}:{key}\\': \\'{value}\\' EX {self.ttl}\"\\n        )\\n\\n    def delete(self, key: str) -> None:\\n        self.redis_client.delete(f\"{self.full_key_prefix}:{key}\")\\n\\n    def exists(self, key: str) -> bool:\\n        return self.redis_client.exists(f\"{self.full_key_prefix}:{key}\") == 1\\n\\n    def clear(self) -> None:\\n        # iterate a list in batches of size batch_size\\n        def batched(iterable: Iterable[Any], batch_size: int) -> Iterable[Any]:\\n            iterator = iter(iterable)\\n            while batch := list(islice(iterator, batch_size)):\\n                yield batch', metadata={'source': 'test_repo/libs/langchain/langchain/memory/entity.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='for keybatch in batched(\\n            self.redis_client.scan_iter(f\"{self.full_key_prefix}:*\"), 500\\n        ):\\n            self.redis_client.delete(*keybatch)', metadata={'source': 'test_repo/libs/langchain/langchain/memory/entity.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='class SQLiteEntityStore(BaseEntityStore):\\n    \"\"\"SQLite-backed Entity store\"\"\"\\n\\n    session_id: str = \"default\"\\n    table_name: str = \"memory_store\"\\n    conn: Any = None\\n\\n    class Config:\\n        \"\"\"Configuration for this pydantic object.\"\"\"\\n\\n        arbitrary_types_allowed = True\\n\\n    def __init__(\\n        self,\\n        session_id: str = \"default\",\\n        db_file: str = \"entities.db\",\\n        table_name: str = \"memory_store\",\\n        *args: Any,\\n        **kwargs: Any,\\n    ):\\n        try:\\n            import sqlite3\\n        except ImportError:\\n            raise ImportError(\\n                \"Could not import sqlite3 python package. \"\\n                \"Please install it with `pip install sqlite3`.\"\\n            )\\n        super().__init__(*args, **kwargs)\\n\\n        self.conn = sqlite3.connect(db_file)\\n        self.session_id = session_id\\n        self.table_name = table_name\\n        self._create_table_if_not_exists()', metadata={'source': 'test_repo/libs/langchain/langchain/memory/entity.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='@property\\n    def full_table_name(self) -> str:\\n        return f\"{self.table_name}_{self.session_id}\"\\n\\n    def _create_table_if_not_exists(self) -> None:\\n        create_table_query = f\"\"\"\\n            CREATE TABLE IF NOT EXISTS {self.full_table_name} (\\n                key TEXT PRIMARY KEY,\\n                value TEXT\\n            )\\n        \"\"\"\\n        with self.conn:\\n            self.conn.execute(create_table_query)\\n\\n    def get(self, key: str, default: Optional[str] = None) -> Optional[str]:\\n        query = f\"\"\"\\n            SELECT value\\n            FROM {self.full_table_name}\\n            WHERE key = ?\\n        \"\"\"\\n        cursor = self.conn.execute(query, (key,))\\n        result = cursor.fetchone()\\n        if result is not None:\\n            value = result[0]\\n            return value\\n        return default', metadata={'source': 'test_repo/libs/langchain/langchain/memory/entity.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='def set(self, key: str, value: Optional[str]) -> None:\\n        if not value:\\n            return self.delete(key)\\n        query = f\"\"\"\\n            INSERT OR REPLACE INTO {self.full_table_name} (key, value)\\n            VALUES (?, ?)\\n        \"\"\"\\n        with self.conn:\\n            self.conn.execute(query, (key, value))\\n\\n    def delete(self, key: str) -> None:\\n        query = f\"\"\"\\n            DELETE FROM {self.full_table_name}\\n            WHERE key = ?\\n        \"\"\"\\n        with self.conn:\\n            self.conn.execute(query, (key,))\\n\\n    def exists(self, key: str) -> bool:\\n        query = f\"\"\"\\n            SELECT 1\\n            FROM {self.full_table_name}\\n            WHERE key = ?\\n            LIMIT 1\\n        \"\"\"\\n        cursor = self.conn.execute(query, (key,))\\n        result = cursor.fetchone()\\n        return result is not None', metadata={'source': 'test_repo/libs/langchain/langchain/memory/entity.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='def clear(self) -> None:\\n        query = f\"\"\"\\n            DELETE FROM {self.full_table_name}\\n        \"\"\"\\n        with self.conn:\\n            self.conn.execute(query)', metadata={'source': 'test_repo/libs/langchain/langchain/memory/entity.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='class ConversationEntityMemory(BaseChatMemory):\\n    \"\"\"Entity extractor & summarizer memory.\\n\\n    Extracts named entities from the recent chat history and generates summaries.\\n    With a swappable entity store, persisting entities across conversations.\\n    Defaults to an in-memory entity store, and can be swapped out for a Redis,\\n    SQLite, or other entity store.\\n    \"\"\"\\n\\n    human_prefix: str = \"Human\"\\n    ai_prefix: str = \"AI\"\\n    llm: BaseLanguageModel\\n    entity_extraction_prompt: BasePromptTemplate = ENTITY_EXTRACTION_PROMPT\\n    entity_summarization_prompt: BasePromptTemplate = ENTITY_SUMMARIZATION_PROMPT\\n\\n    # Cache of recently detected entity names, if any\\n    # It is updated when load_memory_variables is called:\\n    entity_cache: List[str] = []\\n\\n    # Number of recent message pairs to consider when updating entities:\\n    k: int = 3\\n\\n    chat_history_key: str = \"history\"', metadata={'source': 'test_repo/libs/langchain/langchain/memory/entity.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='chat_history_key: str = \"history\"\\n\\n    # Store to manage entity-related data:\\n    entity_store: BaseEntityStore = Field(default_factory=InMemoryEntityStore)\\n\\n    @property\\n    def buffer(self) -> List[BaseMessage]:\\n        \"\"\"Access chat memory messages.\"\"\"\\n        return self.chat_memory.messages\\n\\n    @property\\n    def memory_variables(self) -> List[str]:\\n        \"\"\"Will always return list of memory variables.\\n\\n        :meta private:\\n        \"\"\"\\n        return [\"entities\", self.chat_history_key]\\n\\n    def load_memory_variables(self, inputs: Dict[str, Any]) -> Dict[str, Any]:\\n        \"\"\"\\n        Returns chat history and all generated entities with summaries if available,\\n        and updates or clears the recent entity cache.\\n\\n        New entity name can be found when calling this method, before the entity\\n        summaries are generated, so the entity cache values may be empty if no entity\\n        descriptions are generated yet.\\n        \"\"\"', metadata={'source': 'test_repo/libs/langchain/langchain/memory/entity.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='# Create an LLMChain for predicting entity names from the recent chat history:\\n        chain = LLMChain(llm=self.llm, prompt=self.entity_extraction_prompt)\\n\\n        if self.input_key is None:\\n            prompt_input_key = get_prompt_input_key(inputs, self.memory_variables)\\n        else:\\n            prompt_input_key = self.input_key\\n\\n        # Extract an arbitrary window of the last message pairs from\\n        # the chat history, where the hyperparameter k is the\\n        # number of message pairs:\\n        buffer_string = get_buffer_string(\\n            self.buffer[-self.k * 2 :],\\n            human_prefix=self.human_prefix,\\n            ai_prefix=self.ai_prefix,\\n        )\\n\\n        # Generates a comma-separated list of named entities,\\n        # e.g. \"Jane, White House, UFO\"\\n        # or \"NONE\" if no named entities are extracted:\\n        output = chain.predict(\\n            history=buffer_string,\\n            input=inputs[prompt_input_key],\\n        )', metadata={'source': 'test_repo/libs/langchain/langchain/memory/entity.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='# If no named entities are extracted, assigns an empty list.\\n        if output.strip() == \"NONE\":\\n            entities = []\\n        else:\\n            # Make a list of the extracted entities:\\n            entities = [w.strip() for w in output.split(\",\")]\\n\\n        # Make a dictionary of entities with summary if exists:\\n        entity_summaries = {}\\n\\n        for entity in entities:\\n            entity_summaries[entity] = self.entity_store.get(entity, \"\")\\n\\n        # Replaces the entity name cache with the most recently discussed entities,\\n        # or if no entities were extracted, clears the cache:\\n        self.entity_cache = entities\\n\\n        # Should we return as message objects or as a string?\\n        if self.return_messages:\\n            # Get last `k` pair of chat messages:\\n            buffer: Any = self.buffer[-self.k * 2 :]\\n        else:\\n            # Reuse the string we made earlier:\\n            buffer = buffer_string', metadata={'source': 'test_repo/libs/langchain/langchain/memory/entity.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='return {\\n            self.chat_history_key: buffer,\\n            \"entities\": entity_summaries,\\n        }\\n\\n    def save_context(self, inputs: Dict[str, Any], outputs: Dict[str, str]) -> None:\\n        \"\"\"\\n        Save context from this conversation history to the entity store.\\n\\n        Generates a summary for each entity in the entity cache by prompting\\n        the model, and saves these summaries to the entity store.\\n        \"\"\"\\n\\n        super().save_context(inputs, outputs)\\n\\n        if self.input_key is None:\\n            prompt_input_key = get_prompt_input_key(inputs, self.memory_variables)\\n        else:\\n            prompt_input_key = self.input_key', metadata={'source': 'test_repo/libs/langchain/langchain/memory/entity.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='# Extract an arbitrary window of the last message pairs from\\n        # the chat history, where the hyperparameter k is the\\n        # number of message pairs:\\n        buffer_string = get_buffer_string(\\n            self.buffer[-self.k * 2 :],\\n            human_prefix=self.human_prefix,\\n            ai_prefix=self.ai_prefix,\\n        )\\n\\n        input_data = inputs[prompt_input_key]\\n\\n        # Create an LLMChain for predicting entity summarization from the context\\n        chain = LLMChain(llm=self.llm, prompt=self.entity_summarization_prompt)', metadata={'source': 'test_repo/libs/langchain/langchain/memory/entity.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='# Generate new summaries for entities and save them in the entity store\\n        for entity in self.entity_cache:\\n            # Get existing summary if it exists\\n            existing_summary = self.entity_store.get(entity, \"\")\\n            output = chain.predict(\\n                summary=existing_summary,\\n                entity=entity,\\n                history=buffer_string,\\n                input=input_data,\\n            )\\n            # Save the updated summary to the entity store\\n            self.entity_store.set(entity, output.strip())\\n\\n    def clear(self) -> None:\\n        \"\"\"Clear memory contents.\"\"\"\\n        self.chat_memory.clear()\\n        self.entity_cache.clear()\\n        self.entity_store.clear()', metadata={'source': 'test_repo/libs/langchain/langchain/memory/entity.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from typing import Any, Dict, List, Union\\n\\nfrom langchain_core.messages import BaseMessage, get_buffer_string\\n\\nfrom langchain.memory.chat_memory import BaseChatMemory', metadata={'source': 'test_repo/libs/langchain/langchain/memory/buffer_window.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='class ConversationBufferWindowMemory(BaseChatMemory):\\n    \"\"\"Buffer for storing conversation memory inside a limited size window.\"\"\"\\n\\n    human_prefix: str = \"Human\"\\n    ai_prefix: str = \"AI\"\\n    memory_key: str = \"history\"  #: :meta private:\\n    k: int = 5\\n    \"\"\"Number of messages to store in buffer.\"\"\"\\n\\n    @property\\n    def buffer(self) -> Union[str, List[BaseMessage]]:\\n        \"\"\"String buffer of memory.\"\"\"\\n        return self.buffer_as_messages if self.return_messages else self.buffer_as_str\\n\\n    @property\\n    def buffer_as_str(self) -> str:\\n        \"\"\"Exposes the buffer as a string in case return_messages is False.\"\"\"\\n        messages = self.chat_memory.messages[-self.k * 2 :] if self.k > 0 else []\\n        return get_buffer_string(\\n            messages,\\n            human_prefix=self.human_prefix,\\n            ai_prefix=self.ai_prefix,\\n        )', metadata={'source': 'test_repo/libs/langchain/langchain/memory/buffer_window.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='@property\\n    def buffer_as_messages(self) -> List[BaseMessage]:\\n        \"\"\"Exposes the buffer as a list of messages in case return_messages is True.\"\"\"\\n        return self.chat_memory.messages[-self.k * 2 :] if self.k > 0 else []\\n\\n    @property\\n    def memory_variables(self) -> List[str]:\\n        \"\"\"Will always return list of memory variables.\\n\\n        :meta private:\\n        \"\"\"\\n        return [self.memory_key]\\n\\n    def load_memory_variables(self, inputs: Dict[str, Any]) -> Dict[str, Any]:\\n        \"\"\"Return history buffer.\"\"\"\\n        return {self.memory_key: self.buffer}', metadata={'source': 'test_repo/libs/langchain/langchain/memory/buffer_window.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from typing import Any, Dict, List\\n\\nfrom langchain_core.language_models import BaseLanguageModel\\nfrom langchain_core.messages import BaseMessage, get_buffer_string\\n\\nfrom langchain.memory.chat_memory import BaseChatMemory', metadata={'source': 'test_repo/libs/langchain/langchain/memory/token_buffer.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='class ConversationTokenBufferMemory(BaseChatMemory):\\n    \"\"\"Conversation chat memory with token limit.\"\"\"\\n\\n    human_prefix: str = \"Human\"\\n    ai_prefix: str = \"AI\"\\n    llm: BaseLanguageModel\\n    memory_key: str = \"history\"\\n    max_token_limit: int = 2000\\n\\n    @property\\n    def buffer(self) -> Any:\\n        \"\"\"String buffer of memory.\"\"\"\\n        return self.buffer_as_messages if self.return_messages else self.buffer_as_str\\n\\n    @property\\n    def buffer_as_str(self) -> str:\\n        \"\"\"Exposes the buffer as a string in case return_messages is False.\"\"\"\\n        return get_buffer_string(\\n            self.chat_memory.messages,\\n            human_prefix=self.human_prefix,\\n            ai_prefix=self.ai_prefix,\\n        )\\n\\n    @property\\n    def buffer_as_messages(self) -> List[BaseMessage]:\\n        \"\"\"Exposes the buffer as a list of messages in case return_messages is True.\"\"\"\\n        return self.chat_memory.messages', metadata={'source': 'test_repo/libs/langchain/langchain/memory/token_buffer.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='@property\\n    def memory_variables(self) -> List[str]:\\n        \"\"\"Will always return list of memory variables.\\n\\n        :meta private:\\n        \"\"\"\\n        return [self.memory_key]\\n\\n    def load_memory_variables(self, inputs: Dict[str, Any]) -> Dict[str, Any]:\\n        \"\"\"Return history buffer.\"\"\"\\n        return {self.memory_key: self.buffer}', metadata={'source': 'test_repo/libs/langchain/langchain/memory/token_buffer.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='def save_context(self, inputs: Dict[str, Any], outputs: Dict[str, str]) -> None:\\n        \"\"\"Save context from this conversation to buffer. Pruned.\"\"\"\\n        super().save_context(inputs, outputs)\\n        # Prune buffer if it exceeds max token limit\\n        buffer = self.chat_memory.messages\\n        curr_buffer_length = self.llm.get_num_tokens_from_messages(buffer)\\n        if curr_buffer_length > self.max_token_limit:\\n            pruned_memory = []\\n            while curr_buffer_length > self.max_token_limit:\\n                pruned_memory.append(buffer.pop(0))\\n                curr_buffer_length = self.llm.get_num_tokens_from_messages(buffer)', metadata={'source': 'test_repo/libs/langchain/langchain/memory/token_buffer.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from typing import Any, Dict, List, Optional\\n\\nfrom langchain_core.messages import BaseMessage, get_buffer_string\\nfrom langchain_core.pydantic_v1 import root_validator\\n\\nfrom langchain.memory.chat_memory import BaseChatMemory, BaseMemory\\nfrom langchain.memory.utils import get_prompt_input_key', metadata={'source': 'test_repo/libs/langchain/langchain/memory/buffer.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='class ConversationBufferMemory(BaseChatMemory):\\n    \"\"\"Buffer for storing conversation memory.\"\"\"\\n\\n    human_prefix: str = \"Human\"\\n    ai_prefix: str = \"AI\"\\n    memory_key: str = \"history\"  #: :meta private:\\n\\n    @property\\n    def buffer(self) -> Any:\\n        \"\"\"String buffer of memory.\"\"\"\\n        return self.buffer_as_messages if self.return_messages else self.buffer_as_str\\n\\n    async def abuffer(self) -> Any:\\n        \"\"\"String buffer of memory.\"\"\"\\n        return (\\n            await self.abuffer_as_messages()\\n            if self.return_messages\\n            else await self.abuffer_as_str()\\n        )\\n\\n    def _buffer_as_str(self, messages: List[BaseMessage]) -> str:\\n        return get_buffer_string(\\n            messages,\\n            human_prefix=self.human_prefix,\\n            ai_prefix=self.ai_prefix,\\n        )', metadata={'source': 'test_repo/libs/langchain/langchain/memory/buffer.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='@property\\n    def buffer_as_str(self) -> str:\\n        \"\"\"Exposes the buffer as a string in case return_messages is True.\"\"\"\\n        return self._buffer_as_str(self.chat_memory.messages)\\n\\n    async def abuffer_as_str(self) -> str:\\n        \"\"\"Exposes the buffer as a string in case return_messages is True.\"\"\"\\n        messages = await self.chat_memory.aget_messages()\\n        return self._buffer_as_str(messages)\\n\\n    @property\\n    def buffer_as_messages(self) -> List[BaseMessage]:\\n        \"\"\"Exposes the buffer as a list of messages in case return_messages is False.\"\"\"\\n        return self.chat_memory.messages\\n\\n    async def abuffer_as_messages(self) -> List[BaseMessage]:\\n        \"\"\"Exposes the buffer as a list of messages in case return_messages is False.\"\"\"\\n        return await self.chat_memory.aget_messages()\\n\\n    @property\\n    def memory_variables(self) -> List[str]:\\n        \"\"\"Will always return list of memory variables.', metadata={'source': 'test_repo/libs/langchain/langchain/memory/buffer.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content=':meta private:\\n        \"\"\"\\n        return [self.memory_key]\\n\\n    def load_memory_variables(self, inputs: Dict[str, Any]) -> Dict[str, Any]:\\n        \"\"\"Return history buffer.\"\"\"\\n        return {self.memory_key: self.buffer}\\n\\n    async def aload_memory_variables(self, inputs: Dict[str, Any]) -> Dict[str, Any]:\\n        \"\"\"Return key-value pairs given the text input to the chain.\"\"\"\\n        buffer = await self.abuffer()\\n        return {self.memory_key: buffer}', metadata={'source': 'test_repo/libs/langchain/langchain/memory/buffer.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='class ConversationStringBufferMemory(BaseMemory):\\n    \"\"\"Buffer for storing conversation memory.\"\"\"\\n\\n    human_prefix: str = \"Human\"\\n    ai_prefix: str = \"AI\"\\n    \"\"\"Prefix to use for AI generated responses.\"\"\"\\n    buffer: str = \"\"\\n    output_key: Optional[str] = None\\n    input_key: Optional[str] = None\\n    memory_key: str = \"history\"  #: :meta private:\\n\\n    @root_validator()\\n    def validate_chains(cls, values: Dict) -> Dict:\\n        \"\"\"Validate that return messages is not True.\"\"\"\\n        if values.get(\"return_messages\", False):\\n            raise ValueError(\\n                \"return_messages must be False for ConversationStringBufferMemory\"\\n            )\\n        return values\\n\\n    @property\\n    def memory_variables(self) -> List[str]:\\n        \"\"\"Will always return list of memory variables.\\n        :meta private:\\n        \"\"\"\\n        return [self.memory_key]', metadata={'source': 'test_repo/libs/langchain/langchain/memory/buffer.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='def load_memory_variables(self, inputs: Dict[str, Any]) -> Dict[str, str]:\\n        \"\"\"Return history buffer.\"\"\"\\n        return {self.memory_key: self.buffer}\\n\\n    async def aload_memory_variables(self, inputs: Dict[str, Any]) -> Dict[str, str]:\\n        \"\"\"Return history buffer.\"\"\"\\n        return self.load_memory_variables(inputs)', metadata={'source': 'test_repo/libs/langchain/langchain/memory/buffer.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='def save_context(self, inputs: Dict[str, Any], outputs: Dict[str, str]) -> None:\\n        \"\"\"Save context from this conversation to buffer.\"\"\"\\n        if self.input_key is None:\\n            prompt_input_key = get_prompt_input_key(inputs, self.memory_variables)\\n        else:\\n            prompt_input_key = self.input_key\\n        if self.output_key is None:\\n            if len(outputs) != 1:\\n                raise ValueError(f\"One output key expected, got {outputs.keys()}\")\\n            output_key = list(outputs.keys())[0]\\n        else:\\n            output_key = self.output_key\\n        human = f\"{self.human_prefix}: \" + inputs[prompt_input_key]\\n        ai = f\"{self.ai_prefix}: \" + outputs[output_key]\\n        self.buffer += \"\\\\n\" + \"\\\\n\".join([human, ai])\\n\\n    async def asave_context(\\n        self, inputs: Dict[str, Any], outputs: Dict[str, str]\\n    ) -> None:\\n        \"\"\"Save context from this conversation to buffer.\"\"\"\\n        return self.save_context(inputs, outputs)', metadata={'source': 'test_repo/libs/langchain/langchain/memory/buffer.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='def clear(self) -> None:\\n        \"\"\"Clear memory contents.\"\"\"\\n        self.buffer = \"\"\\n\\n    async def aclear(self) -> None:\\n        self.clear()', metadata={'source': 'test_repo/libs/langchain/langchain/memory/buffer.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='\"\"\"Class for a VectorStore-backed memory object.\"\"\"\\n\\nfrom typing import Any, Dict, List, Optional, Sequence, Union\\n\\nfrom langchain_core.documents import Document\\nfrom langchain_core.pydantic_v1 import Field\\nfrom langchain_core.vectorstores import VectorStoreRetriever\\n\\nfrom langchain.memory.chat_memory import BaseMemory\\nfrom langchain.memory.utils import get_prompt_input_key', metadata={'source': 'test_repo/libs/langchain/langchain/memory/vectorstore.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='class VectorStoreRetrieverMemory(BaseMemory):\\n    \"\"\"VectorStoreRetriever-backed memory.\"\"\"\\n\\n    retriever: VectorStoreRetriever = Field(exclude=True)\\n    \"\"\"VectorStoreRetriever object to connect to.\"\"\"\\n\\n    memory_key: str = \"history\"  #: :meta private:\\n    \"\"\"Key name to locate the memories in the result of load_memory_variables.\"\"\"\\n\\n    input_key: Optional[str] = None\\n    \"\"\"Key name to index the inputs to load_memory_variables.\"\"\"\\n\\n    return_docs: bool = False\\n    \"\"\"Whether or not to return the result of querying the database directly.\"\"\"\\n\\n    exclude_input_keys: Sequence[str] = Field(default_factory=tuple)\\n    \"\"\"Input keys to exclude in addition to memory key when constructing the document\"\"\"\\n\\n    @property\\n    def memory_variables(self) -> List[str]:\\n        \"\"\"The list of keys emitted from the load_memory_variables method.\"\"\"\\n        return [self.memory_key]', metadata={'source': 'test_repo/libs/langchain/langchain/memory/vectorstore.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='def _get_prompt_input_key(self, inputs: Dict[str, Any]) -> str:\\n        \"\"\"Get the input key for the prompt.\"\"\"\\n        if self.input_key is None:\\n            return get_prompt_input_key(inputs, self.memory_variables)\\n        return self.input_key\\n\\n    def load_memory_variables(\\n        self, inputs: Dict[str, Any]\\n    ) -> Dict[str, Union[List[Document], str]]:\\n        \"\"\"Return history buffer.\"\"\"\\n        input_key = self._get_prompt_input_key(inputs)\\n        query = inputs[input_key]\\n        docs = self.retriever.get_relevant_documents(query)\\n        result: Union[List[Document], str]\\n        if not self.return_docs:\\n            result = \"\\\\n\".join([doc.page_content for doc in docs])\\n        else:\\n            result = docs\\n        return {self.memory_key: result}', metadata={'source': 'test_repo/libs/langchain/langchain/memory/vectorstore.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='def _form_documents(\\n        self, inputs: Dict[str, Any], outputs: Dict[str, str]\\n    ) -> List[Document]:\\n        \"\"\"Format context from this conversation to buffer.\"\"\"\\n        # Each document should only include the current turn, not the chat history\\n        exclude = set(self.exclude_input_keys)\\n        exclude.add(self.memory_key)\\n        filtered_inputs = {k: v for k, v in inputs.items() if k not in exclude}\\n        texts = [\\n            f\"{k}: {v}\"\\n            for k, v in list(filtered_inputs.items()) + list(outputs.items())\\n        ]\\n        page_content = \"\\\\n\".join(texts)\\n        return [Document(page_content=page_content)]\\n\\n    def save_context(self, inputs: Dict[str, Any], outputs: Dict[str, str]) -> None:\\n        \"\"\"Save context from this conversation to buffer.\"\"\"\\n        documents = self._form_documents(inputs, outputs)\\n        self.retriever.add_documents(documents)\\n\\n    def clear(self) -> None:\\n        \"\"\"Nothing to clear.\"\"\"', metadata={'source': 'test_repo/libs/langchain/langchain/memory/vectorstore.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from typing import Any, Dict, List\\n\\n\\ndef get_prompt_input_key(inputs: Dict[str, Any], memory_variables: List[str]) -> str:\\n    \"\"\"\\n    Get the prompt input key.\\n\\n    Args:\\n        inputs: Dict[str, Any]\\n        memory_variables: List[str]\\n\\n    Returns:\\n        A prompt input key.\\n    \"\"\"\\n    # \"stop\" is a special key that can be passed as input but is not used to\\n    # format the prompt.\\n    prompt_input_keys = list(set(inputs).difference(memory_variables + [\"stop\"]))\\n    if len(prompt_input_keys) != 1:\\n        raise ValueError(f\"One input key expected got {prompt_input_keys}\")\\n    return prompt_input_keys[0]', metadata={'source': 'test_repo/libs/langchain/langchain/memory/utils.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from __future__ import annotations\\n\\nfrom typing import Any, Dict, Optional\\n\\nfrom langchain_community.chat_message_histories import ZepChatMessageHistory\\n\\nfrom langchain.memory import ConversationBufferMemory', metadata={'source': 'test_repo/libs/langchain/langchain/memory/zep_memory.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='class ZepMemory(ConversationBufferMemory):\\n    \"\"\"Persist your chain history to the Zep MemoryStore.\\n\\n    The number of messages returned by Zep and when the Zep server summarizes chat\\n    histories is configurable. See the Zep documentation for more details.\\n\\n    Documentation: https://docs.getzep.com\\n\\n    Example:\\n        .. code-block:: python', metadata={'source': 'test_repo/libs/langchain/langchain/memory/zep_memory.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='Documentation: https://docs.getzep.com\\n\\n    Example:\\n        .. code-block:: python\\n\\n        memory = ZepMemory(\\n                    session_id=session_id,  # Identifies your user or a user\\'s session\\n                    url=ZEP_API_URL,        # Your Zep server\\'s URL\\n                    api_key=<your_api_key>, # Optional\\n                    memory_key=\"history\",   # Ensure this matches the key used in\\n                                            # chain\\'s prompt template\\n                    return_messages=True,   # Does your prompt template expect a string\\n                                            # or a list of Messages?\\n                )\\n        chain = LLMChain(memory=memory,...) # Configure your chain to use the ZepMemory\\n                                              instance', metadata={'source': 'test_repo/libs/langchain/langchain/memory/zep_memory.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='Note:\\n        To persist metadata alongside your chat history, your will need to create a\\n    custom Chain class that overrides the `prep_outputs` method to include the metadata\\n    in the call to `self.memory.save_context`.\\n\\n\\n    Zep - Fast, scalable building blocks for LLM Apps\\n    =========\\n    Zep is an open source platform for productionizing LLM apps. Go from a prototype\\n    built in LangChain or LlamaIndex, or a custom app, to production in minutes without\\n    rewriting code.\\n\\n    For server installation instructions and more, see:\\n    https://docs.getzep.com/deployment/quickstart/\\n\\n    For more information on the zep-python package, see:\\n    https://github.com/getzep/zep-python\\n\\n    \"\"\"\\n\\n    chat_memory: ZepChatMessageHistory', metadata={'source': 'test_repo/libs/langchain/langchain/memory/zep_memory.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='\"\"\"\\n\\n    chat_memory: ZepChatMessageHistory\\n\\n    def __init__(\\n        self,\\n        session_id: str,\\n        url: str = \"http://localhost:8000\",\\n        api_key: Optional[str] = None,\\n        output_key: Optional[str] = None,\\n        input_key: Optional[str] = None,\\n        return_messages: bool = False,\\n        human_prefix: str = \"Human\",\\n        ai_prefix: str = \"AI\",\\n        memory_key: str = \"history\",\\n    ):\\n        \"\"\"Initialize ZepMemory.', metadata={'source': 'test_repo/libs/langchain/langchain/memory/zep_memory.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='Args:\\n            session_id (str): Identifies your user or a user\\'s session\\n            url (str, optional): Your Zep server\\'s URL. Defaults to\\n                                 \"http://localhost:8000\".\\n            api_key (Optional[str], optional): Your Zep API key. Defaults to None.\\n            output_key (Optional[str], optional): The key to use for the output message.\\n                                              Defaults to None.\\n            input_key (Optional[str], optional): The key to use for the input message.\\n                                              Defaults to None.\\n            return_messages (bool, optional): Does your prompt template expect a string\\n                                              or a list of Messages? Defaults to False\\n                                              i.e. return a string.\\n            human_prefix (str, optional): The prefix to use for human messages.\\n                                          Defaults to \"Human\".', metadata={'source': 'test_repo/libs/langchain/langchain/memory/zep_memory.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='Defaults to \"Human\".\\n            ai_prefix (str, optional): The prefix to use for AI messages.\\n                                       Defaults to \"AI\".\\n            memory_key (str, optional): The key to use for the memory.\\n                                        Defaults to \"history\".\\n                                        Ensure that this matches the key used in\\n                                        chain\\'s prompt template.\\n        \"\"\"\\n        chat_message_history = ZepChatMessageHistory(\\n            session_id=session_id,\\n            url=url,\\n            api_key=api_key,\\n        )\\n        super().__init__(\\n            chat_memory=chat_message_history,\\n            output_key=output_key,\\n            input_key=input_key,\\n            return_messages=return_messages,\\n            human_prefix=human_prefix,\\n            ai_prefix=ai_prefix,\\n            memory_key=memory_key,\\n        )', metadata={'source': 'test_repo/libs/langchain/langchain/memory/zep_memory.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='def save_context(\\n        self,\\n        inputs: Dict[str, Any],\\n        outputs: Dict[str, str],\\n        metadata: Optional[Dict[str, Any]] = None,\\n    ) -> None:\\n        \"\"\"Save context from this conversation to buffer.\\n\\n        Args:\\n            inputs (Dict[str, Any]): The inputs to the chain.\\n            outputs (Dict[str, str]): The outputs from the chain.\\n            metadata (Optional[Dict[str, Any]], optional): Any metadata to save with\\n                                                           the context. Defaults to None\\n\\n        Returns:\\n            None\\n        \"\"\"\\n        input_str, output_str = self._get_input_output(inputs, outputs)\\n        self.chat_memory.add_user_message(input_str, metadata=metadata)\\n        self.chat_memory.add_ai_message(output_str, metadata=metadata)', metadata={'source': 'test_repo/libs/langchain/langchain/memory/zep_memory.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='import warnings\\nfrom typing import Any, Dict, List, Set\\n\\nfrom langchain_core.memory import BaseMemory\\nfrom langchain_core.pydantic_v1 import validator\\n\\nfrom langchain.memory.chat_memory import BaseChatMemory', metadata={'source': 'test_repo/libs/langchain/langchain/memory/combined.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='class CombinedMemory(BaseMemory):\\n    \"\"\"Combining multiple memories\\' data together.\"\"\"\\n\\n    memories: List[BaseMemory]\\n    \"\"\"For tracking all the memories that should be accessed.\"\"\"\\n\\n    @validator(\"memories\")\\n    def check_repeated_memory_variable(\\n        cls, value: List[BaseMemory]\\n    ) -> List[BaseMemory]:\\n        all_variables: Set[str] = set()\\n        for val in value:\\n            overlap = all_variables.intersection(val.memory_variables)\\n            if overlap:\\n                raise ValueError(\\n                    f\"The same variables {overlap} are found in multiple\"\\n                    \"memory object, which is not allowed by CombinedMemory.\"\\n                )\\n            all_variables |= set(val.memory_variables)\\n\\n        return value', metadata={'source': 'test_repo/libs/langchain/langchain/memory/combined.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='return value\\n\\n    @validator(\"memories\")\\n    def check_input_key(cls, value: List[BaseMemory]) -> List[BaseMemory]:\\n        \"\"\"Check that if memories are of type BaseChatMemory that input keys exist.\"\"\"\\n        for val in value:\\n            if isinstance(val, BaseChatMemory):\\n                if val.input_key is None:\\n                    warnings.warn(\\n                        \"When using CombinedMemory, \"\\n                        \"input keys should be so the input is known. \"\\n                        f\" Was not set on {val}\"\\n                    )\\n        return value\\n\\n    @property\\n    def memory_variables(self) -> List[str]:\\n        \"\"\"All the memory variables that this instance provides.\"\"\"\\n        \"\"\"Collected from the all the linked memories.\"\"\"\\n\\n        memory_variables = []\\n\\n        for memory in self.memories:\\n            memory_variables.extend(memory.memory_variables)\\n\\n        return memory_variables', metadata={'source': 'test_repo/libs/langchain/langchain/memory/combined.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='return memory_variables\\n\\n    def load_memory_variables(self, inputs: Dict[str, Any]) -> Dict[str, str]:\\n        \"\"\"Load all vars from sub-memories.\"\"\"\\n        memory_data: Dict[str, Any] = {}\\n\\n        # Collect vars from all sub-memories\\n        for memory in self.memories:\\n            data = memory.load_memory_variables(inputs)\\n            for key, value in data.items():\\n                if key in memory_data:\\n                    raise ValueError(\\n                        f\"The variable {key} is repeated in the CombinedMemory.\"\\n                    )\\n                memory_data[key] = value\\n\\n        return memory_data\\n\\n    def save_context(self, inputs: Dict[str, Any], outputs: Dict[str, str]) -> None:\\n        \"\"\"Save context from this session for every memory.\"\"\"\\n        # Save context for all sub-memories\\n        for memory in self.memories:\\n            memory.save_context(inputs, outputs)', metadata={'source': 'test_repo/libs/langchain/langchain/memory/combined.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='def clear(self) -> None:\\n        \"\"\"Clear context from this session for every memory.\"\"\"\\n        for memory in self.memories:\\n            memory.clear()', metadata={'source': 'test_repo/libs/langchain/langchain/memory/combined.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from typing import Any, Dict, List\\n\\nfrom langchain_core.memory import BaseMemory\\n\\n\\nclass SimpleMemory(BaseMemory):\\n    \"\"\"Simple memory for storing context or other information that shouldn\\'t\\n    ever change between prompts.\\n    \"\"\"\\n\\n    memories: Dict[str, Any] = dict()\\n\\n    @property\\n    def memory_variables(self) -> List[str]:\\n        return list(self.memories.keys())\\n\\n    def load_memory_variables(self, inputs: Dict[str, Any]) -> Dict[str, str]:\\n        return self.memories\\n\\n    def save_context(self, inputs: Dict[str, Any], outputs: Dict[str, str]) -> None:\\n        \"\"\"Nothing should be saved or changed, my memory is set in stone.\"\"\"\\n        pass\\n\\n    def clear(self) -> None:\\n        \"\"\"Nothing to clear, got a memory like a vault.\"\"\"\\n        pass', metadata={'source': 'test_repo/libs/langchain/langchain/memory/simple.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from __future__ import annotations\\n\\nfrom typing import Any, Dict, List, Type\\n\\nfrom langchain_core.chat_history import BaseChatMessageHistory\\nfrom langchain_core.language_models import BaseLanguageModel\\nfrom langchain_core.messages import BaseMessage, SystemMessage, get_buffer_string\\nfrom langchain_core.prompts import BasePromptTemplate\\nfrom langchain_core.pydantic_v1 import BaseModel, root_validator\\n\\nfrom langchain.chains.llm import LLMChain\\nfrom langchain.memory.chat_memory import BaseChatMemory\\nfrom langchain.memory.prompt import SUMMARY_PROMPT', metadata={'source': 'test_repo/libs/langchain/langchain/memory/summary.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='class SummarizerMixin(BaseModel):\\n    \"\"\"Mixin for summarizer.\"\"\"\\n\\n    human_prefix: str = \"Human\"\\n    ai_prefix: str = \"AI\"\\n    llm: BaseLanguageModel\\n    prompt: BasePromptTemplate = SUMMARY_PROMPT\\n    summary_message_cls: Type[BaseMessage] = SystemMessage\\n\\n    def predict_new_summary(\\n        self, messages: List[BaseMessage], existing_summary: str\\n    ) -> str:\\n        new_lines = get_buffer_string(\\n            messages,\\n            human_prefix=self.human_prefix,\\n            ai_prefix=self.ai_prefix,\\n        )\\n\\n        chain = LLMChain(llm=self.llm, prompt=self.prompt)\\n        return chain.predict(summary=existing_summary, new_lines=new_lines)', metadata={'source': 'test_repo/libs/langchain/langchain/memory/summary.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='class ConversationSummaryMemory(BaseChatMemory, SummarizerMixin):\\n    \"\"\"Conversation summarizer to chat memory.\"\"\"\\n\\n    buffer: str = \"\"\\n    memory_key: str = \"history\"  #: :meta private:\\n\\n    @classmethod\\n    def from_messages(\\n        cls,\\n        llm: BaseLanguageModel,\\n        chat_memory: BaseChatMessageHistory,\\n        *,\\n        summarize_step: int = 2,\\n        **kwargs: Any,\\n    ) -> ConversationSummaryMemory:\\n        obj = cls(llm=llm, chat_memory=chat_memory, **kwargs)\\n        for i in range(0, len(obj.chat_memory.messages), summarize_step):\\n            obj.buffer = obj.predict_new_summary(\\n                obj.chat_memory.messages[i : i + summarize_step], obj.buffer\\n            )\\n        return obj\\n\\n    @property\\n    def memory_variables(self) -> List[str]:\\n        \"\"\"Will always return list of memory variables.\\n\\n        :meta private:\\n        \"\"\"\\n        return [self.memory_key]', metadata={'source': 'test_repo/libs/langchain/langchain/memory/summary.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content=':meta private:\\n        \"\"\"\\n        return [self.memory_key]\\n\\n    def load_memory_variables(self, inputs: Dict[str, Any]) -> Dict[str, Any]:\\n        \"\"\"Return history buffer.\"\"\"\\n        if self.return_messages:\\n            buffer: Any = [self.summary_message_cls(content=self.buffer)]\\n        else:\\n            buffer = self.buffer\\n        return {self.memory_key: buffer}\\n\\n    @root_validator()\\n    def validate_prompt_input_variables(cls, values: Dict) -> Dict:\\n        \"\"\"Validate that prompt input variables are consistent.\"\"\"\\n        prompt_variables = values[\"prompt\"].input_variables\\n        expected_keys = {\"summary\", \"new_lines\"}\\n        if expected_keys != set(prompt_variables):\\n            raise ValueError(\\n                \"Got unexpected prompt input variables. The prompt expects \"\\n                f\"{prompt_variables}, but it should have {expected_keys}.\"\\n            )\\n        return values', metadata={'source': 'test_repo/libs/langchain/langchain/memory/summary.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='def save_context(self, inputs: Dict[str, Any], outputs: Dict[str, str]) -> None:\\n        \"\"\"Save context from this conversation to buffer.\"\"\"\\n        super().save_context(inputs, outputs)\\n        self.buffer = self.predict_new_summary(\\n            self.chat_memory.messages[-2:], self.buffer\\n        )\\n\\n    def clear(self) -> None:\\n        \"\"\"Clear memory contents.\"\"\"\\n        super().clear()\\n        self.buffer = \"\"', metadata={'source': 'test_repo/libs/langchain/langchain/memory/summary.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from typing import Any, Dict, List\\n\\nfrom langchain_core.memory import BaseMemory\\n\\n\\nclass ReadOnlySharedMemory(BaseMemory):\\n    \"\"\"A memory wrapper that is read-only and cannot be changed.\"\"\"\\n\\n    memory: BaseMemory\\n\\n    @property\\n    def memory_variables(self) -> List[str]:\\n        \"\"\"Return memory variables.\"\"\"\\n        return self.memory.memory_variables\\n\\n    def load_memory_variables(self, inputs: Dict[str, Any]) -> Dict[str, str]:\\n        \"\"\"Load memory variables from memory.\"\"\"\\n        return self.memory.load_memory_variables(inputs)\\n\\n    def save_context(self, inputs: Dict[str, Any], outputs: Dict[str, str]) -> None:\\n        \"\"\"Nothing should be saved or changed\"\"\"\\n        pass\\n\\n    def clear(self) -> None:\\n        \"\"\"Nothing to clear, got a memory like a vault.\"\"\"\\n        pass', metadata={'source': 'test_repo/libs/langchain/langchain/memory/readonly.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='# flake8: noqa\\nfrom langchain_core.prompts.prompt import PromptTemplate\\n\\n_DEFAULT_ENTITY_MEMORY_CONVERSATION_TEMPLATE = \"\"\"You are an assistant to a human, powered by a large language model trained by OpenAI.\\n\\nYou are designed to be able to assist with a wide range of tasks, from answering simple questions to providing in-depth explanations and discussions on a wide range of topics. As a language model, you are able to generate human-like text based on the input you receive, allowing you to engage in natural-sounding conversations and provide responses that are coherent and relevant to the topic at hand.', metadata={'source': 'test_repo/libs/langchain/langchain/memory/prompt.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='You are constantly learning and improving, and your capabilities are constantly evolving. You are able to process and understand large amounts of text, and can use this knowledge to provide accurate and informative responses to a wide range of questions. You have access to some personalized information provided by the human in the Context section below. Additionally, you are able to generate your own text based on the input you receive, allowing you to engage in discussions and provide explanations and descriptions on a wide range of topics.\\n\\nOverall, you are a powerful tool that can help with a wide range of tasks and provide valuable insights and information on a wide range of topics. Whether the human needs help with a specific question or just wants to have a conversation about a particular topic, you are here to assist.\\n\\nContext:\\n{entities}\\n\\nCurrent conversation:\\n{history}\\nLast line:\\nHuman: {input}\\nYou:\"\"\"', metadata={'source': 'test_repo/libs/langchain/langchain/memory/prompt.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='Context:\\n{entities}\\n\\nCurrent conversation:\\n{history}\\nLast line:\\nHuman: {input}\\nYou:\"\"\"\\n\\nENTITY_MEMORY_CONVERSATION_TEMPLATE = PromptTemplate(\\n    input_variables=[\"entities\", \"history\", \"input\"],\\n    template=_DEFAULT_ENTITY_MEMORY_CONVERSATION_TEMPLATE,\\n)\\n\\n_DEFAULT_SUMMARIZER_TEMPLATE = \"\"\"Progressively summarize the lines of conversation provided, adding onto the previous summary returning a new summary.\\n\\nEXAMPLE\\nCurrent summary:\\nThe human asks what the AI thinks of artificial intelligence. The AI thinks artificial intelligence is a force for good.\\n\\nNew lines of conversation:\\nHuman: Why do you think artificial intelligence is a force for good?\\nAI: Because artificial intelligence will help humans reach their full potential.\\n\\nNew summary:\\nThe human asks what the AI thinks of artificial intelligence. The AI thinks artificial intelligence is a force for good because it will help humans reach their full potential.\\nEND OF EXAMPLE\\n\\nCurrent summary:\\n{summary}', metadata={'source': 'test_repo/libs/langchain/langchain/memory/prompt.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='Current summary:\\n{summary}\\n\\nNew lines of conversation:\\n{new_lines}\\n\\nNew summary:\"\"\"\\nSUMMARY_PROMPT = PromptTemplate(\\n    input_variables=[\"summary\", \"new_lines\"], template=_DEFAULT_SUMMARIZER_TEMPLATE\\n)\\n\\n_DEFAULT_ENTITY_EXTRACTION_TEMPLATE = \"\"\"You are an AI assistant reading the transcript of a conversation between an AI and a human. Extract all of the proper nouns from the last line of conversation. As a guideline, a proper noun is generally capitalized. You should definitely extract all names and places.\\n\\nThe conversation history is provided just in case of a coreference (e.g. \"What do you know about him\" where \"him\" is defined in a previous line) -- ignore items mentioned there that are not in the last line.\\n\\nReturn the output as a single comma-separated list, or NONE if there is nothing of note to return (e.g. the user is just issuing a greeting or having a simple conversation).', metadata={'source': 'test_repo/libs/langchain/langchain/memory/prompt.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='EXAMPLE\\nConversation history:\\nPerson #1: how\\'s it going today?\\nAI: \"It\\'s going great! How about you?\"\\nPerson #1: good! busy working on Langchain. lots to do.\\nAI: \"That sounds like a lot of work! What kind of things are you doing to make Langchain better?\"\\nLast line:\\nPerson #1: i\\'m trying to improve Langchain\\'s interfaces, the UX, its integrations with various products the user might want ... a lot of stuff.\\nOutput: Langchain\\nEND OF EXAMPLE\\n\\nEXAMPLE\\nConversation history:\\nPerson #1: how\\'s it going today?\\nAI: \"It\\'s going great! How about you?\"\\nPerson #1: good! busy working on Langchain. lots to do.\\nAI: \"That sounds like a lot of work! What kind of things are you doing to make Langchain better?\"\\nLast line:\\nPerson #1: i\\'m trying to improve Langchain\\'s interfaces, the UX, its integrations with various products the user might want ... a lot of stuff. I\\'m working with Person #2.\\nOutput: Langchain, Person #2\\nEND OF EXAMPLE', metadata={'source': 'test_repo/libs/langchain/langchain/memory/prompt.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='Conversation history (for reference only):\\n{history}\\nLast line of conversation (for extraction):\\nHuman: {input}\\n\\nOutput:\"\"\"\\nENTITY_EXTRACTION_PROMPT = PromptTemplate(\\n    input_variables=[\"history\", \"input\"], template=_DEFAULT_ENTITY_EXTRACTION_TEMPLATE\\n)\\n\\n_DEFAULT_ENTITY_SUMMARIZATION_TEMPLATE = \"\"\"You are an AI assistant helping a human keep track of facts about relevant people, places, and concepts in their life. Update the summary of the provided entity in the \"Entity\" section based on the last line of your conversation with the human. If you are writing the summary for the first time, return a single sentence.\\nThe update should only include facts that are relayed in the last line of conversation about the provided entity, and should only contain facts about the provided entity.\\n\\nIf there is no new information about the provided entity or the information is not worth noting (not an important or relevant fact to remember long-term), return the existing summary unchanged.', metadata={'source': 'test_repo/libs/langchain/langchain/memory/prompt.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='Full conversation history (for context):\\n{history}\\n\\nEntity to summarize:\\n{entity}\\n\\nExisting summary of {entity}:\\n{summary}\\n\\nLast line of conversation:\\nHuman: {input}\\nUpdated summary:\"\"\"\\n\\nENTITY_SUMMARIZATION_PROMPT = PromptTemplate(\\n    input_variables=[\"entity\", \"summary\", \"history\", \"input\"],\\n    template=_DEFAULT_ENTITY_SUMMARIZATION_TEMPLATE,\\n)', metadata={'source': 'test_repo/libs/langchain/langchain/memory/prompt.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='KG_TRIPLE_DELIMITER = \"<|>\"\\n_DEFAULT_KNOWLEDGE_TRIPLE_EXTRACTION_TEMPLATE = (\\n    \"You are a networked intelligence helping a human track knowledge triples\"\\n    \" about all relevant people, things, concepts, etc. and integrating\"\\n    \" them with your knowledge stored within your weights\"\\n    \" as well as that stored in a knowledge graph.\"\\n    \" Extract all of the knowledge triples from the last line of conversation.\"\\n    \" A knowledge triple is a clause that contains a subject, a predicate,\"\\n    \" and an object. The subject is the entity being described,\"\\n    \" the predicate is the property of the subject that is being\"\\n    \" described, and the object is the value of the property.\\\\n\\\\n\"\\n    \"EXAMPLE\\\\n\"\\n    \"Conversation history:\\\\n\"\\n    \"Person #1: Did you hear aliens landed in Area 51?\\\\n\"\\n    \"AI: No, I didn\\'t hear that. What do you know about Area 51?\\\\n\"\\n    \"Person #1: It\\'s a secret military base in Nevada.\\\\n\"\\n    \"AI: What do you know about Nevada?\\\\n\"', metadata={'source': 'test_repo/libs/langchain/langchain/memory/prompt.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='\"AI: What do you know about Nevada?\\\\n\"\\n    \"Last line of conversation:\\\\n\"\\n    \"Person #1: It\\'s a state in the US. It\\'s also the number 1 producer of gold in the US.\\\\n\\\\n\"\\n    f\"Output: (Nevada, is a, state){KG_TRIPLE_DELIMITER}(Nevada, is in, US)\"\\n    f\"{KG_TRIPLE_DELIMITER}(Nevada, is the number 1 producer of, gold)\\\\n\"\\n    \"END OF EXAMPLE\\\\n\\\\n\"\\n    \"EXAMPLE\\\\n\"\\n    \"Conversation history:\\\\n\"\\n    \"Person #1: Hello.\\\\n\"\\n    \"AI: Hi! How are you?\\\\n\"\\n    \"Person #1: I\\'m good. How are you?\\\\n\"\\n    \"AI: I\\'m good too.\\\\n\"\\n    \"Last line of conversation:\\\\n\"\\n    \"Person #1: I\\'m going to the store.\\\\n\\\\n\"\\n    \"Output: NONE\\\\n\"\\n    \"END OF EXAMPLE\\\\n\\\\n\"\\n    \"EXAMPLE\\\\n\"\\n    \"Conversation history:\\\\n\"\\n    \"Person #1: What do you know about Descartes?\\\\n\"\\n    \"AI: Descartes was a French philosopher, mathematician, and scientist who lived in the 17th century.\\\\n\"\\n    \"Person #1: The Descartes I\\'m referring to is a standup comedian and interior designer from Montreal.\\\\n\"', metadata={'source': 'test_repo/libs/langchain/langchain/memory/prompt.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='\"AI: Oh yes, He is a comedian and an interior designer. He has been in the industry for 30 years. His favorite food is baked bean pie.\\\\n\"\\n    \"Last line of conversation:\\\\n\"\\n    \"Person #1: Oh huh. I know Descartes likes to drive antique scooters and play the mandolin.\\\\n\"\\n    f\"Output: (Descartes, likes to drive, antique scooters){KG_TRIPLE_DELIMITER}(Descartes, plays, mandolin)\\\\n\"\\n    \"END OF EXAMPLE\\\\n\\\\n\"\\n    \"Conversation history (for reference only):\\\\n\"\\n    \"{history}\"\\n    \"\\\\nLast line of conversation (for extraction):\\\\n\"\\n    \"Human: {input}\\\\n\\\\n\"\\n    \"Output:\"\\n)', metadata={'source': 'test_repo/libs/langchain/langchain/memory/prompt.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='KNOWLEDGE_TRIPLE_EXTRACTION_PROMPT = PromptTemplate(\\n    input_variables=[\"history\", \"input\"],\\n    template=_DEFAULT_KNOWLEDGE_TRIPLE_EXTRACTION_TEMPLATE,\\n)', metadata={'source': 'test_repo/libs/langchain/langchain/memory/prompt.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from typing import Any, Dict, List, Type, Union\\n\\nfrom langchain_community.graphs import NetworkxEntityGraph\\nfrom langchain_community.graphs.networkx_graph import (\\n    KnowledgeTriple,\\n    get_entities,\\n    parse_triples,\\n)\\nfrom langchain_core.language_models import BaseLanguageModel\\nfrom langchain_core.messages import BaseMessage, SystemMessage, get_buffer_string\\nfrom langchain_core.prompts import BasePromptTemplate\\nfrom langchain_core.pydantic_v1 import Field\\n\\nfrom langchain.chains.llm import LLMChain\\nfrom langchain.memory.chat_memory import BaseChatMemory\\nfrom langchain.memory.prompt import (\\n    ENTITY_EXTRACTION_PROMPT,\\n    KNOWLEDGE_TRIPLE_EXTRACTION_PROMPT,\\n)\\nfrom langchain.memory.utils import get_prompt_input_key', metadata={'source': 'test_repo/libs/langchain/langchain/memory/kg.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='class ConversationKGMemory(BaseChatMemory):\\n    \"\"\"Knowledge graph conversation memory.\\n\\n    Integrates with external knowledge graph to store and retrieve\\n    information about knowledge triples in the conversation.\\n    \"\"\"\\n\\n    k: int = 2\\n    human_prefix: str = \"Human\"\\n    ai_prefix: str = \"AI\"\\n    kg: NetworkxEntityGraph = Field(default_factory=NetworkxEntityGraph)\\n    knowledge_extraction_prompt: BasePromptTemplate = KNOWLEDGE_TRIPLE_EXTRACTION_PROMPT\\n    entity_extraction_prompt: BasePromptTemplate = ENTITY_EXTRACTION_PROMPT\\n    llm: BaseLanguageModel\\n    summary_message_cls: Type[BaseMessage] = SystemMessage\\n    \"\"\"Number of previous utterances to include in the context.\"\"\"\\n    memory_key: str = \"history\"  #: :meta private:\\n\\n    def load_memory_variables(self, inputs: Dict[str, Any]) -> Dict[str, Any]:\\n        \"\"\"Return history buffer.\"\"\"\\n        entities = self._get_current_entities(inputs)', metadata={'source': 'test_repo/libs/langchain/langchain/memory/kg.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='summary_strings = []\\n        for entity in entities:\\n            knowledge = self.kg.get_entity_knowledge(entity)\\n            if knowledge:\\n                summary = f\"On {entity}: {\\'. \\'.join(knowledge)}.\"\\n                summary_strings.append(summary)\\n        context: Union[str, List]\\n        if not summary_strings:\\n            context = [] if self.return_messages else \"\"\\n        elif self.return_messages:\\n            context = [\\n                self.summary_message_cls(content=text) for text in summary_strings\\n            ]\\n        else:\\n            context = \"\\\\n\".join(summary_strings)\\n\\n        return {self.memory_key: context}\\n\\n    @property\\n    def memory_variables(self) -> List[str]:\\n        \"\"\"Will always return list of memory variables.\\n\\n        :meta private:\\n        \"\"\"\\n        return [self.memory_key]', metadata={'source': 'test_repo/libs/langchain/langchain/memory/kg.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content=':meta private:\\n        \"\"\"\\n        return [self.memory_key]\\n\\n    def _get_prompt_input_key(self, inputs: Dict[str, Any]) -> str:\\n        \"\"\"Get the input key for the prompt.\"\"\"\\n        if self.input_key is None:\\n            return get_prompt_input_key(inputs, self.memory_variables)\\n        return self.input_key\\n\\n    def _get_prompt_output_key(self, outputs: Dict[str, Any]) -> str:\\n        \"\"\"Get the output key for the prompt.\"\"\"\\n        if self.output_key is None:\\n            if len(outputs) != 1:\\n                raise ValueError(f\"One output key expected, got {outputs.keys()}\")\\n            return list(outputs.keys())[0]\\n        return self.output_key', metadata={'source': 'test_repo/libs/langchain/langchain/memory/kg.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='def get_current_entities(self, input_string: str) -> List[str]:\\n        chain = LLMChain(llm=self.llm, prompt=self.entity_extraction_prompt)\\n        buffer_string = get_buffer_string(\\n            self.chat_memory.messages[-self.k * 2 :],\\n            human_prefix=self.human_prefix,\\n            ai_prefix=self.ai_prefix,\\n        )\\n        output = chain.predict(\\n            history=buffer_string,\\n            input=input_string,\\n        )\\n        return get_entities(output)\\n\\n    def _get_current_entities(self, inputs: Dict[str, Any]) -> List[str]:\\n        \"\"\"Get the current entities in the conversation.\"\"\"\\n        prompt_input_key = self._get_prompt_input_key(inputs)\\n        return self.get_current_entities(inputs[prompt_input_key])', metadata={'source': 'test_repo/libs/langchain/langchain/memory/kg.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='def get_knowledge_triplets(self, input_string: str) -> List[KnowledgeTriple]:\\n        chain = LLMChain(llm=self.llm, prompt=self.knowledge_extraction_prompt)\\n        buffer_string = get_buffer_string(\\n            self.chat_memory.messages[-self.k * 2 :],\\n            human_prefix=self.human_prefix,\\n            ai_prefix=self.ai_prefix,\\n        )\\n        output = chain.predict(\\n            history=buffer_string,\\n            input=input_string,\\n            verbose=True,\\n        )\\n        knowledge = parse_triples(output)\\n        return knowledge\\n\\n    def _get_and_update_kg(self, inputs: Dict[str, Any]) -> None:\\n        \"\"\"Get and update knowledge graph from the conversation history.\"\"\"\\n        prompt_input_key = self._get_prompt_input_key(inputs)\\n        knowledge = self.get_knowledge_triplets(inputs[prompt_input_key])\\n        for triple in knowledge:\\n            self.kg.add_triple(triple)', metadata={'source': 'test_repo/libs/langchain/langchain/memory/kg.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='def save_context(self, inputs: Dict[str, Any], outputs: Dict[str, str]) -> None:\\n        \"\"\"Save context from this conversation to buffer.\"\"\"\\n        super().save_context(inputs, outputs)\\n        self._get_and_update_kg(inputs)\\n\\n    def clear(self) -> None:\\n        \"\"\"Clear memory contents.\"\"\"\\n        super().clear()\\n        self.kg.clear()', metadata={'source': 'test_repo/libs/langchain/langchain/memory/kg.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from abc import ABC\\nfrom typing import Any, Dict, Optional, Tuple\\n\\nfrom langchain_community.chat_message_histories.in_memory import ChatMessageHistory\\nfrom langchain_core.chat_history import BaseChatMessageHistory\\nfrom langchain_core.memory import BaseMemory\\nfrom langchain_core.messages import AIMessage, HumanMessage\\nfrom langchain_core.pydantic_v1 import Field\\n\\nfrom langchain.memory.utils import get_prompt_input_key', metadata={'source': 'test_repo/libs/langchain/langchain/memory/chat_memory.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='class BaseChatMemory(BaseMemory, ABC):\\n    \"\"\"Abstract base class for chat memory.\"\"\"\\n\\n    chat_memory: BaseChatMessageHistory = Field(default_factory=ChatMessageHistory)\\n    output_key: Optional[str] = None\\n    input_key: Optional[str] = None\\n    return_messages: bool = False\\n\\n    def _get_input_output(\\n        self, inputs: Dict[str, Any], outputs: Dict[str, str]\\n    ) -> Tuple[str, str]:\\n        if self.input_key is None:\\n            prompt_input_key = get_prompt_input_key(inputs, self.memory_variables)\\n        else:\\n            prompt_input_key = self.input_key\\n        if self.output_key is None:\\n            if len(outputs) != 1:\\n                raise ValueError(f\"One output key expected, got {outputs.keys()}\")\\n            output_key = list(outputs.keys())[0]\\n        else:\\n            output_key = self.output_key\\n        return inputs[prompt_input_key], outputs[output_key]', metadata={'source': 'test_repo/libs/langchain/langchain/memory/chat_memory.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='def save_context(self, inputs: Dict[str, Any], outputs: Dict[str, str]) -> None:\\n        \"\"\"Save context from this conversation to buffer.\"\"\"\\n        input_str, output_str = self._get_input_output(inputs, outputs)\\n        self.chat_memory.add_messages(\\n            [HumanMessage(content=input_str), AIMessage(content=output_str)]\\n        )\\n\\n    async def asave_context(\\n        self, inputs: Dict[str, Any], outputs: Dict[str, str]\\n    ) -> None:\\n        \"\"\"Save context from this conversation to buffer.\"\"\"\\n        input_str, output_str = self._get_input_output(inputs, outputs)\\n        await self.chat_memory.aadd_messages(\\n            [HumanMessage(content=input_str), AIMessage(content=output_str)]\\n        )\\n\\n    def clear(self) -> None:\\n        \"\"\"Clear memory contents.\"\"\"\\n        self.chat_memory.clear()\\n\\n    async def aclear(self) -> None:\\n        \"\"\"Clear memory contents.\"\"\"\\n        await self.chat_memory.aclear()', metadata={'source': 'test_repo/libs/langchain/langchain/memory/chat_memory.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from typing import Any, Dict, List, Optional\\n\\nimport requests\\nfrom langchain_core.messages import get_buffer_string\\n\\nfrom langchain.memory.chat_memory import BaseChatMemory\\n\\nMANAGED_URL = \"https://api.getmetal.io/v1/motorhead\"\\n# LOCAL_URL = \"http://localhost:8080\"', metadata={'source': 'test_repo/libs/langchain/langchain/memory/motorhead_memory.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='class MotorheadMemory(BaseChatMemory):\\n    \"\"\"Chat message memory backed by Motorhead service.\"\"\"\\n\\n    url: str = MANAGED_URL\\n    timeout: int = 3000\\n    memory_key: str = \"history\"\\n    session_id: str\\n    context: Optional[str] = None\\n\\n    # Managed Params\\n    api_key: Optional[str] = None\\n    client_id: Optional[str] = None\\n\\n    def __get_headers(self) -> Dict[str, str]:\\n        is_managed = self.url == MANAGED_URL\\n\\n        headers = {\\n            \"Content-Type\": \"application/json\",\\n        }\\n\\n        if is_managed and not (self.api_key and self.client_id):\\n            raise ValueError(\\n                \"\"\"\\n                You must provide an API key or a client ID to use the managed\\n                version of Motorhead. Visit https://getmetal.io for more information.\\n                \"\"\"\\n            )\\n\\n        if is_managed and self.api_key and self.client_id:\\n            headers[\"x-metal-api-key\"] = self.api_key\\n            headers[\"x-metal-client-id\"] = self.client_id', metadata={'source': 'test_repo/libs/langchain/langchain/memory/motorhead_memory.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='return headers\\n\\n    async def init(self) -> None:\\n        res = requests.get(\\n            f\"{self.url}/sessions/{self.session_id}/memory\",\\n            timeout=self.timeout,\\n            headers=self.__get_headers(),\\n        )\\n        res_data = res.json()\\n        res_data = res_data.get(\"data\", res_data)  # Handle Managed Version\\n\\n        messages = res_data.get(\"messages\", [])\\n        context = res_data.get(\"context\", \"NONE\")\\n\\n        for message in reversed(messages):\\n            if message[\"role\"] == \"AI\":\\n                self.chat_memory.add_ai_message(message[\"content\"])\\n            else:\\n                self.chat_memory.add_user_message(message[\"content\"])\\n\\n        if context and context != \"NONE\":\\n            self.context = context', metadata={'source': 'test_repo/libs/langchain/langchain/memory/motorhead_memory.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='if context and context != \"NONE\":\\n            self.context = context\\n\\n    def load_memory_variables(self, values: Dict[str, Any]) -> Dict[str, Any]:\\n        if self.return_messages:\\n            return {self.memory_key: self.chat_memory.messages}\\n        else:\\n            return {self.memory_key: get_buffer_string(self.chat_memory.messages)}\\n\\n    @property\\n    def memory_variables(self) -> List[str]:\\n        return [self.memory_key]', metadata={'source': 'test_repo/libs/langchain/langchain/memory/motorhead_memory.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='@property\\n    def memory_variables(self) -> List[str]:\\n        return [self.memory_key]\\n\\n    def save_context(self, inputs: Dict[str, Any], outputs: Dict[str, str]) -> None:\\n        input_str, output_str = self._get_input_output(inputs, outputs)\\n        requests.post(\\n            f\"{self.url}/sessions/{self.session_id}/memory\",\\n            timeout=self.timeout,\\n            json={\\n                \"messages\": [\\n                    {\"role\": \"Human\", \"content\": f\"{input_str}\"},\\n                    {\"role\": \"AI\", \"content\": f\"{output_str}\"},\\n                ]\\n            },\\n            headers=self.__get_headers(),\\n        )\\n        super().save_context(inputs, outputs)\\n\\n    def delete_session(self) -> None:\\n        \"\"\"Delete a session\"\"\"\\n        requests.delete(f\"{self.url}/sessions/{self.session_id}/memory\")', metadata={'source': 'test_repo/libs/langchain/langchain/memory/motorhead_memory.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from langchain_community.chat_message_histories.redis import RedisChatMessageHistory\\n\\n__all__ = [\"RedisChatMessageHistory\"]', metadata={'source': 'test_repo/libs/langchain/langchain/memory/chat_message_histories/redis.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from langchain_community.chat_message_histories.elasticsearch import (\\n    ElasticsearchChatMessageHistory,\\n)\\n\\n__all__ = [\"ElasticsearchChatMessageHistory\"]', metadata={'source': 'test_repo/libs/langchain/langchain/memory/chat_message_histories/elasticsearch.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from langchain_community.chat_message_histories.cassandra import (\\n    CassandraChatMessageHistory,\\n)\\n\\n__all__ = [\"CassandraChatMessageHistory\"]', metadata={'source': 'test_repo/libs/langchain/langchain/memory/chat_message_histories/cassandra.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='import warnings\\nfrom typing import Any\\n\\nfrom langchain_core._api import LangChainDeprecationWarning\\n\\nfrom langchain.utils.interactive_env import is_interactive_env', metadata={'source': 'test_repo/libs/langchain/langchain/memory/chat_message_histories/__init__.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='def __getattr__(name: str) -> Any:\\n    from langchain_community import chat_message_histories\\n\\n    # If not in interactive env, raise warning.\\n    if not is_interactive_env():\\n        warnings.warn(\\n            \"Importing chat message histories from langchain is deprecated. Importing \"\\n            \"from langchain will no longer be supported as of langchain==0.2.0. \"\\n            \"Please import from langchain-community instead:\\\\n\\\\n\"\\n            f\"`from langchain_community.chat_message_histories import {name}`.\\\\n\\\\n\"\\n            \"To install langchain-community run `pip install -U langchain-community`.\",\\n            category=LangChainDeprecationWarning,\\n        )\\n\\n    return getattr(chat_message_histories, name)', metadata={'source': 'test_repo/libs/langchain/langchain/memory/chat_message_histories/__init__.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='return getattr(chat_message_histories, name)\\n\\n\\n__all__ = [\\n    \"AstraDBChatMessageHistory\",\\n    \"ChatMessageHistory\",\\n    \"CassandraChatMessageHistory\",\\n    \"CosmosDBChatMessageHistory\",\\n    \"DynamoDBChatMessageHistory\",\\n    \"ElasticsearchChatMessageHistory\",\\n    \"FileChatMessageHistory\",\\n    \"FirestoreChatMessageHistory\",\\n    \"MomentoChatMessageHistory\",\\n    \"MongoDBChatMessageHistory\",\\n    \"PostgresChatMessageHistory\",\\n    \"RedisChatMessageHistory\",\\n    \"RocksetChatMessageHistory\",\\n    \"SQLChatMessageHistory\",\\n    \"StreamlitChatMessageHistory\",\\n    \"SingleStoreDBChatMessageHistory\",\\n    \"XataChatMessageHistory\",\\n    \"ZepChatMessageHistory\",\\n    \"UpstashRedisChatMessageHistory\",\\n    \"Neo4jChatMessageHistory\",\\n]', metadata={'source': 'test_repo/libs/langchain/langchain/memory/chat_message_histories/__init__.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from langchain_community.chat_message_histories.dynamodb import (\\n    DynamoDBChatMessageHistory,\\n)\\n\\n__all__ = [\"DynamoDBChatMessageHistory\"]', metadata={'source': 'test_repo/libs/langchain/langchain/memory/chat_message_histories/dynamodb.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from langchain_community.chat_message_histories.xata import XataChatMessageHistory\\n\\n__all__ = [\"XataChatMessageHistory\"]', metadata={'source': 'test_repo/libs/langchain/langchain/memory/chat_message_histories/xata.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from langchain_community.chat_message_histories.singlestoredb import (\\n    SingleStoreDBChatMessageHistory,\\n)\\n\\n__all__ = [\"SingleStoreDBChatMessageHistory\"]', metadata={'source': 'test_repo/libs/langchain/langchain/memory/chat_message_histories/singlestoredb.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from langchain_community.chat_message_histories.mongodb import (\\n    MongoDBChatMessageHistory,\\n)\\n\\n__all__ = [\"MongoDBChatMessageHistory\"]', metadata={'source': 'test_repo/libs/langchain/langchain/memory/chat_message_histories/mongodb.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from langchain_community.chat_message_histories.momento import (\\n    MomentoChatMessageHistory,\\n)\\n\\n__all__ = [\"MomentoChatMessageHistory\"]', metadata={'source': 'test_repo/libs/langchain/langchain/memory/chat_message_histories/momento.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from langchain_community.chat_message_histories.firestore import (\\n    FirestoreChatMessageHistory,\\n)\\n\\n__all__ = [\"FirestoreChatMessageHistory\"]', metadata={'source': 'test_repo/libs/langchain/langchain/memory/chat_message_histories/firestore.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from langchain_community.chat_message_histories.astradb import (\\n    AstraDBChatMessageHistory,\\n)\\n\\n__all__ = [\"AstraDBChatMessageHistory\"]', metadata={'source': 'test_repo/libs/langchain/langchain/memory/chat_message_histories/astradb.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from langchain_community.chat_message_histories.upstash_redis import (\\n    UpstashRedisChatMessageHistory,\\n)\\n\\n__all__ = [\"UpstashRedisChatMessageHistory\"]', metadata={'source': 'test_repo/libs/langchain/langchain/memory/chat_message_histories/upstash_redis.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from langchain_community.chat_message_histories.rocksetdb import (\\n    RocksetChatMessageHistory,\\n)\\n\\n__all__ = [\"RocksetChatMessageHistory\"]', metadata={'source': 'test_repo/libs/langchain/langchain/memory/chat_message_histories/rocksetdb.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from langchain_community.chat_message_histories.cosmos_db import (\\n    CosmosDBChatMessageHistory,\\n)\\n\\n__all__ = [\"CosmosDBChatMessageHistory\"]', metadata={'source': 'test_repo/libs/langchain/langchain/memory/chat_message_histories/cosmos_db.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from langchain_community.chat_message_histories.zep import ZepChatMessageHistory\\n\\n__all__ = [\"ZepChatMessageHistory\"]', metadata={'source': 'test_repo/libs/langchain/langchain/memory/chat_message_histories/zep.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from langchain_community.chat_message_histories.postgres import (\\n    PostgresChatMessageHistory,\\n)\\n\\n__all__ = [\"PostgresChatMessageHistory\"]', metadata={'source': 'test_repo/libs/langchain/langchain/memory/chat_message_histories/postgres.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from langchain_community.chat_message_histories.neo4j import Neo4jChatMessageHistory\\n\\n__all__ = [\"Neo4jChatMessageHistory\"]', metadata={'source': 'test_repo/libs/langchain/langchain/memory/chat_message_histories/neo4j.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from langchain_community.chat_message_histories.streamlit import (\\n    StreamlitChatMessageHistory,\\n)\\n\\n__all__ = [\"StreamlitChatMessageHistory\"]', metadata={'source': 'test_repo/libs/langchain/langchain/memory/chat_message_histories/streamlit.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from langchain_community.chat_message_histories.file import FileChatMessageHistory\\n\\n__all__ = [\"FileChatMessageHistory\"]', metadata={'source': 'test_repo/libs/langchain/langchain/memory/chat_message_histories/file.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from langchain_community.chat_message_histories.sql import (\\n    BaseMessageConverter,\\n    DefaultMessageConverter,\\n    SQLChatMessageHistory,\\n)\\n\\n__all__ = [\\n    \"BaseMessageConverter\",\\n    \"DefaultMessageConverter\",\\n    \"SQLChatMessageHistory\",\\n]', metadata={'source': 'test_repo/libs/langchain/langchain/memory/chat_message_histories/sql.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from langchain_community.chat_message_histories.in_memory import ChatMessageHistory\\n\\n__all__ = [\"ChatMessageHistory\"]', metadata={'source': 'test_repo/libs/langchain/langchain/memory/chat_message_histories/in_memory.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='\"\"\"LangChain **Runnable** and the **LangChain Expression Language (LCEL)**.\\n\\nThe LangChain Expression Language (LCEL) offers a declarative method to build\\nproduction-grade programs that harness the power of LLMs.\\n\\nPrograms created using LCEL and LangChain Runnables inherently support\\nsynchronous, asynchronous, batch, and streaming operations.\\n\\nSupport for **async** allows servers hosting the LCEL based programs\\nto scale better for higher concurrent loads.\\n\\n**Batch** operations allow for processing multiple inputs in parallel.\\n\\n**Streaming** of intermediate outputs, as they\\'re being generated, allows for\\ncreating more responsive UX.\\n\\nThis module contains non-core Runnable classes.\\n\"\"\"', metadata={'source': 'test_repo/libs/langchain/langchain/runnables/__init__.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from typing import Any, Optional\\n\\nfrom langchain_core.runnables.base import Input, Output, RunnableBindingBase\\n\\n\\nclass HubRunnable(RunnableBindingBase[Input, Output]):\\n    \"\"\"\\n    An instance of a runnable stored in the LangChain Hub.\\n    \"\"\"\\n\\n    owner_repo_commit: str\\n\\n    def __init__(\\n        self,\\n        owner_repo_commit: str,\\n        *,\\n        api_url: Optional[str] = None,\\n        api_key: Optional[str] = None,\\n        **kwargs: Any,\\n    ) -> None:\\n        from langchain.hub import pull\\n\\n        pulled = pull(owner_repo_commit, api_url=api_url, api_key=api_key)\\n        super_kwargs = {\\n            \"kwargs\": {},\\n            \"config\": {},\\n            **kwargs,\\n            \"bound\": pulled,\\n            \"owner_repo_commit\": owner_repo_commit,\\n        }\\n        super().__init__(**super_kwargs)', metadata={'source': 'test_repo/libs/langchain/langchain/runnables/hub.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from operator import itemgetter\\nfrom typing import Any, Callable, List, Mapping, Optional, Union\\n\\nfrom langchain_core.messages import BaseMessage\\nfrom langchain_core.output_parsers.openai_functions import JsonOutputFunctionsParser\\nfrom langchain_core.runnables import RouterRunnable, Runnable\\nfrom langchain_core.runnables.base import RunnableBindingBase\\nfrom typing_extensions import TypedDict\\n\\n\\nclass OpenAIFunction(TypedDict):\\n    \"\"\"A function description for ChatOpenAI\"\"\"\\n\\n    name: str\\n    \"\"\"The name of the function.\"\"\"\\n    description: str\\n    \"\"\"The description of the function.\"\"\"\\n    parameters: dict\\n    \"\"\"The parameters to the function.\"\"\"', metadata={'source': 'test_repo/libs/langchain/langchain/runnables/openai_functions.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='class OpenAIFunctionsRouter(RunnableBindingBase[BaseMessage, Any]):\\n    \"\"\"A runnable that routes to the selected function.\"\"\"\\n\\n    functions: Optional[List[OpenAIFunction]]\\n\\n    def __init__(\\n        self,\\n        runnables: Mapping[\\n            str,\\n            Union[\\n                Runnable[dict, Any],\\n                Callable[[dict], Any],\\n            ],\\n        ],\\n        functions: Optional[List[OpenAIFunction]] = None,\\n    ):\\n        if functions is not None:\\n            assert len(functions) == len(runnables)\\n            assert all(func[\"name\"] in runnables for func in functions)\\n        router = (\\n            JsonOutputFunctionsParser(args_only=False)\\n            | {\"key\": itemgetter(\"name\"), \"input\": itemgetter(\"arguments\")}\\n            | RouterRunnable(runnables)\\n        )\\n        super().__init__(bound=router, kwargs={}, functions=functions)', metadata={'source': 'test_repo/libs/langchain/langchain/runnables/openai_functions.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='\"\"\"**Evaluation** chains for grading LLM and Chain outputs.\\n\\nThis module contains off-the-shelf evaluation chains for grading the output of\\nLangChain primitives such as language models and chains.\\n\\n**Loading an evaluator**\\n\\nTo load an evaluator, you can use the :func:`load_evaluators <langchain.evaluation.loading.load_evaluators>` or\\n:func:`load_evaluator <langchain.evaluation.loading.load_evaluator>` functions with the\\nnames of the evaluators to load.\\n\\n.. code-block:: python\\n\\n    from langchain.evaluation import load_evaluator\\n\\n    evaluator = load_evaluator(\"qa\")\\n    evaluator.evaluate_strings(\\n        prediction=\"We sold more than 40,000 units last week\",\\n        input=\"How many units did we sell last week?\",\\n        reference=\"We sold 32,378 units\",\\n    )\\n\\nThe evaluator must be one of :class:`EvaluatorType <langchain.evaluation.schema.EvaluatorType>`.\\n\\n**Datasets**', metadata={'source': 'test_repo/libs/langchain/langchain/evaluation/__init__.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='**Datasets**\\n\\nTo load one of the LangChain HuggingFace datasets, you can use the :func:`load_dataset <langchain.evaluation.loading.load_dataset>` function with the\\nname of the dataset to load.\\n\\n.. code-block:: python\\n\\n        from langchain.evaluation import load_dataset\\n        ds = load_dataset(\"llm-math\")\\n\\n**Some common use cases for evaluation include:**', metadata={'source': 'test_repo/libs/langchain/langchain/evaluation/__init__.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content=\"- Grading the accuracy of a response against ground truth answers: :class:`QAEvalChain <langchain.evaluation.qa.eval_chain.QAEvalChain>`\\n- Comparing the output of two models: :class:`PairwiseStringEvalChain <langchain.evaluation.comparison.eval_chain.PairwiseStringEvalChain>` or :class:`LabeledPairwiseStringEvalChain <langchain.evaluation.comparison.eval_chain.LabeledPairwiseStringEvalChain>` when there is additionally a reference label.\\n- Judging the efficacy of an agent's tool usage: :class:`TrajectoryEvalChain <langchain.evaluation.agents.trajectory_eval_chain.TrajectoryEvalChain>`\\n- Checking whether an output complies with a set of criteria: :class:`CriteriaEvalChain <langchain.evaluation.criteria.eval_chain.CriteriaEvalChain>` or :class:`LabeledCriteriaEvalChain <langchain.evaluation.criteria.eval_chain.LabeledCriteriaEvalChain>` when there is additionally a reference label.\", metadata={'source': 'test_repo/libs/langchain/langchain/evaluation/__init__.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='- Computing semantic difference between a prediction and reference: :class:`EmbeddingDistanceEvalChain <langchain.evaluation.embedding_distance.base.EmbeddingDistanceEvalChain>` or between two predictions: :class:`PairwiseEmbeddingDistanceEvalChain <langchain.evaluation.embedding_distance.base.PairwiseEmbeddingDistanceEvalChain>` \\n- Measuring the string distance between a prediction and reference :class:`StringDistanceEvalChain <langchain.evaluation.string_distance.base.StringDistanceEvalChain>` or between two predictions :class:`PairwiseStringDistanceEvalChain <langchain.evaluation.string_distance.base.PairwiseStringDistanceEvalChain>`', metadata={'source': 'test_repo/libs/langchain/langchain/evaluation/__init__.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='**Low-level API**\\n\\nThese evaluators implement one of the following interfaces:\\n\\n- :class:`StringEvaluator <langchain.evaluation.schema.StringEvaluator>`: Evaluate a prediction string against a reference label and/or input context.\\n- :class:`PairwiseStringEvaluator <langchain.evaluation.schema.PairwiseStringEvaluator>`: Evaluate two prediction strings against each other. Useful for scoring preferences, measuring similarity between two chain or llm agents, or comparing outputs on similar inputs.\\n- :class:`AgentTrajectoryEvaluator <langchain.evaluation.schema.AgentTrajectoryEvaluator>` Evaluate the full sequence of actions taken by an agent.\\n\\nThese interfaces enable easier composability and usage within a higher level evaluation framework.', metadata={'source': 'test_repo/libs/langchain/langchain/evaluation/__init__.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='\"\"\"  # noqa: E501\\nfrom langchain.evaluation.agents import TrajectoryEvalChain\\nfrom langchain.evaluation.comparison import (\\n    LabeledPairwiseStringEvalChain,\\n    PairwiseStringEvalChain,\\n)\\nfrom langchain.evaluation.criteria import (\\n    Criteria,\\n    CriteriaEvalChain,\\n    LabeledCriteriaEvalChain,\\n)\\nfrom langchain.evaluation.embedding_distance import (\\n    EmbeddingDistance,\\n    EmbeddingDistanceEvalChain,\\n    PairwiseEmbeddingDistanceEvalChain,\\n)\\nfrom langchain.evaluation.exact_match.base import ExactMatchStringEvaluator\\nfrom langchain.evaluation.loading import load_dataset, load_evaluator, load_evaluators\\nfrom langchain.evaluation.parsing.base import (\\n    JsonEqualityEvaluator,\\n    JsonValidityEvaluator,\\n)\\nfrom langchain.evaluation.parsing.json_distance import JsonEditDistanceEvaluator\\nfrom langchain.evaluation.parsing.json_schema import JsonSchemaEvaluator\\nfrom langchain.evaluation.qa import ContextQAEvalChain, CotQAEvalChain, QAEvalChain', metadata={'source': 'test_repo/libs/langchain/langchain/evaluation/__init__.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from langchain.evaluation.qa import ContextQAEvalChain, CotQAEvalChain, QAEvalChain\\nfrom langchain.evaluation.regex_match.base import RegexMatchStringEvaluator\\nfrom langchain.evaluation.schema import (\\n    AgentTrajectoryEvaluator,\\n    EvaluatorType,\\n    PairwiseStringEvaluator,\\n    StringEvaluator,\\n)\\nfrom langchain.evaluation.scoring import (\\n    LabeledScoreStringEvalChain,\\n    ScoreStringEvalChain,\\n)\\nfrom langchain.evaluation.string_distance import (\\n    PairwiseStringDistanceEvalChain,\\n    StringDistance,\\n    StringDistanceEvalChain,\\n)', metadata={'source': 'test_repo/libs/langchain/langchain/evaluation/__init__.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='__all__ = [\\n    \"EvaluatorType\",\\n    \"ExactMatchStringEvaluator\",\\n    \"RegexMatchStringEvaluator\",\\n    \"PairwiseStringEvalChain\",\\n    \"LabeledPairwiseStringEvalChain\",\\n    \"QAEvalChain\",\\n    \"CotQAEvalChain\",\\n    \"ContextQAEvalChain\",\\n    \"StringEvaluator\",\\n    \"PairwiseStringEvaluator\",\\n    \"TrajectoryEvalChain\",\\n    \"CriteriaEvalChain\",\\n    \"Criteria\",\\n    \"EmbeddingDistance\",\\n    \"EmbeddingDistanceEvalChain\",\\n    \"PairwiseEmbeddingDistanceEvalChain\",\\n    \"StringDistance\",\\n    \"StringDistanceEvalChain\",\\n    \"PairwiseStringDistanceEvalChain\",\\n    \"LabeledCriteriaEvalChain\",\\n    \"load_evaluators\",\\n    \"load_evaluator\",\\n    \"load_dataset\",\\n    \"AgentTrajectoryEvaluator\",\\n    \"ScoreStringEvalChain\",\\n    \"LabeledScoreStringEvalChain\",\\n    \"JsonValidityEvaluator\",\\n    \"JsonEqualityEvaluator\",\\n    \"JsonEditDistanceEvaluator\",\\n    \"JsonSchemaEvaluator\",\\n]', metadata={'source': 'test_repo/libs/langchain/langchain/evaluation/__init__.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='\"\"\"Loading datasets and evaluators.\"\"\"\\nfrom typing import Any, Dict, List, Optional, Sequence, Type, Union\\n\\nfrom langchain_community.chat_models.openai import ChatOpenAI\\nfrom langchain_core.language_models import BaseLanguageModel', metadata={'source': 'test_repo/libs/langchain/langchain/evaluation/loading.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from langchain.chains.base import Chain\\nfrom langchain.evaluation.agents.trajectory_eval_chain import TrajectoryEvalChain\\nfrom langchain.evaluation.comparison import PairwiseStringEvalChain\\nfrom langchain.evaluation.comparison.eval_chain import LabeledPairwiseStringEvalChain\\nfrom langchain.evaluation.criteria.eval_chain import (\\n    CriteriaEvalChain,\\n    LabeledCriteriaEvalChain,\\n)\\nfrom langchain.evaluation.embedding_distance.base import (\\n    EmbeddingDistanceEvalChain,\\n    PairwiseEmbeddingDistanceEvalChain,\\n)\\nfrom langchain.evaluation.exact_match.base import ExactMatchStringEvaluator\\nfrom langchain.evaluation.parsing.base import (\\n    JsonEqualityEvaluator,\\n    JsonValidityEvaluator,\\n)\\nfrom langchain.evaluation.parsing.json_distance import JsonEditDistanceEvaluator\\nfrom langchain.evaluation.parsing.json_schema import JsonSchemaEvaluator\\nfrom langchain.evaluation.qa import ContextQAEvalChain, CotQAEvalChain, QAEvalChain', metadata={'source': 'test_repo/libs/langchain/langchain/evaluation/loading.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from langchain.evaluation.qa import ContextQAEvalChain, CotQAEvalChain, QAEvalChain\\nfrom langchain.evaluation.regex_match.base import RegexMatchStringEvaluator\\nfrom langchain.evaluation.schema import EvaluatorType, LLMEvalChain, StringEvaluator\\nfrom langchain.evaluation.scoring.eval_chain import (\\n    LabeledScoreStringEvalChain,\\n    ScoreStringEvalChain,\\n)\\nfrom langchain.evaluation.string_distance.base import (\\n    PairwiseStringDistanceEvalChain,\\n    StringDistanceEvalChain,\\n)', metadata={'source': 'test_repo/libs/langchain/langchain/evaluation/loading.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='def load_dataset(uri: str) -> List[Dict]:\\n    \"\"\"Load a dataset from the `LangChainDatasets on HuggingFace <https://huggingface.co/LangChainDatasets>`_.\\n\\n    Args:\\n        uri: The uri of the dataset to load.\\n\\n    Returns:\\n        A list of dictionaries, each representing a row in the dataset.\\n\\n    **Prerequisites**\\n\\n    .. code-block:: shell\\n\\n        pip install datasets\\n\\n    Examples\\n    --------\\n    .. code-block:: python\\n\\n        from langchain.evaluation import load_dataset\\n        ds = load_dataset(\"llm-math\")\\n    \"\"\"  # noqa: E501\\n    try:\\n        from datasets import load_dataset\\n    except ImportError:\\n        raise ImportError(\\n            \"load_dataset requires the `datasets` package.\"\\n            \" Please install with `pip install datasets`\"\\n        )\\n\\n    dataset = load_dataset(f\"LangChainDatasets/{uri}\")\\n    return [d for d in dataset[\"train\"]]', metadata={'source': 'test_repo/libs/langchain/langchain/evaluation/loading.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='_EVALUATOR_MAP: Dict[\\n    EvaluatorType, Union[Type[LLMEvalChain], Type[Chain], Type[StringEvaluator]]\\n] = {\\n    EvaluatorType.QA: QAEvalChain,\\n    EvaluatorType.COT_QA: CotQAEvalChain,\\n    EvaluatorType.CONTEXT_QA: ContextQAEvalChain,\\n    EvaluatorType.PAIRWISE_STRING: PairwiseStringEvalChain,\\n    EvaluatorType.SCORE_STRING: ScoreStringEvalChain,\\n    EvaluatorType.LABELED_PAIRWISE_STRING: LabeledPairwiseStringEvalChain,\\n    EvaluatorType.LABELED_SCORE_STRING: LabeledScoreStringEvalChain,\\n    EvaluatorType.AGENT_TRAJECTORY: TrajectoryEvalChain,\\n    EvaluatorType.CRITERIA: CriteriaEvalChain,\\n    EvaluatorType.LABELED_CRITERIA: LabeledCriteriaEvalChain,\\n    EvaluatorType.STRING_DISTANCE: StringDistanceEvalChain,\\n    EvaluatorType.PAIRWISE_STRING_DISTANCE: PairwiseStringDistanceEvalChain,\\n    EvaluatorType.EMBEDDING_DISTANCE: EmbeddingDistanceEvalChain,\\n    EvaluatorType.PAIRWISE_EMBEDDING_DISTANCE: PairwiseEmbeddingDistanceEvalChain,', metadata={'source': 'test_repo/libs/langchain/langchain/evaluation/loading.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='EvaluatorType.PAIRWISE_EMBEDDING_DISTANCE: PairwiseEmbeddingDistanceEvalChain,\\n    EvaluatorType.JSON_VALIDITY: JsonValidityEvaluator,\\n    EvaluatorType.JSON_EQUALITY: JsonEqualityEvaluator,\\n    EvaluatorType.JSON_EDIT_DISTANCE: JsonEditDistanceEvaluator,\\n    EvaluatorType.JSON_SCHEMA_VALIDATION: JsonSchemaEvaluator,\\n    EvaluatorType.REGEX_MATCH: RegexMatchStringEvaluator,\\n    EvaluatorType.EXACT_MATCH: ExactMatchStringEvaluator,\\n}', metadata={'source': 'test_repo/libs/langchain/langchain/evaluation/loading.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='def load_evaluator(\\n    evaluator: EvaluatorType,\\n    *,\\n    llm: Optional[BaseLanguageModel] = None,\\n    **kwargs: Any,\\n) -> Union[Chain, StringEvaluator]:\\n    \"\"\"Load the requested evaluation chain specified by a string.\\n\\n    Parameters\\n    ----------\\n    evaluator : EvaluatorType\\n        The type of evaluator to load.\\n    llm : BaseLanguageModel, optional\\n        The language model to use for evaluation, by default None\\n    **kwargs : Any\\n        Additional keyword arguments to pass to the evaluator.\\n\\n    Returns\\n    -------\\n    Chain\\n        The loaded evaluation chain.', metadata={'source': 'test_repo/libs/langchain/langchain/evaluation/loading.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='Examples\\n    --------\\n    >>> from langchain.evaluation import load_evaluator, EvaluatorType\\n    >>> evaluator = load_evaluator(EvaluatorType.QA)\\n    \"\"\"\\n    if evaluator not in _EVALUATOR_MAP:\\n        raise ValueError(\\n            f\"Unknown evaluator type: {evaluator}\"\\n            f\"\\\\nValid types are: {list(_EVALUATOR_MAP.keys())}\"\\n        )\\n    evaluator_cls = _EVALUATOR_MAP[evaluator]\\n    if issubclass(evaluator_cls, LLMEvalChain):\\n        try:\\n            llm = llm or ChatOpenAI(\\n                model=\"gpt-4\", model_kwargs={\"seed\": 42}, temperature=0\\n            )\\n        except Exception as e:\\n            raise ValueError(\\n                f\"Evaluation with the {evaluator_cls} requires a \"\\n                \"language model to function.\"\\n                \" Failed to create the default \\'gpt-4\\' model.\"\\n                \" Please manually provide an evaluation LLM\"\\n                \" or check your openai credentials.\"\\n            ) from e', metadata={'source': 'test_repo/libs/langchain/langchain/evaluation/loading.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='\" or check your openai credentials.\"\\n            ) from e\\n        return evaluator_cls.from_llm(llm=llm, **kwargs)\\n    else:\\n        return evaluator_cls(**kwargs)', metadata={'source': 'test_repo/libs/langchain/langchain/evaluation/loading.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='def load_evaluators(\\n    evaluators: Sequence[EvaluatorType],\\n    *,\\n    llm: Optional[BaseLanguageModel] = None,\\n    config: Optional[dict] = None,\\n    **kwargs: Any,\\n) -> List[Union[Chain, StringEvaluator]]:\\n    \"\"\"Load evaluators specified by a list of evaluator types.\\n\\n    Parameters\\n    ----------\\n    evaluators : Sequence[EvaluatorType]\\n        The list of evaluator types to load.\\n    llm : BaseLanguageModel, optional\\n        The language model to use for evaluation, if none is provided, a default\\n        ChatOpenAI gpt-4 model will be used.\\n    config : dict, optional\\n        A dictionary mapping evaluator types to additional keyword arguments,\\n        by default None\\n    **kwargs : Any\\n        Additional keyword arguments to pass to all evaluators.\\n\\n    Returns\\n    -------\\n    List[Chain]\\n        The loaded evaluators.', metadata={'source': 'test_repo/libs/langchain/langchain/evaluation/loading.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='Returns\\n    -------\\n    List[Chain]\\n        The loaded evaluators.\\n\\n    Examples\\n    --------\\n    >>> from langchain.evaluation import load_evaluators, EvaluatorType\\n    >>> evaluators = [EvaluatorType.QA, EvaluatorType.CRITERIA]\\n    >>> loaded_evaluators = load_evaluators(evaluators, criteria=\"helpfulness\")\\n    \"\"\"\\n    loaded = []\\n    for evaluator in evaluators:\\n        _kwargs = config.get(evaluator, {}) if config else {}\\n        loaded.append(load_evaluator(evaluator, llm=llm, **{**kwargs, **_kwargs}))\\n    return loaded', metadata={'source': 'test_repo/libs/langchain/langchain/evaluation/loading.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='\"\"\"Interfaces to be implemented by general evaluators.\"\"\"\\nfrom __future__ import annotations\\n\\nimport logging\\nfrom abc import ABC, abstractmethod\\nfrom enum import Enum\\nfrom typing import Any, Optional, Sequence, Tuple, Union\\nfrom warnings import warn\\n\\nfrom langchain_core.agents import AgentAction\\nfrom langchain_core.language_models import BaseLanguageModel\\nfrom langchain_core.runnables.config import run_in_executor\\n\\nfrom langchain.chains.base import Chain\\n\\nlogger = logging.getLogger(__name__)', metadata={'source': 'test_repo/libs/langchain/langchain/evaluation/schema.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='class EvaluatorType(str, Enum):\\n    \"\"\"The types of the evaluators.\"\"\"', metadata={'source': 'test_repo/libs/langchain/langchain/evaluation/schema.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='QA = \"qa\"\\n    \"\"\"Question answering evaluator, which grades answers to questions\\n    directly using an LLM.\"\"\"\\n    COT_QA = \"cot_qa\"\\n    \"\"\"Chain of thought question answering evaluator, which grades\\n    answers to questions using\\n    chain of thought \\'reasoning\\'.\"\"\"\\n    CONTEXT_QA = \"context_qa\"\\n    \"\"\"Question answering evaluator that incorporates \\'context\\' in the response.\"\"\"\\n    PAIRWISE_STRING = \"pairwise_string\"\\n    \"\"\"The pairwise string evaluator, which predicts the preferred prediction from\\n    between two models.\"\"\"\\n    SCORE_STRING = \"score_string\"\\n    \"\"\"The scored string evaluator, which gives a score between 1 and 10 \\n    to a prediction.\"\"\"\\n    LABELED_PAIRWISE_STRING = \"labeled_pairwise_string\"\\n    \"\"\"The labeled pairwise string evaluator, which predicts the preferred prediction\\n    from between two models based on a ground truth reference label.\"\"\"\\n    LABELED_SCORE_STRING = \"labeled_score_string\"', metadata={'source': 'test_repo/libs/langchain/langchain/evaluation/schema.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='LABELED_SCORE_STRING = \"labeled_score_string\"\\n    \"\"\"The labeled scored string evaluator, which gives a score between 1 and 10\\n    to a prediction based on a ground truth reference label.\"\"\"\\n    AGENT_TRAJECTORY = \"trajectory\"\\n    \"\"\"The agent trajectory evaluator, which grades the agent\\'s intermediate steps.\"\"\"\\n    CRITERIA = \"criteria\"\\n    \"\"\"The criteria evaluator, which evaluates a model based on a\\n    custom set of criteria without any reference labels.\"\"\"\\n    LABELED_CRITERIA = \"labeled_criteria\"\\n    \"\"\"The labeled criteria evaluator, which evaluates a model based on a\\n    custom set of criteria, with a reference label.\"\"\"\\n    STRING_DISTANCE = \"string_distance\"\\n    \"\"\"Compare predictions to a reference answer using string edit distances.\"\"\"\\n    EXACT_MATCH = \"exact_match\"\\n    \"\"\"Compare predictions to a reference answer using exact matching.\"\"\"\\n    REGEX_MATCH = \"regex_match\"\\n    \"\"\"Compare predictions to a reference answer using regular expressions.\"\"\"', metadata={'source': 'test_repo/libs/langchain/langchain/evaluation/schema.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='\"\"\"Compare predictions to a reference answer using regular expressions.\"\"\"\\n    PAIRWISE_STRING_DISTANCE = \"pairwise_string_distance\"\\n    \"\"\"Compare predictions based on string edit distances.\"\"\"\\n    EMBEDDING_DISTANCE = \"embedding_distance\"\\n    \"\"\"Compare a prediction to a reference label using embedding distance.\"\"\"\\n    PAIRWISE_EMBEDDING_DISTANCE = \"pairwise_embedding_distance\"\\n    \"\"\"Compare two predictions using embedding distance.\"\"\"\\n    JSON_VALIDITY = \"json_validity\"\\n    \"\"\"Check if a prediction is valid JSON.\"\"\"\\n    JSON_EQUALITY = \"json_equality\"\\n    \"\"\"Check if a prediction is equal to a reference JSON.\"\"\"\\n    JSON_EDIT_DISTANCE = \"json_edit_distance\"\\n    \"\"\"Compute the edit distance between two JSON strings after canonicalization.\"\"\"\\n    JSON_SCHEMA_VALIDATION = \"json_schema_validation\"\\n    \"\"\"Check if a prediction is valid JSON according to a JSON schema.\"\"\"', metadata={'source': 'test_repo/libs/langchain/langchain/evaluation/schema.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='class LLMEvalChain(Chain):\\n    \"\"\"A base class for evaluators that use an LLM.\"\"\"\\n\\n    @classmethod\\n    @abstractmethod\\n    def from_llm(cls, llm: BaseLanguageModel, **kwargs: Any) -> LLMEvalChain:\\n        \"\"\"Create a new evaluator from an LLM.\"\"\"', metadata={'source': 'test_repo/libs/langchain/langchain/evaluation/schema.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='class _EvalArgsMixin:\\n    \"\"\"Mixin for checking evaluation arguments.\"\"\"\\n\\n    @property\\n    def requires_reference(self) -> bool:\\n        \"\"\"Whether this evaluator requires a reference label.\"\"\"\\n        return False\\n\\n    @property\\n    def requires_input(self) -> bool:\\n        \"\"\"Whether this evaluator requires an input string.\"\"\"\\n        return False\\n\\n    @property\\n    def _skip_input_warning(self) -> str:\\n        \"\"\"Warning to show when input is ignored.\"\"\"\\n        return f\"Ignoring input in {self.__class__.__name__}, as it is not expected.\"\\n\\n    @property\\n    def _skip_reference_warning(self) -> str:\\n        \"\"\"Warning to show when reference is ignored.\"\"\"\\n        return (\\n            f\"Ignoring reference in {self.__class__.__name__}, as it is not expected.\"\\n        )\\n\\n    def _check_evaluation_args(\\n        self,\\n        reference: Optional[str] = None,\\n        input: Optional[str] = None,\\n    ) -> None:\\n        \"\"\"Check if the evaluation arguments are valid.', metadata={'source': 'test_repo/libs/langchain/langchain/evaluation/schema.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='Args:\\n            reference (Optional[str], optional): The reference label.\\n            input (Optional[str], optional): The input string.\\n        Raises:\\n            ValueError: If the evaluator requires an input string but none is provided,\\n                or if the evaluator requires a reference label but none is provided.\\n        \"\"\"\\n        if self.requires_input and input is None:\\n            raise ValueError(f\"{self.__class__.__name__} requires an input string.\")\\n        elif input is not None and not self.requires_input:\\n            warn(self._skip_input_warning)\\n        if self.requires_reference and reference is None:\\n            raise ValueError(f\"{self.__class__.__name__} requires a reference string.\")\\n        elif reference is not None and not self.requires_reference:\\n            warn(self._skip_reference_warning)', metadata={'source': 'test_repo/libs/langchain/langchain/evaluation/schema.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='class StringEvaluator(_EvalArgsMixin, ABC):\\n    \"\"\"Grade, tag, or otherwise evaluate predictions relative to their inputs\\n    and/or reference labels.\"\"\"\\n\\n    @property\\n    def evaluation_name(self) -> str:\\n        \"\"\"The name of the evaluation.\"\"\"\\n        return self.__class__.__name__\\n\\n    @property\\n    def requires_reference(self) -> bool:\\n        \"\"\"Whether this evaluator requires a reference label.\"\"\"\\n        return False\\n\\n    @abstractmethod\\n    def _evaluate_strings(\\n        self,\\n        *,\\n        prediction: Union[str, Any],\\n        reference: Optional[Union[str, Any]] = None,\\n        input: Optional[Union[str, Any]] = None,\\n        **kwargs: Any,\\n    ) -> dict:\\n        \"\"\"Evaluate Chain or LLM output, based on optional input and label.', metadata={'source': 'test_repo/libs/langchain/langchain/evaluation/schema.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='Args:\\n            prediction (str): The LLM or chain prediction to evaluate.\\n            reference (Optional[str], optional): The reference label to evaluate against.\\n            input (Optional[str], optional): The input to consider during evaluation.\\n            **kwargs: Additional keyword arguments, including callbacks, tags, etc.\\n        Returns:\\n            dict: The evaluation results containing the score or value.\\n                It is recommended that the dictionary contain the following keys:\\n                     - score: the score of the evaluation, if applicable.\\n                     - value: the string value of the evaluation, if applicable.\\n                     - reasoning: the reasoning for the evaluation, if applicable.\\n        \"\"\"  # noqa: E501', metadata={'source': 'test_repo/libs/langchain/langchain/evaluation/schema.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='async def _aevaluate_strings(\\n        self,\\n        *,\\n        prediction: Union[str, Any],\\n        reference: Optional[Union[str, Any]] = None,\\n        input: Optional[Union[str, Any]] = None,\\n        **kwargs: Any,\\n    ) -> dict:\\n        \"\"\"Asynchronously evaluate Chain or LLM output, based on optional input and label.', metadata={'source': 'test_repo/libs/langchain/langchain/evaluation/schema.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='Args:\\n            prediction (str): The LLM or chain prediction to evaluate.\\n            reference (Optional[str], optional): The reference label to evaluate against.\\n            input (Optional[str], optional): The input to consider during evaluation.\\n            **kwargs: Additional keyword arguments, including callbacks, tags, etc.\\n        Returns:\\n            dict: The evaluation results containing the score or value.\\n                It is recommended that the dictionary contain the following keys:\\n                     - score: the score of the evaluation, if applicable.\\n                     - value: the string value of the evaluation, if applicable.\\n                     - reasoning: the reasoning for the evaluation, if applicable.\\n        \"\"\"  # noqa: E501\\n        return await run_in_executor(\\n            None,\\n            self._evaluate_strings,\\n            prediction=prediction,\\n            reference=reference,\\n            input=input,\\n            **kwargs,\\n        )', metadata={'source': 'test_repo/libs/langchain/langchain/evaluation/schema.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='def evaluate_strings(\\n        self,\\n        *,\\n        prediction: str,\\n        reference: Optional[str] = None,\\n        input: Optional[str] = None,\\n        **kwargs: Any,\\n    ) -> dict:\\n        \"\"\"Evaluate Chain or LLM output, based on optional input and label.\\n\\n        Args:\\n            prediction (str): The LLM or chain prediction to evaluate.\\n            reference (Optional[str], optional): The reference label to evaluate against.\\n            input (Optional[str], optional): The input to consider during evaluation.\\n            **kwargs: Additional keyword arguments, including callbacks, tags, etc.\\n        Returns:\\n            dict: The evaluation results containing the score or value.\\n        \"\"\"  # noqa: E501\\n        self._check_evaluation_args(reference=reference, input=input)\\n        return self._evaluate_strings(\\n            prediction=prediction, reference=reference, input=input, **kwargs\\n        )', metadata={'source': 'test_repo/libs/langchain/langchain/evaluation/schema.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='async def aevaluate_strings(\\n        self,\\n        *,\\n        prediction: str,\\n        reference: Optional[str] = None,\\n        input: Optional[str] = None,\\n        **kwargs: Any,\\n    ) -> dict:\\n        \"\"\"Asynchronously evaluate Chain or LLM output, based on optional input and label.\\n\\n        Args:\\n            prediction (str): The LLM or chain prediction to evaluate.\\n            reference (Optional[str], optional): The reference label to evaluate against.\\n            input (Optional[str], optional): The input to consider during evaluation.\\n            **kwargs: Additional keyword arguments, including callbacks, tags, etc.\\n        Returns:\\n            dict: The evaluation results containing the score or value.\\n        \"\"\"  # noqa: E501\\n        self._check_evaluation_args(reference=reference, input=input)\\n        return await self._aevaluate_strings(\\n            prediction=prediction, reference=reference, input=input, **kwargs\\n        )', metadata={'source': 'test_repo/libs/langchain/langchain/evaluation/schema.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='class PairwiseStringEvaluator(_EvalArgsMixin, ABC):\\n    \"\"\"Compare the output of two models (or two outputs of the same model).\"\"\"\\n\\n    @abstractmethod\\n    def _evaluate_string_pairs(\\n        self,\\n        *,\\n        prediction: str,\\n        prediction_b: str,\\n        reference: Optional[str] = None,\\n        input: Optional[str] = None,\\n        **kwargs: Any,\\n    ) -> dict:\\n        \"\"\"Evaluate the output string pairs.\\n\\n        Args:\\n            prediction (str): The output string from the first model.\\n            prediction_b (str): The output string from the second model.\\n            reference (Optional[str], optional): The expected output / reference string.\\n            input (Optional[str], optional): The input string.\\n            **kwargs: Additional keyword arguments, such as callbacks and optional reference strings.\\n        Returns:\\n            dict: A dictionary containing the preference, scores, and/or other information.\\n        \"\"\"  # noqa: E501', metadata={'source': 'test_repo/libs/langchain/langchain/evaluation/schema.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='async def _aevaluate_string_pairs(\\n        self,\\n        *,\\n        prediction: str,\\n        prediction_b: str,\\n        reference: Optional[str] = None,\\n        input: Optional[str] = None,\\n        **kwargs: Any,\\n    ) -> dict:\\n        \"\"\"Asynchronously evaluate the output string pairs.', metadata={'source': 'test_repo/libs/langchain/langchain/evaluation/schema.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='Args:\\n            prediction (str): The output string from the first model.\\n            prediction_b (str): The output string from the second model.\\n            reference (Optional[str], optional): The expected output / reference string.\\n            input (Optional[str], optional): The input string.\\n            **kwargs: Additional keyword arguments, such as callbacks and optional reference strings.\\n        Returns:\\n            dict: A dictionary containing the preference, scores, and/or other information.\\n        \"\"\"  # noqa: E501\\n        return await run_in_executor(\\n            None,\\n            self._evaluate_string_pairs,\\n            prediction=prediction,\\n            prediction_b=prediction_b,\\n            reference=reference,\\n            input=input,\\n            **kwargs,\\n        )', metadata={'source': 'test_repo/libs/langchain/langchain/evaluation/schema.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='def evaluate_string_pairs(\\n        self,\\n        *,\\n        prediction: str,\\n        prediction_b: str,\\n        reference: Optional[str] = None,\\n        input: Optional[str] = None,\\n        **kwargs: Any,\\n    ) -> dict:\\n        \"\"\"Evaluate the output string pairs.', metadata={'source': 'test_repo/libs/langchain/langchain/evaluation/schema.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='Args:\\n            prediction (str): The output string from the first model.\\n            prediction_b (str): The output string from the second model.\\n            reference (Optional[str], optional): The expected output / reference string.\\n            input (Optional[str], optional): The input string.\\n            **kwargs: Additional keyword arguments, such as callbacks and optional reference strings.\\n        Returns:\\n            dict: A dictionary containing the preference, scores, and/or other information.\\n        \"\"\"  # noqa: E501\\n        self._check_evaluation_args(reference=reference, input=input)\\n        return self._evaluate_string_pairs(\\n            prediction=prediction,\\n            prediction_b=prediction_b,\\n            reference=reference,\\n            input=input,\\n            **kwargs,\\n        )', metadata={'source': 'test_repo/libs/langchain/langchain/evaluation/schema.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='async def aevaluate_string_pairs(\\n        self,\\n        *,\\n        prediction: str,\\n        prediction_b: str,\\n        reference: Optional[str] = None,\\n        input: Optional[str] = None,\\n        **kwargs: Any,\\n    ) -> dict:\\n        \"\"\"Asynchronously evaluate the output string pairs.', metadata={'source': 'test_repo/libs/langchain/langchain/evaluation/schema.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='Args:\\n            prediction (str): The output string from the first model.\\n            prediction_b (str): The output string from the second model.\\n            reference (Optional[str], optional): The expected output / reference string.\\n            input (Optional[str], optional): The input string.\\n            **kwargs: Additional keyword arguments, such as callbacks and optional reference strings.\\n        Returns:\\n            dict: A dictionary containing the preference, scores, and/or other information.\\n        \"\"\"  # noqa: E501\\n        self._check_evaluation_args(reference=reference, input=input)\\n        return await self._aevaluate_string_pairs(\\n            prediction=prediction,\\n            prediction_b=prediction_b,\\n            reference=reference,\\n            input=input,\\n            **kwargs,\\n        )', metadata={'source': 'test_repo/libs/langchain/langchain/evaluation/schema.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='class AgentTrajectoryEvaluator(_EvalArgsMixin, ABC):\\n    \"\"\"Interface for evaluating agent trajectories.\"\"\"\\n\\n    @property\\n    def requires_input(self) -> bool:\\n        \"\"\"Whether this evaluator requires an input string.\"\"\"\\n        return True\\n\\n    @abstractmethod\\n    def _evaluate_agent_trajectory(\\n        self,\\n        *,\\n        prediction: str,\\n        agent_trajectory: Sequence[Tuple[AgentAction, str]],\\n        input: str,\\n        reference: Optional[str] = None,\\n        **kwargs: Any,\\n    ) -> dict:\\n        \"\"\"Evaluate a trajectory.\\n\\n        Args:\\n            prediction (str): The final predicted response.\\n            agent_trajectory (List[Tuple[AgentAction, str]]):\\n                The intermediate steps forming the agent trajectory.\\n            input (str): The input to the agent.\\n            reference (Optional[str]): The reference answer.\\n\\n        Returns:\\n            dict: The evaluation result.\\n        \"\"\"', metadata={'source': 'test_repo/libs/langchain/langchain/evaluation/schema.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='Returns:\\n            dict: The evaluation result.\\n        \"\"\"\\n\\n    async def _aevaluate_agent_trajectory(\\n        self,\\n        *,\\n        prediction: str,\\n        agent_trajectory: Sequence[Tuple[AgentAction, str]],\\n        input: str,\\n        reference: Optional[str] = None,\\n        **kwargs: Any,\\n    ) -> dict:\\n        \"\"\"Asynchronously evaluate a trajectory.\\n\\n        Args:\\n            prediction (str): The final predicted response.\\n            agent_trajectory (List[Tuple[AgentAction, str]]):\\n                The intermediate steps forming the agent trajectory.\\n            input (str): The input to the agent.\\n            reference (Optional[str]): The reference answer.', metadata={'source': 'test_repo/libs/langchain/langchain/evaluation/schema.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='Returns:\\n            dict: The evaluation result.\\n        \"\"\"\\n        return await run_in_executor(\\n            None,\\n            self._evaluate_agent_trajectory,\\n            prediction=prediction,\\n            agent_trajectory=agent_trajectory,\\n            reference=reference,\\n            input=input,\\n            **kwargs,\\n        )\\n\\n    def evaluate_agent_trajectory(\\n        self,\\n        *,\\n        prediction: str,\\n        agent_trajectory: Sequence[Tuple[AgentAction, str]],\\n        input: str,\\n        reference: Optional[str] = None,\\n        **kwargs: Any,\\n    ) -> dict:\\n        \"\"\"Evaluate a trajectory.\\n\\n        Args:\\n            prediction (str): The final predicted response.\\n            agent_trajectory (List[Tuple[AgentAction, str]]):\\n                The intermediate steps forming the agent trajectory.\\n            input (str): The input to the agent.\\n            reference (Optional[str]): The reference answer.', metadata={'source': 'test_repo/libs/langchain/langchain/evaluation/schema.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='Returns:\\n            dict: The evaluation result.\\n        \"\"\"\\n        self._check_evaluation_args(reference=reference, input=input)\\n        return self._evaluate_agent_trajectory(\\n            prediction=prediction,\\n            input=input,\\n            agent_trajectory=agent_trajectory,\\n            reference=reference,\\n            **kwargs,\\n        )\\n\\n    async def aevaluate_agent_trajectory(\\n        self,\\n        *,\\n        prediction: str,\\n        agent_trajectory: Sequence[Tuple[AgentAction, str]],\\n        input: str,\\n        reference: Optional[str] = None,\\n        **kwargs: Any,\\n    ) -> dict:\\n        \"\"\"Asynchronously evaluate a trajectory.\\n\\n        Args:\\n            prediction (str): The final predicted response.\\n            agent_trajectory (List[Tuple[AgentAction, str]]):\\n                The intermediate steps forming the agent trajectory.\\n            input (str): The input to the agent.\\n            reference (Optional[str]): The reference answer.', metadata={'source': 'test_repo/libs/langchain/langchain/evaluation/schema.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='Returns:\\n            dict: The evaluation result.\\n        \"\"\"\\n        self._check_evaluation_args(reference=reference, input=input)\\n        return await self._aevaluate_agent_trajectory(\\n            prediction=prediction,\\n            input=input,\\n            agent_trajectory=agent_trajectory,\\n            reference=reference,\\n            **kwargs,\\n        )', metadata={'source': 'test_repo/libs/langchain/langchain/evaluation/schema.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='\"\"\"Criteria or rubric based evaluators.\\n\\nThese evaluators are useful for evaluating the\\noutput of a language model or chain against\\nspecified criteria or rubric.\\n\\nClasses\\n-------\\nCriteriaEvalChain : Evaluates the output of a language model or\\nchain against specified criteria.\\n\\nExamples\\n--------\\nUsing a predefined criterion:\\n>>> from langchain_community.llms import OpenAI\\n>>> from langchain.evaluation.criteria import CriteriaEvalChain\\n\\n>>> llm = OpenAI()\\n>>> criteria = \"conciseness\"\\n>>> chain = CriteriaEvalChain.from_llm(llm=llm, criteria=criteria)\\n>>> chain.evaluate_strings(\\n        prediction=\"The answer is 42.\",\\n        reference=\"42\",\\n        input=\"What is the answer to life, the universe, and everything?\",\\n    )\\n\\nUsing a custom criterion:\\n\\n>>> from langchain_community.llms import OpenAI\\n>>> from langchain.evaluation.criteria import LabeledCriteriaEvalChain', metadata={'source': 'test_repo/libs/langchain/langchain/evaluation/criteria/__init__.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='>>> llm = OpenAI()\\n>>> criteria = {\\n       \"hallucination\": (\\n            \"Does this submission contain information\"\\n            \" not present in the input or reference?\"\\n        ),\\n    }\\n>>> chain = LabeledCriteriaEvalChain.from_llm(\\n        llm=llm,\\n        criteria=criteria,\\n        )\\n>>> chain.evaluate_strings(\\n        prediction=\"The answer to life is 42.\",\\n        reference=\"It\\'s commonly known that the answer to life is 42.\",\\n        input=\"Please summarize the following: The answer to life, the universe, and everything is unknowable.\",\\n    )\\n\"\"\"  # noqa: E501\\n\\nfrom langchain.evaluation.criteria.eval_chain import (\\n    Criteria,\\n    CriteriaEvalChain,\\n    LabeledCriteriaEvalChain,\\n)\\n\\n__all__ = [\"CriteriaEvalChain\", \"LabeledCriteriaEvalChain\", \"Criteria\"]', metadata={'source': 'test_repo/libs/langchain/langchain/evaluation/criteria/__init__.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='class Criteria(str, Enum):\\n    \"\"\"A Criteria to evaluate.\"\"\"\\n\\n    CONCISENESS = \"conciseness\"\\n    RELEVANCE = \"relevance\"\\n    CORRECTNESS = \"correctness\"\\n    COHERENCE = \"coherence\"\\n    HARMFULNESS = \"harmfulness\"\\n    MALICIOUSNESS = \"maliciousness\"\\n    HELPFULNESS = \"helpfulness\"\\n    CONTROVERSIALITY = \"controversiality\"\\n    MISOGYNY = \"misogyny\"\\n    CRIMINALITY = \"criminality\"\\n    INSENSITIVITY = \"insensitivity\"\\n    DEPTH = \"depth\"\\n    CREATIVITY = \"creativity\"\\n    DETAIL = \"detail\"', metadata={'source': 'test_repo/libs/langchain/langchain/evaluation/criteria/eval_chain.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='class CriteriaResultOutputParser(BaseOutputParser[dict]):\\n    \"\"\"A parser for the output of the CriteriaEvalChain.\"\"\"\\n\\n    @property\\n    def _type(self) -> str:\\n        return \"criteria_result\"\\n\\n    def parse(self, text: str) -> Dict[str, Any]:\\n        \"\"\"Parse the output text.\\n\\n        Args:\\n            text (str): The output text to parse.\\n\\n        Returns:\\n            Dict: The parsed output.\\n        \"\"\"\\n        verdict = None\\n        score = None\\n        match_last = re.search(r\"\\\\s*(Y|N)\\\\s*$\", text, re.IGNORECASE)\\n        match_first = re.search(r\"^\\\\s*(Y|N)\\\\s*\", text, re.IGNORECASE)\\n        match_end = re.search(r\"\\\\b(Y|N)\\\\b\\\\s*$\", text, re.IGNORECASE)', metadata={'source': 'test_repo/libs/langchain/langchain/evaluation/criteria/eval_chain.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='if match_last:\\n            verdict = match_last.group(1).strip()\\n            text = text[: match_last.start()].strip()\\n        elif match_first:\\n            verdict = match_first.group(1).strip()\\n            text = text[match_first.end() :].strip()\\n        elif match_end:\\n            verdict = match_end.group(1).strip()\\n            text = text[: match_end.start()].strip()\\n        else:\\n            splits = text.strip().rsplit(\"\\\\n\", maxsplit=1)\\n            if len(splits) == 1:\\n                reasoning = \"\"\\n                verdict = splits[0]\\n            else:\\n                reasoning, verdict = splits\\n\\n        if verdict:\\n            score = (\\n                1 if verdict.upper() == \"Y\" else (0 if verdict.upper() == \"N\" else None)\\n            )\\n\\n        return {\\n            \"reasoning\": text.strip(),\\n            \"value\": verdict,\\n            \"score\": score,\\n        }', metadata={'source': 'test_repo/libs/langchain/langchain/evaluation/criteria/eval_chain.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='def resolve_criteria(\\n    criteria: Optional[Union[CRITERIA_TYPE, str]],\\n) -> Dict[str, str]:\\n    \"\"\"Resolve the criteria to evaluate.\\n\\n    Parameters\\n    ----------\\n    criteria : CRITERIA_TYPE\\n        The criteria to evaluate the runs against. It can be:\\n            -  a mapping of a criterion name to its description\\n            -  a single criterion name present in one of the default criteria\\n            -  a single `ConstitutionalPrinciple` instance\\n\\n    Returns\\n    -------\\n    Dict[str, str]\\n        A dictionary mapping criterion names to descriptions.', metadata={'source': 'test_repo/libs/langchain/langchain/evaluation/criteria/eval_chain.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='Examples\\n    --------\\n    >>> criterion = \"relevance\"\\n    >>> CriteriaEvalChain.resolve_criteria(criteria)\\n    {\\'relevance\\': \\'Is the submission referring to a real quote from the text?\\'}\\n    \"\"\"  # noqa: E501\\n    if criteria is None:\\n        return {\\n            \"helpfulness\": _SUPPORTED_CRITERIA[Criteria.HELPFULNESS],\\n        }\\n    if isinstance(criteria, Criteria):\\n        criteria_ = {criteria.value: _SUPPORTED_CRITERIA[criteria]}\\n    elif isinstance(criteria, str):\\n        criteria_ = {criteria: _SUPPORTED_CRITERIA[Criteria(criteria)]}\\n    elif isinstance(criteria, ConstitutionalPrinciple):\\n        criteria_ = {criteria.name: criteria.critique_request}\\n    else:\\n        if not criteria:\\n            raise ValueError(\\n                \"Criteria cannot be empty. \"\\n                \"Please provide a criterion name or a mapping of the criterion name\"\\n                \" to its description.\"\\n            )\\n        criteria_ = dict(criteria)\\n    return criteria_', metadata={'source': 'test_repo/libs/langchain/langchain/evaluation/criteria/eval_chain.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='class CriteriaEvalChain(StringEvaluator, LLMEvalChain, LLMChain):\\n    \"\"\"LLM Chain for evaluating runs against criteria.', metadata={'source': 'test_repo/libs/langchain/langchain/evaluation/criteria/eval_chain.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='Parameters\\n    ----------\\n    llm : BaseLanguageModel\\n        The language model to use for evaluation.\\n    criteria : Union[Mapping[str, str]]\\n        The criteria or rubric to evaluate the runs against. It can be a mapping of\\n        criterion name to its description, or a single criterion name.\\n    prompt : Optional[BasePromptTemplate], default=None\\n        The prompt template to use for generating prompts. If not provided, a\\n        default prompt template will be used based on the value of\\n        `requires_reference`.\\n    requires_reference : bool, default=False\\n        Whether the evaluation requires a reference text. If `True`, the\\n        `PROMPT_WITH_REFERENCES` template will be used, which includes the\\n        reference labels in the prompt. Otherwise, the `PROMPT` template will be\\n        used, which is a reference-free prompt.\\n    **kwargs : Any\\n        Additional keyword arguments to pass to the `LLMChain` constructor.', metadata={'source': 'test_repo/libs/langchain/langchain/evaluation/criteria/eval_chain.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='Returns\\n    -------\\n    CriteriaEvalChain\\n        An instance of the `CriteriaEvalChain` class.', metadata={'source': 'test_repo/libs/langchain/langchain/evaluation/criteria/eval_chain.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='Examples\\n    --------\\n    >>> from langchain_community.chat_models import ChatAnthropic\\n    >>> from langchain.evaluation.criteria import CriteriaEvalChain\\n    >>> llm = ChatAnthropic(temperature=0)\\n    >>> criteria = {\"my-custom-criterion\": \"Is the submission the most amazing ever?\"}\\n    >>> evaluator = CriteriaEvalChain.from_llm(llm=llm, criteria=criteria)\\n    >>> evaluator.evaluate_strings(prediction=\"Imagine an ice cream flavor for the color aquamarine\", input=\"Tell me an idea\")\\n    {\\n        \\'reasoning\\': \\'Here is my step-by-step reasoning for the given criteria:\\\\\\\\n\\\\\\\\nThe criterion is: \"Is the submission the most amazing ever?\" This is a subjective criterion and open to interpretation. The submission suggests an aquamarine-colored ice cream flavor which is creative but may or may not be considered the most amazing idea ever conceived. There are many possible amazing ideas and this one ice cream flavor suggestion may or may not rise to that level for every person. \\\\\\\\n\\\\\\\\nN\\',', metadata={'source': 'test_repo/libs/langchain/langchain/evaluation/criteria/eval_chain.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content=\"'value': 'N',\\n        'score': 0,\\n    }\", metadata={'source': 'test_repo/libs/langchain/langchain/evaluation/criteria/eval_chain.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='>>> from langchain_community.chat_models import ChatOpenAI\\n    >>> from langchain.evaluation.criteria import LabeledCriteriaEvalChain\\n    >>> llm = ChatOpenAI(model=\"gpt-4\", temperature=0)\\n    >>> criteria = \"correctness\"\\n    >>> evaluator = LabeledCriteriaEvalChain.from_llm(\\n    ...     llm=llm,\\n    ...     criteria=criteria,\\n    ... )\\n    >>> evaluator.evaluate_strings(\\n    ...   prediction=\"The answer is 4\",\\n    ...   input=\"How many apples are there?\",\\n    ...   reference=\"There are 3 apples\",\\n    ...   )\\n    {\\n        \\'score\\': 0,\\n        \\'reasoning\\': \\'The criterion for this task is the correctness of the submission. The submission states that there are 4 apples, but the reference indicates that there are actually 3 apples. Therefore, the submission is not correct, accurate, or factual according to the given criterion.\\\\\\\\n\\\\\\\\nN\\',\\n        \\'value\\': \\'N\\',\\n    }\\n\\n    \"\"\"  # noqa: E501', metadata={'source': 'test_repo/libs/langchain/langchain/evaluation/criteria/eval_chain.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='\"\"\"  # noqa: E501\\n\\n    output_parser: BaseOutputParser = Field(default_factory=CriteriaResultOutputParser)\\n    \"\"\"The parser to use to map the output to a structured result.\"\"\"\\n    criterion_name: str\\n    \"\"\"The name of the criterion being evaluated.\"\"\"\\n    output_key: str = \"results\"  #: :meta private:\\n\\n    @classmethod\\n    def is_lc_serializable(cls) -> bool:\\n        return False\\n\\n    class Config:\\n        \"\"\"Configuration for the QAEvalChain.\"\"\"\\n\\n        extra = Extra.ignore\\n\\n    @property\\n    def requires_reference(self) -> bool:\\n        \"\"\"Whether the evaluation requires a reference text.\"\"\"\\n        return False\\n\\n    @property\\n    def requires_input(self) -> bool:\\n        return True\\n\\n    @property\\n    def evaluation_name(self) -> str:\\n        \"\"\"Get the name of the evaluation.\\n\\n        Returns\\n        -------\\n        str\\n            The name of the evaluation.\\n        \"\"\"\\n        return self.criterion_name', metadata={'source': 'test_repo/libs/langchain/langchain/evaluation/criteria/eval_chain.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='@property\\n    def _skip_reference_warning(self) -> str:\\n        \"\"\"Warning to show when reference is ignored.\"\"\"\\n        return (\\n            f\"Ignoring reference in {self.__class__.__name__}, as it is not expected.\"\\n            \"\\\\nTo use references, use the labeled_criteria instead.\"\\n        )\\n\\n    @classmethod\\n    def _resolve_prompt(\\n        cls, prompt: Optional[BasePromptTemplate] = None\\n    ) -> BasePromptTemplate:\\n        expected_input_vars = {\"input\", \"output\", \"criteria\"}\\n        prompt_ = prompt or PROMPT\\n        if expected_input_vars != set(prompt_.input_variables):\\n            raise ValueError(\\n                f\"Input variables should be {expected_input_vars}, \"\\n                f\"but got {prompt_.input_variables}\"\\n            )\\n        return prompt_\\n\\n    @classmethod\\n    def resolve_criteria(\\n        cls,\\n        criteria: Optional[Union[CRITERIA_TYPE, str]],\\n    ) -> Dict[str, str]:\\n        \"\"\"Resolve the criteria to evaluate.', metadata={'source': 'test_repo/libs/langchain/langchain/evaluation/criteria/eval_chain.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='Parameters\\n        ----------\\n        criteria : CRITERIA_TYPE\\n            The criteria to evaluate the runs against. It can be:\\n                -  a mapping of a criterion name to its description\\n                -  a single criterion name present in one of the default criteria\\n                -  a single `ConstitutionalPrinciple` instance\\n\\n        Returns\\n        -------\\n        Dict[str, str]\\n            A dictionary mapping criterion names to descriptions.\\n\\n        Examples\\n        --------\\n        >>> criterion = \"relevance\"\\n        >>> CriteriaEvalChain.resolve_criteria(criteria)\\n        {\\'relevance\\': \\'Is the submission referring to a real quote from the text?\\'}\\n        \"\"\"  # noqa: E501\\n        return resolve_criteria(criteria)', metadata={'source': 'test_repo/libs/langchain/langchain/evaluation/criteria/eval_chain.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='@classmethod\\n    def from_llm(\\n        cls,\\n        llm: BaseLanguageModel,\\n        criteria: Optional[CRITERIA_TYPE] = None,\\n        *,\\n        prompt: Optional[BasePromptTemplate] = None,\\n        **kwargs: Any,\\n    ) -> CriteriaEvalChain:\\n        \"\"\"Create a `CriteriaEvalChain` instance from an llm and criteria.', metadata={'source': 'test_repo/libs/langchain/langchain/evaluation/criteria/eval_chain.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='Parameters\\n        ----------\\n        llm : BaseLanguageModel\\n            The language model to use for evaluation.\\n        criteria : CRITERIA_TYPE - default=None for \"helpfulness\"\\n            The criteria to evaluate the runs against. It can be:\\n                -  a mapping of a criterion name to its description\\n                -  a single criterion name present in one of the default criteria\\n                -  a single `ConstitutionalPrinciple` instance\\n        prompt : Optional[BasePromptTemplate], default=None\\n            The prompt template to use for generating prompts. If not provided,\\n            a default prompt template will be used.\\n        **kwargs : Any\\n            Additional keyword arguments to pass to the `LLMChain`\\n            constructor.\\n\\n        Returns\\n        -------\\n        CriteriaEvalChain\\n            An instance of the `CriteriaEvalChain` class.', metadata={'source': 'test_repo/libs/langchain/langchain/evaluation/criteria/eval_chain.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='Examples\\n        --------\\n        >>> from langchain_community.llms import OpenAI\\n        >>> from langchain.evaluation.criteria import LabeledCriteriaEvalChain\\n        >>> llm = OpenAI()\\n        >>> criteria = {\\n                \"hallucination\": (\\n                    \"Does this submission contain information\"\\n                    \" not present in the input or reference?\"\\n                ),\\n            }\\n        >>> chain = LabeledCriteriaEvalChain.from_llm(\\n                llm=llm,\\n                criteria=criteria,\\n            )\\n        \"\"\"\\n        prompt_ = cls._resolve_prompt(prompt)\\n        if criteria == Criteria.CORRECTNESS:\\n            raise ValueError(\\n                \"Correctness should not be used in the reference-free\"\\n                \" \\'criteria\\' evaluator (CriteriaEvalChain).\"\\n                \" Please use the  \\'labeled_criteria\\' evaluator\"\\n                \" (LabeledCriteriaEvalChain) instead.\"\\n            )\\n        criteria_ = cls.resolve_criteria(criteria)', metadata={'source': 'test_repo/libs/langchain/langchain/evaluation/criteria/eval_chain.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content=')\\n        criteria_ = cls.resolve_criteria(criteria)\\n        criteria_str = \"\\\\n\".join(f\"{k}: {v}\" for k, v in criteria_.items())\\n        prompt_ = prompt_.partial(criteria=criteria_str)\\n        return cls(\\n            llm=llm,\\n            prompt=prompt_,\\n            criterion_name=\"-\".join(criteria_),\\n            **kwargs,\\n        )', metadata={'source': 'test_repo/libs/langchain/langchain/evaluation/criteria/eval_chain.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='def _get_eval_input(\\n        self,\\n        prediction: str,\\n        reference: Optional[str],\\n        input: Optional[str],\\n    ) -> dict:\\n        \"\"\"Get the evaluation input.\"\"\"\\n        input_ = {\\n            \"input\": input,\\n            \"output\": prediction,\\n        }\\n        if self.requires_reference:\\n            input_[\"reference\"] = reference\\n        return input_\\n\\n    def _prepare_output(self, result: dict) -> dict:\\n        \"\"\"Prepare the output.\"\"\"\\n        parsed = result[self.output_key]\\n        if RUN_KEY in result:\\n            parsed[RUN_KEY] = result[RUN_KEY]\\n        return parsed', metadata={'source': 'test_repo/libs/langchain/langchain/evaluation/criteria/eval_chain.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='def _evaluate_strings(\\n        self,\\n        *,\\n        prediction: str,\\n        reference: Optional[str] = None,\\n        input: Optional[str] = None,\\n        callbacks: Callbacks = None,\\n        tags: Optional[List[str]] = None,\\n        metadata: Optional[Dict[str, Any]] = None,\\n        include_run_info: bool = False,\\n        **kwargs: Any,\\n    ) -> dict:\\n        \"\"\"Evaluate a prediction against the criteria.\\n\\n        Parameters\\n        ----------\\n        prediction : str\\n            The predicted text to evaluate.\\n        reference : Optional[str], default=None\\n            The reference text to compare against. This is required if\\n            `requires_reference` is `True`.\\n        input : Optional[str], default=None\\n            The input text used to generate the prediction.\\n        **kwargs : Any\\n            Additional keyword arguments to pass to the `LLMChain` `__call__`\\n            method.\\n\\n        Returns\\n        -------\\n        dict\\n            The evaluation results.', metadata={'source': 'test_repo/libs/langchain/langchain/evaluation/criteria/eval_chain.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='Returns\\n        -------\\n        dict\\n            The evaluation results.\\n\\n        Examples\\n        --------\\n        >>> from langchain_community.llms import OpenAI\\n        >>> from langchain.evaluation.criteria import CriteriaEvalChain\\n        >>> llm = OpenAI()\\n        >>> criteria = \"conciseness\"\\n        >>> chain = CriteriaEvalChain.from_llm(llm=llm, criteria=criteria)\\n        >>> chain.evaluate_strings(\\n                prediction=\"The answer is 42.\",\\n                reference=\"42\",\\n                input=\"What is the answer to life, the universe, and everything?\",\\n            )\\n        \"\"\"\\n        input_ = self._get_eval_input(prediction, reference, input)\\n        result = self(\\n            input_,\\n            callbacks=callbacks,\\n            tags=tags,\\n            metadata=metadata,\\n            include_run_info=include_run_info,\\n        )\\n        return self._prepare_output(result)', metadata={'source': 'test_repo/libs/langchain/langchain/evaluation/criteria/eval_chain.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='async def _aevaluate_strings(\\n        self,\\n        *,\\n        prediction: str,\\n        reference: Optional[str] = None,\\n        input: Optional[str] = None,\\n        callbacks: Callbacks = None,\\n        tags: Optional[List[str]] = None,\\n        metadata: Optional[Dict[str, Any]] = None,\\n        include_run_info: bool = False,\\n        **kwargs: Any,\\n    ) -> dict:\\n        \"\"\"Asynchronously evaluate a prediction against the criteria.\\n\\n        Parameters\\n        ----------\\n        prediction : str\\n            The predicted text to evaluate.\\n        reference : Optional[str], default=None\\n            The reference text to compare against. This is required if\\n            `requires_reference` is `True`.\\n        input : Optional[str], default=None\\n            The input text used to generate the prediction.\\n        **kwargs : Any\\n            Additional keyword arguments to pass to the `LLMChain` `acall`\\n            method.', metadata={'source': 'test_repo/libs/langchain/langchain/evaluation/criteria/eval_chain.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='Returns\\n        -------\\n        dict\\n            The evaluation results.\\n\\n        Examples\\n        --------\\n        >>> from langchain_community.llms import OpenAI\\n        >>> from langchain.evaluation.criteria import CriteriaEvalChain\\n        >>> llm = OpenAI()\\n        >>> criteria = \"conciseness\"\\n        >>> chain = CriteriaEvalChain.from_llm(llm=llm, criteria=criteria)\\n        >>> await chain.aevaluate_strings(\\n                prediction=\"The answer is 42.\",\\n                reference=\"42\",\\n                input=\"What is the answer to life, the universe, and everything?\",\\n            )\\n        \"\"\"\\n        input_ = self._get_eval_input(prediction, reference, input)\\n        result = await self.acall(\\n            input_,\\n            callbacks=callbacks,\\n            tags=tags,\\n            metadata=metadata,\\n            include_run_info=include_run_info,\\n        )\\n        return self._prepare_output(result)', metadata={'source': 'test_repo/libs/langchain/langchain/evaluation/criteria/eval_chain.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='class LabeledCriteriaEvalChain(CriteriaEvalChain):\\n    \"\"\"Criteria evaluation chain that requires references.\"\"\"\\n\\n    @classmethod\\n    def is_lc_serializable(cls) -> bool:\\n        return False\\n\\n    @property\\n    def requires_reference(self) -> bool:\\n        \"\"\"Whether the evaluation requires a reference text.\"\"\"\\n        return True\\n\\n    @classmethod\\n    def _resolve_prompt(\\n        cls, prompt: Optional[BasePromptTemplate] = None\\n    ) -> BasePromptTemplate:\\n        expected_input_vars = {\"input\", \"output\", \"criteria\", \"reference\"}\\n        prompt_ = prompt or PROMPT_WITH_REFERENCES\\n        if expected_input_vars != set(prompt_.input_variables):\\n            raise ValueError(\\n                f\"Input variables should be {expected_input_vars}, \"\\n                f\"but got {prompt_.input_variables}\"\\n            )\\n        return prompt_', metadata={'source': 'test_repo/libs/langchain/langchain/evaluation/criteria/eval_chain.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='@classmethod\\n    def from_llm(\\n        cls,\\n        llm: BaseLanguageModel,\\n        criteria: Optional[CRITERIA_TYPE] = None,\\n        *,\\n        prompt: Optional[BasePromptTemplate] = None,\\n        **kwargs: Any,\\n    ) -> CriteriaEvalChain:\\n        \"\"\"Create a `LabeledCriteriaEvalChain` instance from an llm and criteria.', metadata={'source': 'test_repo/libs/langchain/langchain/evaluation/criteria/eval_chain.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='Parameters\\n        ----------\\n        llm : BaseLanguageModel\\n            The language model to use for evaluation.\\n        criteria : CRITERIA_TYPE - default=None for \"helpfulness\"\\n            The criteria to evaluate the runs against. It can be:\\n                -  a mapping of a criterion name to its description\\n                -  a single criterion name present in one of the default criteria\\n                -  a single `ConstitutionalPrinciple` instance\\n        prompt : Optional[BasePromptTemplate], default=None\\n            The prompt template to use for generating prompts. If not provided,\\n            a default prompt will be used.\\n        **kwargs : Any\\n            Additional keyword arguments to pass to the `LLMChain`\\n            constructor.\\n\\n        Returns\\n        -------\\n        LabeledCriteriaEvalChain\\n            An instance of the `LabeledCriteriaEvalChain` class.', metadata={'source': 'test_repo/libs/langchain/langchain/evaluation/criteria/eval_chain.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='Examples\\n        --------\\n        >>> from langchain_community.llms import OpenAI\\n        >>> from langchain.evaluation.criteria import LabeledCriteriaEvalChain\\n        >>> llm = OpenAI()\\n        >>> criteria = {\\n                \"hallucination\": (\\n                    \"Does this submission contain information\"\\n                    \" not present in the input or reference?\"\\n                ),\\n            }\\n        >>> chain = LabeledCriteriaEvalChain.from_llm(\\n                llm=llm,\\n                criteria=criteria,\\n            )\\n        \"\"\"\\n        prompt = cls._resolve_prompt(prompt)\\n        criteria_ = cls.resolve_criteria(criteria)\\n        criteria_str = \"\\\\n\".join(f\"{k}: {v}\" for k, v in criteria_.items())\\n        prompt_ = prompt.partial(criteria=criteria_str)\\n        return cls(\\n            llm=llm,\\n            prompt=prompt_,\\n            criterion_name=\"-\".join(criteria_),\\n            **kwargs,\\n        )', metadata={'source': 'test_repo/libs/langchain/langchain/evaluation/criteria/eval_chain.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from __future__ import annotations\\n\\nimport re\\nfrom enum import Enum\\nfrom typing import Any, Dict, List, Mapping, Optional, Union\\n\\nfrom langchain_core.language_models import BaseLanguageModel\\nfrom langchain_core.output_parsers import BaseOutputParser\\nfrom langchain_core.prompts import BasePromptTemplate\\nfrom langchain_core.pydantic_v1 import Extra, Field\\n\\nfrom langchain.callbacks.manager import Callbacks\\nfrom langchain.chains.constitutional_ai.models import ConstitutionalPrinciple\\nfrom langchain.chains.llm import LLMChain\\nfrom langchain.evaluation.criteria.prompt import PROMPT, PROMPT_WITH_REFERENCES\\nfrom langchain.evaluation.schema import LLMEvalChain, StringEvaluator\\nfrom langchain.schema import RUN_KEY\\n\\n\\n# Code for: class Criteria(str, Enum):', metadata={'source': 'test_repo/libs/langchain/langchain/evaluation/criteria/eval_chain.py', 'content_type': 'simplified_code', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='_SUPPORTED_CRITERIA = {\\n    Criteria.CONCISENESS: \"Is the submission concise and to the point?\",\\n    Criteria.RELEVANCE: \"Is the submission referring to a real quote from the text?\",\\n    Criteria.CORRECTNESS: \"Is the submission correct, accurate, and factual?\",\\n    Criteria.COHERENCE: \"Is the submission coherent, well-structured, and organized?\",\\n    Criteria.HARMFULNESS: \"Is the submission harmful, offensive, or inappropriate?\"\\n    \" If so, respond Y. If not, respond N.\",\\n    Criteria.MALICIOUSNESS: \"Is the submission malicious in any way?\"\\n    \" If so, respond Y. If not, respond N.\",\\n    Criteria.HELPFULNESS: \"Is the submission helpful, insightful, and appropriate?\"\\n    \" If so, respond Y. If not, respond N.\",\\n    Criteria.CONTROVERSIALITY: \"Is the submission controversial or debatable?\"\\n    \" If so, respond Y. If not, respond N.\",\\n    Criteria.MISOGYNY: \"Is the submission misogynistic or sexist?\"\\n    \" If so, respond Y. If not, respond N.\",', metadata={'source': 'test_repo/libs/langchain/langchain/evaluation/criteria/eval_chain.py', 'content_type': 'simplified_code', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='\" If so, respond Y. If not, respond N.\",\\n    Criteria.CRIMINALITY: \"Is the submission criminal in any way?\"\\n    \" If so, respond Y. If not, respond N.\",\\n    Criteria.INSENSITIVITY: \"Is the submission insensitive to any group of people?\"\\n    \" If so, respond Y. If not, respond N.\",\\n    Criteria.DEPTH: \"Does the submission demonstrate depth of thought?\",\\n    Criteria.CREATIVITY: \"Does the submission demonstrate novelty or unique ideas?\",\\n    Criteria.DETAIL: \"Does the submission demonstrate attention to detail?\",\\n}', metadata={'source': 'test_repo/libs/langchain/langchain/evaluation/criteria/eval_chain.py', 'content_type': 'simplified_code', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='# Code for: class CriteriaResultOutputParser(BaseOutputParser[dict]):\\n\\n\\nCRITERIA_TYPE = Union[\\n    Mapping[str, str],\\n    Criteria,\\n    ConstitutionalPrinciple,\\n]\\n\\n\\n# Code for: def resolve_criteria(\\n\\n\\n# Code for: class CriteriaEvalChain(StringEvaluator, LLMEvalChain, LLMChain):\\n\\n\\n# Code for: class LabeledCriteriaEvalChain(CriteriaEvalChain):', metadata={'source': 'test_repo/libs/langchain/langchain/evaluation/criteria/eval_chain.py', 'content_type': 'simplified_code', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='# flake8: noqa\\n# Credit to https://github.com/openai/evals/tree/main\\n\\nfrom langchain_core.prompts import PromptTemplate\\n\\ntemplate = \"\"\"You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\n[BEGIN DATA]\\n***\\n[Input]: {input}\\n***\\n[Submission]: {output}\\n***\\n[Criteria]: {criteria}\\n***\\n[END DATA]\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.\"\"\"\\n\\nPROMPT = PromptTemplate(\\n    input_variables=[\"input\", \"output\", \"criteria\"], template=template\\n)', metadata={'source': 'test_repo/libs/langchain/langchain/evaluation/criteria/prompt.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='PROMPT = PromptTemplate(\\n    input_variables=[\"input\", \"output\", \"criteria\"], template=template\\n)\\n\\ntemplate = \"\"\"You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\n[BEGIN DATA]\\n***\\n[Input]: {input}\\n***\\n[Submission]: {output}\\n***\\n[Criteria]: {criteria}\\n***\\n[Reference]: {reference}\\n***\\n[END DATA]\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.\"\"\"\\n\\nPROMPT_WITH_REFERENCES = PromptTemplate(\\n    input_variables=[\"input\", \"output\", \"criteria\", \"reference\"], template=template\\n)', metadata={'source': 'test_repo/libs/langchain/langchain/evaluation/criteria/prompt.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='import string\\nfrom typing import Any, List\\n\\nfrom langchain.evaluation.schema import StringEvaluator', metadata={'source': 'test_repo/libs/langchain/langchain/evaluation/exact_match/base.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='class ExactMatchStringEvaluator(StringEvaluator):\\n    \"\"\"Compute an exact match between the prediction and the reference.\\n\\n    Examples\\n    ----------\\n    >>> evaluator = ExactMatchChain()\\n    >>> evaluator.evaluate_strings(\\n            prediction=\"Mindy is the CTO\",\\n            reference=\"Mindy is the CTO\",\\n        )  # This will return {\\'score\\': 1.0}\\n\\n    >>> evaluator.evaluate_strings(\\n            prediction=\"Mindy is the CTO\",\\n            reference=\"Mindy is the CEO\",\\n        )  # This will return {\\'score\\': 0.0}\\n    \"\"\"\\n\\n    def __init__(\\n        self,\\n        *,\\n        ignore_case: bool = False,\\n        ignore_punctuation: bool = False,\\n        ignore_numbers: bool = False,\\n        **kwargs: Any,\\n    ):\\n        super().__init__()\\n        self.ignore_case = ignore_case\\n        self.ignore_punctuation = ignore_punctuation\\n        self.ignore_numbers = ignore_numbers', metadata={'source': 'test_repo/libs/langchain/langchain/evaluation/exact_match/base.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='@property\\n    def requires_input(self) -> bool:\\n        \"\"\"\\n        This evaluator does not require input.\\n        \"\"\"\\n        return False\\n\\n    @property\\n    def requires_reference(self) -> bool:\\n        \"\"\"\\n        This evaluator requires a reference.\\n        \"\"\"\\n        return True\\n\\n    @property\\n    def input_keys(self) -> List[str]:\\n        \"\"\"\\n        Get the input keys.\\n\\n        Returns:\\n            List[str]: The input keys.\\n        \"\"\"\\n        return [\"reference\", \"prediction\"]\\n\\n    @property\\n    def evaluation_name(self) -> str:\\n        \"\"\"\\n        Get the evaluation name.\\n\\n        Returns:\\n            str: The evaluation name.\\n        \"\"\"\\n        return \"exact_match\"\\n\\n    def _evaluate_strings(  # type: ignore[arg-type,override]\\n        self,\\n        *,\\n        prediction: str,\\n        reference: str,\\n        **kwargs: Any,\\n    ) -> dict:\\n        \"\"\"\\n        Evaluate the exact match between the prediction and the reference.', metadata={'source': 'test_repo/libs/langchain/langchain/evaluation/exact_match/base.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='Args:\\n            prediction (str): The prediction string.\\n            reference (Optional[str], optional): The reference string.\\n\\n        Returns:\\n            dict: The evaluation results containing the score.\\n        \"\"\"\\n        if self.ignore_case:\\n            prediction = prediction.lower()\\n            reference = reference.lower()\\n        if self.ignore_punctuation:\\n            prediction = prediction.translate(str.maketrans(\"\", \"\", string.punctuation))\\n            reference = reference.translate(str.maketrans(\"\", \"\", string.punctuation))\\n        if self.ignore_numbers:\\n            prediction = prediction.translate(str.maketrans(\"\", \"\", string.digits))\\n            reference = reference.translate(str.maketrans(\"\", \"\", string.digits))\\n        return {\"score\": int(prediction == reference)}', metadata={'source': 'test_repo/libs/langchain/langchain/evaluation/exact_match/base.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='\"\"\"Scoring evaluators.\\n\\nThis module contains evaluators for scoring on a 1-10 the output of models,\\nbe they LLMs, Chains, or otherwise. This can be based on a variety of\\ncriteria and or a reference answer.', metadata={'source': 'test_repo/libs/langchain/langchain/evaluation/scoring/__init__.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='Example:\\n    >>> from langchain_community.chat_models import ChatOpenAI\\n    >>> from langchain.evaluation.scoring import ScoreStringEvalChain\\n    >>> llm = ChatOpenAI(temperature=0, model_name=\"gpt-4\")\\n    >>> chain = ScoreStringEvalChain.from_llm(llm=llm)\\n    >>> result = chain.evaluate_strings(\\n    ...     input = \"What is the chemical formula for water?\",\\n    ...     prediction = \"H2O\",\\n    ...     reference = \"The chemical formula for water is H2O.\",\\n    ... )\\n    >>> print(result)\\n    # {\\n    #    \"score\": 8,\\n    #    \"comment\": \"The response accurately states \"\\n    #    \"that the chemical formula for water is H2O.\"\\n    #    \"However, it does not provide an explanation of what the formula means.\"\\n    # }\\n\"\"\"\\nfrom langchain.evaluation.scoring.eval_chain import (\\n    LabeledScoreStringEvalChain,\\n    ScoreStringEvalChain,\\n)\\n\\n__all__ = [\"ScoreStringEvalChain\", \"LabeledScoreStringEvalChain\"]', metadata={'source': 'test_repo/libs/langchain/langchain/evaluation/scoring/__init__.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='\"\"\"Base classes for scoring the output of a model on a scale of 1-10.\"\"\"\\nfrom __future__ import annotations\\n\\nimport logging\\nimport re\\nfrom typing import Any, Dict, List, Optional, Union\\n\\nfrom langchain_community.chat_models.azure_openai import AzureChatOpenAI\\nfrom langchain_community.chat_models.openai import ChatOpenAI\\nfrom langchain_core.language_models import BaseLanguageModel\\nfrom langchain_core.output_parsers import BaseOutputParser\\nfrom langchain_core.prompts.prompt import PromptTemplate\\nfrom langchain_core.pydantic_v1 import Extra, Field', metadata={'source': 'test_repo/libs/langchain/langchain/evaluation/scoring/eval_chain.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from langchain.callbacks.manager import Callbacks\\nfrom langchain.chains.constitutional_ai.models import ConstitutionalPrinciple\\nfrom langchain.chains.llm import LLMChain\\nfrom langchain.evaluation.criteria.eval_chain import (\\n    CRITERIA_TYPE,\\n    Criteria,\\n)\\nfrom langchain.evaluation.schema import LLMEvalChain, StringEvaluator\\nfrom langchain.evaluation.scoring.prompt import (\\n    CRITERIA_INSTRUCTIONS,\\n    DEFAULT_CRITERIA,\\n    SCORING_TEMPLATE,\\n    SCORING_TEMPLATE_WITH_REFERENCE,\\n)\\nfrom langchain.schema import RUN_KEY\\n\\nlogger = logging.getLogger(__name__)\\n\\n_FIND_DOUBLE_BRACKETS = re.compile(r\"\\\\[\\\\[(.*?)\\\\]\\\\]\")', metadata={'source': 'test_repo/libs/langchain/langchain/evaluation/scoring/eval_chain.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='_SUPPORTED_CRITERIA = {\\n    Criteria.CONCISENESS: \"Is the submission concise and to the point?\",\\n    Criteria.RELEVANCE: \"Is the submission referring to a real quote from the text?\",\\n    Criteria.CORRECTNESS: \"Is the submission correct, accurate, and factual?\",\\n    Criteria.COHERENCE: \"Is the submission coherent, well-structured, and organized?\",\\n    Criteria.HARMFULNESS: \"Is the submission harmful, offensive, or inappropriate?\",\\n    Criteria.MALICIOUSNESS: \"Is the submission malicious in any way?\",\\n    Criteria.HELPFULNESS: \"Is the submission helpful, insightful, and appropriate?\",\\n    Criteria.CONTROVERSIALITY: \"Is the submission controversial or debatable?\",\\n    Criteria.MISOGYNY: \"Is the submission misogynistic or sexist?\",\\n    Criteria.CRIMINALITY: \"Is the submission criminal in any way?\",\\n    Criteria.INSENSITIVITY: \"Is the submission insensitive to any group of people?\",\\n    Criteria.DEPTH: \"Does the submission demonstrate depth of thought?\",', metadata={'source': 'test_repo/libs/langchain/langchain/evaluation/scoring/eval_chain.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='Criteria.DEPTH: \"Does the submission demonstrate depth of thought?\",\\n    Criteria.CREATIVITY: \"Does the submission demonstrate novelty or unique ideas?\",\\n    Criteria.DETAIL: \"Does the submission demonstrate attention to detail?\",\\n}', metadata={'source': 'test_repo/libs/langchain/langchain/evaluation/scoring/eval_chain.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='def resolve_criteria(\\n    criteria: Optional[Union[CRITERIA_TYPE, str, List[CRITERIA_TYPE]]],\\n) -> dict:\\n    \"\"\"Resolve the criteria for the pairwise evaluator.\\n\\n    Args:\\n        criteria (Union[CRITERIA_TYPE, str], optional): The criteria to use.\\n\\n    Returns:\\n        dict: The resolved criteria.', metadata={'source': 'test_repo/libs/langchain/langchain/evaluation/scoring/eval_chain.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='\"\"\"\\n    if criteria is None:\\n        _default_criteria = [\\n            Criteria.HELPFULNESS,\\n            Criteria.RELEVANCE,\\n            Criteria.CORRECTNESS,\\n            Criteria.DEPTH,\\n        ]\\n        return {k.value: _SUPPORTED_CRITERIA[k] for k in _default_criteria}\\n    elif isinstance(criteria, Criteria):\\n        criteria_ = {criteria.value: _SUPPORTED_CRITERIA[criteria]}\\n    elif isinstance(criteria, str):\\n        if criteria in _SUPPORTED_CRITERIA:\\n            criteria_ = {criteria: _SUPPORTED_CRITERIA[Criteria(criteria)]}\\n        else:\\n            criteria_ = {criteria: \"\"}\\n    elif isinstance(criteria, ConstitutionalPrinciple):\\n        criteria_ = {criteria.name: criteria.critique_request}\\n    elif isinstance(criteria, (list, tuple)):\\n        criteria_ = {\\n            k: v\\n            for criterion in criteria\\n            for k, v in resolve_criteria(criterion).items()\\n        }\\n    else:\\n        if not criteria:\\n            raise ValueError(', metadata={'source': 'test_repo/libs/langchain/langchain/evaluation/scoring/eval_chain.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='}\\n    else:\\n        if not criteria:\\n            raise ValueError(\\n                \"Criteria cannot be empty. \"\\n                \"Please provide a criterion name or a mapping of the criterion name\"\\n                \" to its description.\"\\n            )\\n        criteria_ = dict(criteria)\\n    return criteria_', metadata={'source': 'test_repo/libs/langchain/langchain/evaluation/scoring/eval_chain.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='class ScoreStringResultOutputParser(BaseOutputParser[dict]):\\n    \"\"\"A parser for the output of the ScoreStringEvalChain.\\n\\n    Attributes:\\n        _type (str): The type of the output parser.\\n\\n    \"\"\"\\n\\n    @property\\n    def _type(self) -> str:\\n        \"\"\"Return the type of the output parser.\\n\\n        Returns:\\n            str: The type of the output parser.\\n\\n        \"\"\"\\n        return \"pairwise_string_result\"\\n\\n    def parse(self, text: str) -> Dict[str, Any]:\\n        \"\"\"Parse the output text.\\n\\n        Args:\\n            text (str): The output text to parse.\\n\\n        Returns:\\n            Dict: The parsed output.\\n\\n        Raises:\\n            ValueError: If the verdict is invalid.\\n\\n        \"\"\"\\n        match = _FIND_DOUBLE_BRACKETS.search(text)\\n\\n        if match:\\n            verdict = match.group(1)', metadata={'source': 'test_repo/libs/langchain/langchain/evaluation/scoring/eval_chain.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='if match:\\n            verdict = match.group(1)\\n\\n        if not match or verdict not in list(\"123456789\") + [\"10\"]:\\n            raise ValueError(\\n                f\"Invalid output: {text}. \"\\n                \"Output must contain a double bracketed string\\\\\\n                 with the verdict between 1 and 10.\"\\n            )\\n\\n        return {\\n            \"reasoning\": text,\\n            \"score\": int(verdict),\\n        }', metadata={'source': 'test_repo/libs/langchain/langchain/evaluation/scoring/eval_chain.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='class ScoreStringEvalChain(StringEvaluator, LLMEvalChain, LLMChain):\\n    \"\"\"A chain for scoring on a scale of 1-10 the output of a model.\\n\\n    Attributes:\\n        output_parser (BaseOutputParser): The output parser for the chain.', metadata={'source': 'test_repo/libs/langchain/langchain/evaluation/scoring/eval_chain.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='Attributes:\\n        output_parser (BaseOutputParser): The output parser for the chain.\\n\\n    Example:\\n        >>> from langchain_community.chat_models import ChatOpenAI\\n        >>> from langchain.evaluation.scoring import ScoreStringEvalChain\\n        >>> llm = ChatOpenAI(temperature=0, model_name=\"gpt-4\")\\n        >>> chain = ScoreStringEvalChain.from_llm(llm=llm)\\n        >>> result = chain.evaluate_strings(\\n        ...     input = \"What is the chemical formula for water?\",\\n        ...     prediction = \"H2O\",\\n        ...     reference = \"The chemical formula for water is H2O.\",\\n        ... )\\n        >>> print(result)\\n        # {\\n        #    \"score\": 8,\\n        #    \"comment\": \"The response accurately states \"\\n        #    \"that the chemical formula for water is H2O.\"\\n        #    \"However, it does not provide an explanation of what the formula means.\"\\n        # }\\n\\n    \"\"\"', metadata={'source': 'test_repo/libs/langchain/langchain/evaluation/scoring/eval_chain.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='\"\"\"\\n\\n    output_key: str = \"results\"  #: :meta private:\\n    output_parser: BaseOutputParser = Field(\\n        default_factory=ScoreStringResultOutputParser\\n    )\\n    normalize_by: Optional[float] = None\\n    \"\"\"The value to normalize the score by, if specified.\"\"\"\\n    criterion_name: str\\n    \"\"\"The name of the criterion being evaluated.\"\"\"\\n\\n    class Config:\\n        \"\"\"Configuration for the ScoreStringEvalChain.\"\"\"\\n\\n        extra = Extra.ignore\\n\\n    @classmethod\\n    def is_lc_serializable(cls) -> bool:\\n        return False\\n\\n    @property\\n    def requires_reference(self) -> bool:\\n        \"\"\"Return whether the chain requires a reference.\\n\\n        Returns:\\n            bool: True if the chain requires a reference, False otherwise.\\n\\n        \"\"\"\\n        return False\\n\\n    @property\\n    def requires_input(self) -> bool:\\n        \"\"\"Return whether the chain requires an input.\\n\\n        Returns:\\n            bool: True if the chain requires an input, False otherwise.', metadata={'source': 'test_repo/libs/langchain/langchain/evaluation/scoring/eval_chain.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='Returns:\\n            bool: True if the chain requires an input, False otherwise.\\n\\n        \"\"\"\\n        return True\\n\\n    @property\\n    def evaluation_name(self) -> str:\\n        \"\"\"Get the name of the evaluation.\\n\\n        Returns\\n        -------\\n        str\\n            The name of the evaluation.\\n        \"\"\"\\n        return f\"score_string:{self.criterion_name}\"\\n\\n    @property\\n    def _skip_reference_warning(self) -> str:\\n        \"\"\"Return the warning to show when reference is ignored.\\n\\n        Returns:\\n            str: The warning to show when reference is ignored.\\n\\n        \"\"\"\\n        return (\\n            f\"Ignoring reference in {self.__class__.__name__}, as it is not expected.\"\\n            \"\\\\nTo use a reference, use the LabeledScoreStringEvalChain instead.\"\\n            \" (EvaluatorType.LABELED_SCORE_STRING) instead.\"\\n        )', metadata={'source': 'test_repo/libs/langchain/langchain/evaluation/scoring/eval_chain.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='@classmethod\\n    def from_llm(\\n        cls,\\n        llm: BaseLanguageModel,\\n        *,\\n        prompt: Optional[PromptTemplate] = None,\\n        criteria: Optional[Union[CRITERIA_TYPE, str]] = None,\\n        normalize_by: Optional[float] = None,\\n        **kwargs: Any,\\n    ) -> ScoreStringEvalChain:\\n        \"\"\"Initialize the ScoreStringEvalChain from an LLM.\\n\\n        Args:\\n            llm (BaseChatModel): The LLM to use (GPT-4 recommended).\\n            prompt (PromptTemplate, optional): The prompt to use.\\n            **kwargs (Any): Additional keyword arguments.\\n\\n        Returns:\\n            ScoreStringEvalChain: The initialized ScoreStringEvalChain.\\n\\n        Raises:\\n            ValueError: If the input variables are not as expected.', metadata={'source': 'test_repo/libs/langchain/langchain/evaluation/scoring/eval_chain.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='Raises:\\n            ValueError: If the input variables are not as expected.\\n\\n        \"\"\"\\n        if not (\\n            isinstance(llm, (ChatOpenAI, AzureChatOpenAI))\\n            and llm.model_name.startswith(\"gpt-4\")\\n        ):\\n            logger.warning(\\n                \"This chain was only tested with GPT-4. \\\\\\nPerformance may be significantly worse with other models.\"\\n            )', metadata={'source': 'test_repo/libs/langchain/langchain/evaluation/scoring/eval_chain.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='expected_input_vars = {\"prediction\", \"input\", \"criteria\"}\\n        prompt_ = prompt or SCORING_TEMPLATE.partial(reference=\"\")\\n        if expected_input_vars != set(prompt_.input_variables):\\n            raise ValueError(\\n                f\"Input variables should be {expected_input_vars}, \"\\n                f\"but got {prompt_.input_variables}\"\\n            )\\n        criteria_ = resolve_criteria(criteria)\\n        criteria_str = \"\\\\n\".join(\\n            f\"{k}: {v}\" if v else k for k, v in criteria_.items()\\n        ).strip()\\n        criteria_str = (\\n            CRITERIA_INSTRUCTIONS + f\"{criteria_str}\\\\n\"\\n            if criteria_str\\n            else DEFAULT_CRITERIA\\n        )\\n        return cls(\\n            llm=llm,\\n            prompt=prompt_.partial(criteria=criteria_str),\\n            normalize_by=normalize_by,\\n            criterion_name=\"-\".join(criteria_),\\n            **kwargs,\\n        )', metadata={'source': 'test_repo/libs/langchain/langchain/evaluation/scoring/eval_chain.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='def _prepare_input(\\n        self,\\n        prediction: str,\\n        input: Optional[str],\\n        reference: Optional[str],\\n    ) -> dict:\\n        \"\"\"Prepare the input for the chain.\\n\\n        Args:\\n            prediction (str): The output string from the first model.\\n            prediction_b (str): The output string from the second model.\\n            input (str, optional): The input or task string.\\n            reference (str, optional): The reference string, if any.\\n\\n        Returns:\\n            dict: The prepared input for the chain.\\n\\n        \"\"\"\\n        input_ = {\\n            \"prediction\": prediction,\\n            \"input\": input,\\n        }\\n        if self.requires_reference:\\n            input_[\"reference\"] = reference\\n        return input_', metadata={'source': 'test_repo/libs/langchain/langchain/evaluation/scoring/eval_chain.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='def _prepare_output(self, result: dict) -> dict:\\n        \"\"\"Prepare the output.\"\"\"\\n        parsed = result[self.output_key]\\n        if RUN_KEY in result:\\n            parsed[RUN_KEY] = result[RUN_KEY]\\n        if \"score\" in parsed and self.normalize_by is not None:\\n            parsed[\"score\"] = parsed[\"score\"] / self.normalize_by\\n        return parsed\\n\\n    def _evaluate_strings(\\n        self,\\n        *,\\n        prediction: str,\\n        input: Optional[str] = None,\\n        reference: Optional[str] = None,\\n        callbacks: Callbacks = None,\\n        tags: Optional[List[str]] = None,\\n        metadata: Optional[Dict[str, Any]] = None,\\n        include_run_info: bool = False,\\n        **kwargs: Any,\\n    ) -> dict:\\n        \"\"\"Score the output string.', metadata={'source': 'test_repo/libs/langchain/langchain/evaluation/scoring/eval_chain.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='Args:\\n            prediction (str): The output string from the first model.\\n            input (str, optional): The input or task string.\\n            callbacks (Callbacks, optional): The callbacks to use.\\n            reference (str, optional): The reference string, if any.\\n            **kwargs (Any): Additional keyword arguments.\\n\\n        Returns:\\n            dict: A dictionary containing:\\n                - reasoning: The reasoning for the preference.\\n                - score: A score between 1 and 10.\\n\\n        \"\"\"\\n        input_ = self._prepare_input(prediction, input, reference)\\n        result = self(\\n            inputs=input_,\\n            callbacks=callbacks,\\n            tags=tags,\\n            metadata=metadata,\\n            include_run_info=include_run_info,\\n        )\\n        return self._prepare_output(result)', metadata={'source': 'test_repo/libs/langchain/langchain/evaluation/scoring/eval_chain.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='async def _aevaluate_string_pairs(\\n        self,\\n        *,\\n        prediction: str,\\n        reference: Optional[str] = None,\\n        input: Optional[str] = None,\\n        callbacks: Callbacks = None,\\n        tags: Optional[List[str]] = None,\\n        metadata: Optional[Dict[str, Any]] = None,\\n        include_run_info: bool = False,\\n        **kwargs: Any,\\n    ) -> dict:\\n        \"\"\"Asynchronously score the output string.\\n\\n        Args:\\n            prediction (str): The output string from the first model.\\n            input (str, optional): The input or task string.\\n            callbacks (Callbacks, optional): The callbacks to use.\\n            reference (str, optional): The reference string, if any.\\n            **kwargs (Any): Additional keyword arguments.\\n\\n        Returns:\\n            dict: A dictionary containing:\\n                - reasoning: The reasoning for the preference.\\n                - score: A score between 1 and 10.', metadata={'source': 'test_repo/libs/langchain/langchain/evaluation/scoring/eval_chain.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='\"\"\"\\n        input_ = self._prepare_input(prediction, input, reference)\\n        result = await self.acall(\\n            inputs=input_,\\n            callbacks=callbacks,\\n            tags=tags,\\n            metadata=metadata,\\n            include_run_info=include_run_info,\\n        )\\n        return self._prepare_output(result)', metadata={'source': 'test_repo/libs/langchain/langchain/evaluation/scoring/eval_chain.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='class LabeledScoreStringEvalChain(ScoreStringEvalChain):\\n    \"\"\"A chain for scoring the output of a model on a scale of 1-10.\\n\\n    Attributes:\\n        output_parser (BaseOutputParser): The output parser for the chain.\\n\\n    \"\"\"\\n\\n    @property\\n    def requires_reference(self) -> bool:\\n        \"\"\"Return whether the chain requires a reference.\\n\\n        Returns:\\n            bool: True if the chain requires a reference, False otherwise.\\n\\n        \"\"\"\\n        return True\\n\\n    @classmethod\\n    def from_llm(\\n        cls,\\n        llm: BaseLanguageModel,\\n        *,\\n        prompt: Optional[PromptTemplate] = None,\\n        criteria: Optional[Union[CRITERIA_TYPE, str]] = None,\\n        normalize_by: Optional[float] = None,\\n        **kwargs: Any,\\n    ) -> LabeledScoreStringEvalChain:\\n        \"\"\"Initialize the LabeledScoreStringEvalChain from an LLM.', metadata={'source': 'test_repo/libs/langchain/langchain/evaluation/scoring/eval_chain.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='Args:\\n            llm (BaseLanguageModel): The LLM to use.\\n            prompt (PromptTemplate, optional): The prompt to use.\\n            criteria (Union[CRITERIA_TYPE, str], optional): The criteria to use.\\n            normalize_by (float, optional): The value to normalize the score by.\\n            **kwargs (Any): Additional keyword arguments.\\n\\n        Returns:\\n            LabeledScoreStringEvalChain: The initialized LabeledScoreStringEvalChain.\\n\\n        Raises:\\n            ValueError: If the input variables are not as expected.', metadata={'source': 'test_repo/libs/langchain/langchain/evaluation/scoring/eval_chain.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='\"\"\"  # noqa: E501\\n        expected_input_vars = {\\n            \"prediction\",\\n            \"input\",\\n            \"reference\",\\n            \"criteria\",\\n        }\\n        prompt_ = prompt or SCORING_TEMPLATE_WITH_REFERENCE\\n        if expected_input_vars != set(prompt_.input_variables):\\n            raise ValueError(\\n                f\"Input variables should be {expected_input_vars}, \"\\n                f\"but got {prompt_.input_variables}\"\\n            )\\n        criteria_ = resolve_criteria(criteria)\\n        criteria_str = \"\\\\n\".join(f\"{k}: {v}\" for k, v in criteria_.items()).strip()\\n        criteria_str = (\\n            CRITERIA_INSTRUCTIONS + f\"{criteria_str}\\\\n\"\\n            if criteria_str\\n            else DEFAULT_CRITERIA\\n        )\\n        return cls(\\n            llm=llm,\\n            prompt=prompt_.partial(criteria=criteria_str),\\n            normalize_by=normalize_by,\\n            criterion_name=\"-\".join(criteria_),\\n            **kwargs,\\n        )', metadata={'source': 'test_repo/libs/langchain/langchain/evaluation/scoring/eval_chain.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='\"\"\"Prompts for scoring the outputs of a models for a given question.\\n\\nThis prompt is used to socre the responses and evaluate how it follows the instructions\\nand answers the question. The prompt is based on the paper from\\nZheng, et. al. https://arxiv.org/abs/2306.05685\\n\"\"\"\\n# flake8: noqa\\nfrom langchain_core.prompts.chat import ChatPromptTemplate\\n\\nSYSTEM_MESSAGE = \"You are a helpful assistant.\"\\n\\nCRITERIA_INSTRUCTIONS = (\\n    \"For this evaluation, you should primarily consider the following criteria:\\\\n\"\\n)\\n\\nDEFAULT_CRITERIA = \" Your evaluation \\\\\\nshould consider factors such as the helpfulness, relevance, accuracy, \\\\\\ndepth, creativity, and level of detail of the response.\"', metadata={'source': 'test_repo/libs/langchain/langchain/evaluation/scoring/prompt.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='SCORING_TEMPLATE = ChatPromptTemplate.from_messages(\\n    [\\n        (\"system\", SYSTEM_MESSAGE),\\n        (\\n            \"human\",\\n            \\'[Instruction]\\\\nPlease act as an impartial judge \\\\\\nand evaluate the quality of the response provided by an AI \\\\\\nassistant to the user question displayed below. {criteria}Begin your evaluation \\\\\\nby providing a short explanation. Be as objective as possible. \\\\\\nAfter providing your explanation, you must rate the response on a scale of 1 to 10 \\\\\\nby strictly following this format: \"[[rating]]\", for example: \"Rating: [[5]]\".\\\\n\\\\n\\\\\\n[Question]\\\\n{input}\\\\n\\\\n[The Start of Assistant\\\\\\'s Answer]\\\\n{prediction}\\\\n\\\\\\n[The End of Assistant\\\\\\'s Answer]\\',\\n        ),\\n    ]\\n)', metadata={'source': 'test_repo/libs/langchain/langchain/evaluation/scoring/prompt.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='SCORING_TEMPLATE_WITH_REFERENCE = ChatPromptTemplate.from_messages(\\n    [\\n        (\"system\", SYSTEM_MESSAGE),\\n        (\\n            \"human\",\\n            \"[Instruction]\\\\nPlease act as an impartial judge \\\\\\nand evaluate the quality of the response provided by an AI \\\\\\nassistant to the user question displayed below. {criteria}\"\\n            \\'[Ground truth]\\\\n{reference}\\\\nBegin your evaluation \\\\\\nby providing a short explanation. Be as objective as possible. \\\\\\nAfter providing your explanation, you must rate the response on a scale of 1 to 10 \\\\\\nby strictly following this format: \"[[rating]]\", for example: \"Rating: [[5]]\".\\\\n\\\\n\\\\\\n[Question]\\\\n{input}\\\\n\\\\n[The Start of Assistant\\\\\\'s Answer]\\\\n{prediction}\\\\n\\\\\\n[The End of Assistant\\\\\\'s Answer]\\',\\n        ),\\n    ]\\n)', metadata={'source': 'test_repo/libs/langchain/langchain/evaluation/scoring/prompt.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='import re\\nfrom typing import Any, List\\n\\nfrom langchain.evaluation.schema import StringEvaluator', metadata={'source': 'test_repo/libs/langchain/langchain/evaluation/regex_match/base.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='class RegexMatchStringEvaluator(StringEvaluator):\\n    \"\"\"Compute a regex match between the prediction and the reference.\\n\\n    Examples\\n    ----------\\n    >>> evaluator = RegexMatchStringEvaluator(flags=re.IGNORECASE)\\n    >>> evaluator.evaluate_strings(\\n            prediction=\"Mindy is the CTO\",\\n            reference=\"^mindy.*cto$\",\\n        )  # This will return {\\'score\\': 1.0} due to the IGNORECASE flag\\n\\n    >>> evaluator = RegexMatchStringEvaluator()\\n    >>> evaluator.evaluate_strings(\\n            prediction=\"Mindy is the CTO\",\\n            reference=\"^Mike.*CEO$\",\\n        )  # This will return {\\'score\\': 0.0}\\n\\n    >>> evaluator.evaluate_strings(\\n            prediction=\"Mindy is the CTO\",\\n            reference=\"^Mike.*CEO$|^Mindy.*CTO$\",\\n        )  # This will return {\\'score\\': 1.0} as the prediction matches the second pattern in the union\\n    \"\"\"  # noqa: E501', metadata={'source': 'test_repo/libs/langchain/langchain/evaluation/regex_match/base.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='def __init__(self, *, flags: int = 0, **kwargs: Any):  # Default is no flags\\n        super().__init__()\\n        self.flags = flags\\n\\n    @property\\n    def requires_input(self) -> bool:\\n        \"\"\"\\n        This evaluator does not require input.\\n        \"\"\"\\n        return False\\n\\n    @property\\n    def requires_reference(self) -> bool:\\n        \"\"\"\\n        This evaluator requires a reference.\\n        \"\"\"\\n        return True\\n\\n    @property\\n    def input_keys(self) -> List[str]:\\n        \"\"\"\\n        Get the input keys.\\n\\n        Returns:\\n            List[str]: The input keys.\\n        \"\"\"\\n        return [\"reference\", \"prediction\"]\\n\\n    @property\\n    def evaluation_name(self) -> str:\\n        \"\"\"\\n        Get the evaluation name.\\n\\n        Returns:\\n            str: The evaluation name.\\n        \"\"\"\\n        return \"regex_match\"', metadata={'source': 'test_repo/libs/langchain/langchain/evaluation/regex_match/base.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='Returns:\\n            str: The evaluation name.\\n        \"\"\"\\n        return \"regex_match\"\\n\\n    def _evaluate_strings(  # type: ignore[arg-type,override]\\n        self,\\n        *,\\n        prediction: str,\\n        reference: str,\\n        **kwargs: Any,\\n    ) -> dict:\\n        \"\"\"\\n        Evaluate the regex match between the prediction and the reference.\\n\\n        Args:\\n            prediction (str): The prediction string.\\n            reference (Optional[str], optional): The reference regex pattern.\\n\\n        Returns:\\n            dict: The evaluation results containing the score.\\n        \"\"\"\\n        match = re.match(reference, prediction, flags=self.flags)\\n        return {\"score\": int(bool(match))}', metadata={'source': 'test_repo/libs/langchain/langchain/evaluation/regex_match/base.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='\"\"\"Comparison evaluators.\\n\\nThis module contains evaluators for comparing the output of two models,\\nbe they LLMs, Chains, or otherwise. This can be used for scoring\\npreferences, measuring similarity / semantic equivalence between outputs,\\nor any other comparison task.', metadata={'source': 'test_repo/libs/langchain/langchain/evaluation/comparison/__init__.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='Example:\\n    >>> from langchain_community.chat_models import ChatOpenAI\\n    >>> from langchain.evaluation.comparison import PairwiseStringEvalChain\\n    >>> llm = ChatOpenAI(temperature=0)\\n    >>> chain = PairwiseStringEvalChain.from_llm(llm=llm)\\n    >>> result = chain.evaluate_string_pairs(\\n    ...     input = \"What is the chemical formula for water?\",\\n    ...     prediction = \"H2O\",\\n    ...     prediction_b = (\\n    ...        \"The chemical formula for water is H2O, which means\"\\n    ...        \" there are two hydrogen atoms and one oxygen atom.\"\\n    ...     reference = \"The chemical formula for water is H2O.\",\\n    ... )\\n    >>> print(result)\\n    # {\\n    #    \"value\": \"B\",\\n    #    \"comment\": \"Both responses accurately state\"\\n    #       \" that the chemical formula for water is H2O.\"\\n    #       \" However, Response B provides additional information\"\\n    # .     \" by explaining what the formula means.\\\\\\\\n[[B]]\"\\n    # }\\n\"\"\"\\nfrom langchain.evaluation.comparison.eval_chain import (', metadata={'source': 'test_repo/libs/langchain/langchain/evaluation/comparison/__init__.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='# }\\n\"\"\"\\nfrom langchain.evaluation.comparison.eval_chain import (\\n    LabeledPairwiseStringEvalChain,\\n    PairwiseStringEvalChain,\\n)', metadata={'source': 'test_repo/libs/langchain/langchain/evaluation/comparison/__init__.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='__all__ = [\"PairwiseStringEvalChain\", \"LabeledPairwiseStringEvalChain\"]', metadata={'source': 'test_repo/libs/langchain/langchain/evaluation/comparison/__init__.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='\"\"\"Base classes for comparing the output of two models.\"\"\"\\nfrom __future__ import annotations\\n\\nimport logging\\nimport re\\nfrom typing import Any, Dict, List, Optional, Union\\n\\nfrom langchain_community.chat_models.azure_openai import AzureChatOpenAI\\nfrom langchain_community.chat_models.openai import ChatOpenAI\\nfrom langchain_core.language_models import BaseLanguageModel\\nfrom langchain_core.output_parsers import BaseOutputParser\\nfrom langchain_core.prompts.prompt import PromptTemplate\\nfrom langchain_core.pydantic_v1 import Extra, Field', metadata={'source': 'test_repo/libs/langchain/langchain/evaluation/comparison/eval_chain.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from langchain.callbacks.manager import Callbacks\\nfrom langchain.chains.constitutional_ai.models import ConstitutionalPrinciple\\nfrom langchain.chains.llm import LLMChain\\nfrom langchain.evaluation.comparison.prompt import (\\n    COMPARISON_TEMPLATE,\\n    COMPARISON_TEMPLATE_WITH_REFERENCE,\\n    CRITERIA_INSTRUCTIONS,\\n)\\nfrom langchain.evaluation.criteria.eval_chain import (\\n    CRITERIA_TYPE,\\n    Criteria,\\n)\\nfrom langchain.evaluation.schema import LLMEvalChain, PairwiseStringEvaluator\\nfrom langchain.schema import RUN_KEY\\n\\nlogger = logging.getLogger(__name__)\\n\\n_FIND_DOUBLE_BRACKETS = re.compile(r\"\\\\[\\\\[(.*?)\\\\]\\\\]\")', metadata={'source': 'test_repo/libs/langchain/langchain/evaluation/comparison/eval_chain.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='_SUPPORTED_CRITERIA = {\\n    Criteria.CONCISENESS: \"Is the submission concise and to the point?\",\\n    Criteria.RELEVANCE: \"Is the submission referring to a real quote from the text?\",\\n    Criteria.CORRECTNESS: \"Is the submission correct, accurate, and factual?\",\\n    Criteria.COHERENCE: \"Is the submission coherent, well-structured, and organized?\",\\n    Criteria.HARMFULNESS: \"Is the submission harmful, offensive, or inappropriate?\",\\n    Criteria.MALICIOUSNESS: \"Is the submission malicious in any way?\",\\n    Criteria.HELPFULNESS: \"Is the submission helpful, insightful, and appropriate?\",\\n    Criteria.CONTROVERSIALITY: \"Is the submission controversial or debatable?\",\\n    Criteria.MISOGYNY: \"Is the submission misogynistic or sexist?\",\\n    Criteria.CRIMINALITY: \"Is the submission criminal in any way?\",\\n    Criteria.INSENSITIVITY: \"Is the submission insensitive to any group of people?\",\\n    Criteria.DEPTH: \"Does the submission demonstrate depth of thought?\",', metadata={'source': 'test_repo/libs/langchain/langchain/evaluation/comparison/eval_chain.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='Criteria.DEPTH: \"Does the submission demonstrate depth of thought?\",\\n    Criteria.CREATIVITY: \"Does the submission demonstrate novelty or unique ideas?\",\\n    Criteria.DETAIL: \"Does the submission demonstrate attention to detail?\",\\n}', metadata={'source': 'test_repo/libs/langchain/langchain/evaluation/comparison/eval_chain.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='def resolve_pairwise_criteria(\\n    criteria: Optional[Union[CRITERIA_TYPE, str, List[CRITERIA_TYPE]]],\\n) -> dict:\\n    \"\"\"Resolve the criteria for the pairwise evaluator.\\n\\n    Args:\\n        criteria (Union[CRITERIA_TYPE, str, List[CRITERIA_TYPE]], optional):\\n        The criteria to use.\\n\\n    Returns:\\n        dict: The resolved criteria.', metadata={'source': 'test_repo/libs/langchain/langchain/evaluation/comparison/eval_chain.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='\"\"\"\\n    if criteria is None:\\n        _default_criteria = [\\n            Criteria.HELPFULNESS,\\n            Criteria.RELEVANCE,\\n            Criteria.CORRECTNESS,\\n            Criteria.DEPTH,\\n        ]\\n        return {k.value: _SUPPORTED_CRITERIA[k] for k in _default_criteria}\\n    elif isinstance(criteria, Criteria):\\n        criteria_ = {criteria.value: _SUPPORTED_CRITERIA[criteria]}\\n    elif isinstance(criteria, str):\\n        if criteria in _SUPPORTED_CRITERIA:\\n            criteria_ = {criteria: _SUPPORTED_CRITERIA[Criteria(criteria)]}\\n        else:\\n            criteria_ = {criteria: \"\"}\\n    elif isinstance(criteria, ConstitutionalPrinciple):\\n        criteria_ = {criteria.name: criteria.critique_request}\\n    elif isinstance(criteria, (list, tuple)):\\n        criteria_ = {\\n            k: v\\n            for criterion in criteria\\n            for k, v in resolve_pairwise_criteria(criterion).items()\\n        }\\n    else:\\n        if not criteria:\\n            raise ValueError(', metadata={'source': 'test_repo/libs/langchain/langchain/evaluation/comparison/eval_chain.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='}\\n    else:\\n        if not criteria:\\n            raise ValueError(\\n                \"Criteria cannot be empty. \"\\n                \"Please provide a criterion name or a mapping of the criterion name\"\\n                \" to its description.\"\\n            )\\n        criteria_ = dict(criteria)\\n    return criteria_', metadata={'source': 'test_repo/libs/langchain/langchain/evaluation/comparison/eval_chain.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='class PairwiseStringResultOutputParser(BaseOutputParser[dict]):\\n    \"\"\"A parser for the output of the PairwiseStringEvalChain.\\n\\n    Attributes:\\n        _type (str): The type of the output parser.\\n\\n    \"\"\"\\n\\n    @property\\n    def _type(self) -> str:\\n        \"\"\"Return the type of the output parser.\\n\\n        Returns:\\n            str: The type of the output parser.\\n\\n        \"\"\"\\n        return \"pairwise_string_result\"\\n\\n    def parse(self, text: str) -> Dict[str, Any]:\\n        \"\"\"Parse the output text.\\n\\n        Args:\\n            text (str): The output text to parse.\\n\\n        Returns:\\n            Dict: The parsed output.\\n\\n        Raises:\\n            ValueError: If the verdict is invalid.\\n\\n        \"\"\"\\n        match = _FIND_DOUBLE_BRACKETS.search(text)\\n\\n        if match:\\n            verdict = match.group(1)', metadata={'source': 'test_repo/libs/langchain/langchain/evaluation/comparison/eval_chain.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='if match:\\n            verdict = match.group(1)\\n\\n        if not match or verdict not in {\"A\", \"B\", \"C\"}:\\n            raise ValueError(\\n                f\"Invalid output: {text}. \"\\n                \"Output must contain a double bracketed string\\\\\\n                 with the verdict \\'A\\', \\'B\\', or \\'C\\'.\"\\n            )\\n        # C means the models are tied. Return \\'None\\' meaning no preference\\n        verdict_ = None if verdict == \"C\" else verdict\\n        score = {\\n            \"A\": 1,\\n            \"B\": 0,\\n            \"C\": 0.5,\\n        }[verdict]\\n        return {\\n            \"reasoning\": text,\\n            \"value\": verdict_,\\n            \"score\": score,\\n        }', metadata={'source': 'test_repo/libs/langchain/langchain/evaluation/comparison/eval_chain.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='class PairwiseStringEvalChain(PairwiseStringEvaluator, LLMEvalChain, LLMChain):\\n    \"\"\"A chain for comparing two outputs, such as the outputs\\n     of two models, prompts, or outputs of a single model on similar inputs.\\n\\n    Attributes:\\n        output_parser (BaseOutputParser): The output parser for the chain.', metadata={'source': 'test_repo/libs/langchain/langchain/evaluation/comparison/eval_chain.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='Example:\\n        >>> from langchain_community.chat_models import ChatOpenAI\\n        >>> from langchain.evaluation.comparison import PairwiseStringEvalChain\\n        >>> llm = ChatOpenAI(temperature=0, model_name=\"gpt-4\", model_kwargs={\"random_seed\": 42})\\n        >>> chain = PairwiseStringEvalChain.from_llm(llm=llm)\\n        >>> result = chain.evaluate_string_pairs(\\n        ...     input = \"What is the chemical formula for water?\",\\n        ...     prediction = \"H2O\",\\n        ...     prediction_b = (\\n        ...        \"The chemical formula for water is H2O, which means\"\\n        ...        \" there are two hydrogen atoms and one oxygen atom.\"\\n        ...     reference = \"The chemical formula for water is H2O.\",\\n        ... )\\n        >>> print(result)\\n        # {\\n        #    \"value\": \"B\",\\n        #    \"comment\": \"Both responses accurately state\"\\n        #       \" that the chemical formula for water is H2O.\"\\n        #       \" However, Response B provides additional information\"', metadata={'source': 'test_repo/libs/langchain/langchain/evaluation/comparison/eval_chain.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='#       \" However, Response B provides additional information\"\\n        # .     \" by explaining what the formula means.\\\\\\\\n[[B]]\"\\n        # }', metadata={'source': 'test_repo/libs/langchain/langchain/evaluation/comparison/eval_chain.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='\"\"\"  # noqa: E501\\n\\n    output_key: str = \"results\"  #: :meta private:\\n    output_parser: BaseOutputParser = Field(\\n        default_factory=PairwiseStringResultOutputParser\\n    )\\n\\n    @classmethod\\n    def is_lc_serializable(cls) -> bool:\\n        return False\\n\\n    class Config:\\n        \"\"\"Configuration for the PairwiseStringEvalChain.\"\"\"\\n\\n        extra = Extra.ignore\\n\\n    @property\\n    def requires_reference(self) -> bool:\\n        \"\"\"Return whether the chain requires a reference.\\n\\n        Returns:\\n            bool: True if the chain requires a reference, False otherwise.\\n\\n        \"\"\"\\n        return False\\n\\n    @property\\n    def requires_input(self) -> bool:\\n        \"\"\"Return whether the chain requires an input.\\n\\n        Returns:\\n            bool: True if the chain requires an input, False otherwise.\\n\\n        \"\"\"\\n        return True\\n\\n    @property\\n    def _skip_reference_warning(self) -> str:\\n        \"\"\"Return the warning to show when reference is ignored.', metadata={'source': 'test_repo/libs/langchain/langchain/evaluation/comparison/eval_chain.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='Returns:\\n            str: The warning to show when reference is ignored.\\n\\n        \"\"\"\\n        return (\\n            f\"Ignoring reference in {self.__class__.__name__}, as it is not expected.\"\\n            \"\\\\nTo use a reference, use the LabeledPairwiseStringEvalChain\"\\n            \" (EvaluatorType.LABELED_PAIRWISE_STRING) instead.\"\\n        )\\n\\n    @classmethod\\n    def from_llm(\\n        cls,\\n        llm: BaseLanguageModel,\\n        *,\\n        prompt: Optional[PromptTemplate] = None,\\n        criteria: Optional[Union[CRITERIA_TYPE, str]] = None,\\n        **kwargs: Any,\\n    ) -> PairwiseStringEvalChain:\\n        \"\"\"Initialize the PairwiseStringEvalChain from an LLM.\\n\\n        Args:\\n            llm (BaseChatModel): The LLM to use (GPT-4 recommended).\\n            prompt (PromptTemplate, optional): The prompt to use.\\n            **kwargs (Any): Additional keyword arguments.\\n\\n        Returns:\\n            PairwiseStringEvalChain: The initialized PairwiseStringEvalChain.', metadata={'source': 'test_repo/libs/langchain/langchain/evaluation/comparison/eval_chain.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='Returns:\\n            PairwiseStringEvalChain: The initialized PairwiseStringEvalChain.\\n\\n        Raises:\\n            ValueError: If the input variables are not as expected.\\n\\n        \"\"\"\\n        if not (\\n            isinstance(llm, (ChatOpenAI, AzureChatOpenAI))\\n            and llm.model_name.startswith(\"gpt-4\")\\n        ):\\n            logger.warning(\\n                \"This chain was only tested with GPT-4. \\\\\\nPerformance may be significantly worse with other models.\"\\n            )', metadata={'source': 'test_repo/libs/langchain/langchain/evaluation/comparison/eval_chain.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='expected_input_vars = {\"prediction\", \"prediction_b\", \"input\", \"criteria\"}\\n        prompt_ = prompt or COMPARISON_TEMPLATE.partial(reference=\"\")\\n        if expected_input_vars != set(prompt_.input_variables):\\n            raise ValueError(\\n                f\"Input variables should be {expected_input_vars}, \"\\n                f\"but got {prompt_.input_variables}\"\\n            )\\n        criteria_ = resolve_pairwise_criteria(criteria)\\n        criteria_str = \"\\\\n\".join(f\"{k}: {v}\" if v else k for k, v in criteria_.items())\\n        criteria_str = CRITERIA_INSTRUCTIONS + criteria_str if criteria_str else \"\"\\n        return cls(llm=llm, prompt=prompt_.partial(criteria=criteria_str), **kwargs)\\n\\n    def _prepare_input(\\n        self,\\n        prediction: str,\\n        prediction_b: str,\\n        input: Optional[str],\\n        reference: Optional[str],\\n    ) -> dict:\\n        \"\"\"Prepare the input for the chain.', metadata={'source': 'test_repo/libs/langchain/langchain/evaluation/comparison/eval_chain.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='Args:\\n            prediction (str): The output string from the first model.\\n            prediction_b (str): The output string from the second model.\\n            input (str, optional): The input or task string.\\n            reference (str, optional): The reference string, if any.\\n\\n        Returns:\\n            dict: The prepared input for the chain.\\n\\n        \"\"\"\\n        input_ = {\\n            \"prediction\": prediction,\\n            \"prediction_b\": prediction_b,\\n            \"input\": input,\\n        }\\n        if self.requires_reference:\\n            input_[\"reference\"] = reference\\n        return input_\\n\\n    def _prepare_output(self, result: dict) -> dict:\\n        \"\"\"Prepare the output.\"\"\"\\n        parsed = result[self.output_key]\\n        if RUN_KEY in result:\\n            parsed[RUN_KEY] = result[RUN_KEY]\\n        return parsed', metadata={'source': 'test_repo/libs/langchain/langchain/evaluation/comparison/eval_chain.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='def _evaluate_string_pairs(\\n        self,\\n        *,\\n        prediction: str,\\n        prediction_b: str,\\n        input: Optional[str] = None,\\n        reference: Optional[str] = None,\\n        callbacks: Callbacks = None,\\n        tags: Optional[List[str]] = None,\\n        metadata: Optional[Dict[str, Any]] = None,\\n        include_run_info: bool = False,\\n        **kwargs: Any,\\n    ) -> dict:\\n        \"\"\"Evaluate whether output A is preferred to output B.\\n\\n        Args:\\n            prediction (str): The output string from the first model.\\n            prediction_b (str): The output string from the second model.\\n            input (str, optional): The input or task string.\\n            callbacks (Callbacks, optional): The callbacks to use.\\n            reference (str, optional): The reference string, if any.\\n            **kwargs (Any): Additional keyword arguments.', metadata={'source': 'test_repo/libs/langchain/langchain/evaluation/comparison/eval_chain.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='Returns:\\n            dict: A dictionary containing:\\n                - reasoning: The reasoning for the preference.\\n                - value: The preference value, which is either \\'A\\', \\'B\\', or None\\n                    for no preference.\\n                - score: The preference score, which is 1 for \\'A\\', 0 for \\'B\\',\\n                    and 0.5 for None.\\n\\n        \"\"\"\\n        input_ = self._prepare_input(prediction, prediction_b, input, reference)\\n        result = self(\\n            inputs=input_,\\n            callbacks=callbacks,\\n            tags=tags,\\n            metadata=metadata,\\n            include_run_info=include_run_info,\\n        )\\n        return self._prepare_output(result)', metadata={'source': 'test_repo/libs/langchain/langchain/evaluation/comparison/eval_chain.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='async def _aevaluate_string_pairs(\\n        self,\\n        *,\\n        prediction: str,\\n        prediction_b: str,\\n        reference: Optional[str] = None,\\n        input: Optional[str] = None,\\n        callbacks: Callbacks = None,\\n        tags: Optional[List[str]] = None,\\n        metadata: Optional[Dict[str, Any]] = None,\\n        include_run_info: bool = False,\\n        **kwargs: Any,\\n    ) -> dict:\\n        \"\"\"Asynchronously evaluate whether output A is preferred to output B.\\n\\n        Args:\\n            prediction (str): The output string from the first model.\\n            prediction_b (str): The output string from the second model.\\n            input (str, optional): The input or task string.\\n            callbacks (Callbacks, optional): The callbacks to use.\\n            reference (str, optional): The reference string, if any.\\n            **kwargs (Any): Additional keyword arguments.', metadata={'source': 'test_repo/libs/langchain/langchain/evaluation/comparison/eval_chain.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='Returns:\\n            dict: A dictionary containing:\\n                - reasoning: The reasoning for the preference.\\n                - value: The preference value, which is either \\'A\\', \\'B\\', or None\\n                    for no preference.\\n                - score: The preference score, which is 1 for \\'A\\', 0 for \\'B\\',\\n                    and 0.5 for None.\\n\\n        \"\"\"\\n        input_ = self._prepare_input(prediction, prediction_b, input, reference)\\n        result = await self.acall(\\n            inputs=input_,\\n            callbacks=callbacks,\\n            tags=tags,\\n            metadata=metadata,\\n            include_run_info=include_run_info,\\n        )\\n        return self._prepare_output(result)', metadata={'source': 'test_repo/libs/langchain/langchain/evaluation/comparison/eval_chain.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='class LabeledPairwiseStringEvalChain(PairwiseStringEvalChain):\\n    \"\"\"A chain for comparing two outputs, such as the outputs\\n     of two models, prompts, or outputs of a single model on similar inputs,\\n     with labeled preferences.\\n\\n    Attributes:\\n        output_parser (BaseOutputParser): The output parser for the chain.\\n\\n    \"\"\"\\n\\n    @property\\n    def requires_reference(self) -> bool:\\n        \"\"\"Return whether the chain requires a reference.\\n\\n        Returns:\\n            bool: True if the chain requires a reference, False otherwise.\\n\\n        \"\"\"\\n        return True\\n\\n    @classmethod\\n    def from_llm(\\n        cls,\\n        llm: BaseLanguageModel,\\n        *,\\n        prompt: Optional[PromptTemplate] = None,\\n        criteria: Optional[Union[CRITERIA_TYPE, str]] = None,\\n        **kwargs: Any,\\n    ) -> PairwiseStringEvalChain:\\n        \"\"\"Initialize the LabeledPairwiseStringEvalChain from an LLM.', metadata={'source': 'test_repo/libs/langchain/langchain/evaluation/comparison/eval_chain.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='Args:\\n            llm (BaseLanguageModel): The LLM to use.\\n            prompt (PromptTemplate, optional): The prompt to use.\\n            criteria (Union[CRITERIA_TYPE, str], optional): The criteria to use.\\n            **kwargs (Any): Additional keyword arguments.\\n\\n        Returns:\\n            LabeledPairwiseStringEvalChain: The initialized LabeledPairwiseStringEvalChain.\\n\\n        Raises:\\n            ValueError: If the input variables are not as expected.', metadata={'source': 'test_repo/libs/langchain/langchain/evaluation/comparison/eval_chain.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='Raises:\\n            ValueError: If the input variables are not as expected.\\n\\n        \"\"\"  # noqa: E501\\n        expected_input_vars = {\\n            \"prediction\",\\n            \"prediction_b\",\\n            \"input\",\\n            \"reference\",\\n            \"criteria\",\\n        }\\n        prompt_ = prompt or COMPARISON_TEMPLATE_WITH_REFERENCE\\n        if expected_input_vars != set(prompt_.input_variables):\\n            raise ValueError(\\n                f\"Input variables should be {expected_input_vars}, \"\\n                f\"but got {prompt_.input_variables}\"\\n            )\\n        criteria_ = resolve_pairwise_criteria(criteria)\\n        criteria_str = \"\\\\n\".join(f\"{k}: {v}\" for k, v in criteria_.items())\\n        criteria_str = CRITERIA_INSTRUCTIONS + criteria_str if criteria_str else \"\"\\n        return cls(llm=llm, prompt=prompt_.partial(criteria=criteria_str), **kwargs)', metadata={'source': 'test_repo/libs/langchain/langchain/evaluation/comparison/eval_chain.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='\"\"\"Prompts for comparing the outputs of two models for a given question.\\n\\nThis prompt is used to compare two responses and evaluate which one best follows the instructions\\nand answers the question. The prompt is based on the paper from\\nZheng, et. al. https://arxiv.org/abs/2306.05685\\n\"\"\"\\n# flake8: noqa\\nfrom langchain_core.prompts.chat import ChatPromptTemplate', metadata={'source': 'test_repo/libs/langchain/langchain/evaluation/comparison/prompt.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='SYSTEM_MESSAGE = \\'Please act as an impartial judge and evaluate the quality \\\\\\nof the responses provided by two AI assistants to the user question displayed below. \\\\\\nYou should choose the assistant that follows the user\\\\\\'s instructions \\\\\\nand answers \\\\the user\\\\\\'s question better. \\\\\\nYour evaluation should consider factors such as the \\\\\\nhelpfulness, relevance, accuracy, depth, creativity, \\\\\\nand level of detail of their responses. \\\\\\nBegin your evaluation by comparing the two responses and provide a short explanation. \\\\\\nAvoid any position biases and ensure that the order in which \\\\\\nthe responses were presented does not influence your decision. \\\\\\nDo not allow the length of the responses to influence your evaluation. \\\\\\nDo not favor certain names of the assistants. Be as objective as possible. \\\\\\nAfter providing your explanation, output your final verdict by strictly following \\\\\\nthis format: \"[[A]]\" if assistant A is better, \"[[B]]\" if assistant B is better, \\\\\\nand \"[[C]]\" for a tie.\\'', metadata={'source': 'test_repo/libs/langchain/langchain/evaluation/comparison/prompt.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='CRITERIA_INSTRUCTIONS = (\\n    \"For this evaluation, you should primarily consider the following criteria:\\\\n\"\\n)\\n\\nCOMPARISON_TEMPLATE = ChatPromptTemplate.from_messages(\\n    [\\n        (\"system\", SYSTEM_MESSAGE),\\n        (\\n            \"human\",\\n            \"{criteria}[User Question]\\\\n{input}\\\\n\\\\n\\\\\\n[The Start of Assistant A\\'s Answer]\\\\n{prediction}\\\\n\\\\\\n[The End of Assistant A\\'s Answer]\\\\\\n\\\\n\\\\n[The Start of Assistant B\\'s Answer]\\\\n{prediction_b}\\\\n\\\\\\n[The End of Assistant B\\'s Answer]\",\\n        ),\\n    ]\\n)', metadata={'source': 'test_repo/libs/langchain/langchain/evaluation/comparison/prompt.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='COMPARISON_TEMPLATE_WITH_REFERENCE = ChatPromptTemplate.from_messages(\\n    [\\n        (\"system\", SYSTEM_MESSAGE),\\n        (\\n            \"human\",\\n            \"{criteria}\\\\n\\\\nTo help you evaluate the responses, \\\\\\nhere is a reference answer to the user\\'s question:\\\\n\\\\\\n{reference}\\\\\\n[User Question]\\\\n{input}\\\\n\\\\n\\\\\\n[The Start of Assistant A\\'s Answer]\\\\n{prediction}\\\\n\\\\\\n[The End of Assistant A\\'s Answer]\\\\\\n\\\\n\\\\n[The Start of Assistant B\\'s Answer]\\\\n{prediction_b}\\\\n\\\\\\n[The End of Assistant B\\'s Answer]\",\\n        ),\\n    ]\\n)', metadata={'source': 'test_repo/libs/langchain/langchain/evaluation/comparison/prompt.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='\"\"\"Chains and utils related to evaluating question answering functionality.\"\"\"\\nfrom langchain.evaluation.qa.eval_chain import (\\n    ContextQAEvalChain,\\n    CotQAEvalChain,\\n    QAEvalChain,\\n)\\nfrom langchain.evaluation.qa.generate_chain import QAGenerateChain\\n\\n__all__ = [\"QAEvalChain\", \"QAGenerateChain\", \"ContextQAEvalChain\", \"CotQAEvalChain\"]', metadata={'source': 'test_repo/libs/langchain/langchain/evaluation/qa/__init__.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='\"\"\"LLM Chain for generating examples for question answering.\"\"\"\\nfrom __future__ import annotations\\n\\nfrom typing import Any\\n\\nfrom langchain_core.language_models import BaseLanguageModel\\nfrom langchain_core.output_parsers import BaseLLMOutputParser\\nfrom langchain_core.pydantic_v1 import Field\\n\\nfrom langchain.chains.llm import LLMChain\\nfrom langchain.evaluation.qa.generate_prompt import PROMPT\\nfrom langchain.output_parsers.regex import RegexParser\\n\\n_QA_OUTPUT_PARSER = RegexParser(\\n    regex=r\"QUESTION: (.*?)\\\\n+ANSWER: (.*)\", output_keys=[\"query\", \"answer\"]\\n)', metadata={'source': 'test_repo/libs/langchain/langchain/evaluation/qa/generate_chain.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='class QAGenerateChain(LLMChain):\\n    \"\"\"LLM Chain for generating examples for question answering.\"\"\"\\n\\n    output_parser: BaseLLMOutputParser = Field(default=_QA_OUTPUT_PARSER)\\n    output_key: str = \"qa_pairs\"\\n\\n    @classmethod\\n    def is_lc_serializable(cls) -> bool:\\n        return False\\n\\n    @classmethod\\n    def from_llm(cls, llm: BaseLanguageModel, **kwargs: Any) -> QAGenerateChain:\\n        \"\"\"Load QA Generate Chain from LLM.\"\"\"\\n        return cls(llm=llm, prompt=PROMPT, **kwargs)', metadata={'source': 'test_repo/libs/langchain/langchain/evaluation/qa/generate_chain.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='\"\"\"LLM Chains for evaluating question answering.\"\"\"\\nfrom __future__ import annotations\\n\\nimport re\\nimport string\\nfrom typing import Any, List, Optional, Sequence, Tuple\\n\\nfrom langchain_core.language_models import BaseLanguageModel\\nfrom langchain_core.prompts import PromptTemplate\\nfrom langchain_core.pydantic_v1 import Extra\\n\\nfrom langchain.callbacks.manager import Callbacks\\nfrom langchain.chains.llm import LLMChain\\nfrom langchain.evaluation.qa.eval_prompt import CONTEXT_PROMPT, COT_PROMPT, PROMPT\\nfrom langchain.evaluation.schema import LLMEvalChain, StringEvaluator\\nfrom langchain.schema import RUN_KEY', metadata={'source': 'test_repo/libs/langchain/langchain/evaluation/qa/eval_chain.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='def _get_score(text: str) -> Optional[Tuple[str, int]]:\\n    match = re.search(r\"grade:\\\\s*(correct|incorrect)\", text.strip(), re.IGNORECASE)\\n    if match:\\n        if match.group(1).upper() == \"CORRECT\":\\n            return \"CORRECT\", 1\\n        elif match.group(1).upper() == \"INCORRECT\":\\n            return \"INCORRECT\", 0\\n    try:\\n        first_word = (\\n            text.strip().split()[0].translate(str.maketrans(\"\", \"\", string.punctuation))\\n        )\\n        if first_word.upper() == \"CORRECT\":\\n            return \"CORRECT\", 1\\n        elif first_word.upper() == \"INCORRECT\":\\n            return \"INCORRECT\", 0\\n        last_word = (\\n            text.strip()\\n            .split()[-1]\\n            .translate(str.maketrans(\"\", \"\", string.punctuation))\\n        )\\n        if last_word.upper() == \"CORRECT\":\\n            return \"CORRECT\", 1\\n        elif last_word.upper() == \"INCORRECT\":\\n            return \"INCORRECT\", 0\\n    except IndexError:\\n        pass\\n    return None', metadata={'source': 'test_repo/libs/langchain/langchain/evaluation/qa/eval_chain.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='def _parse_string_eval_output(text: str) -> dict:\\n    \"\"\"Parse the output text.\\n\\n    Args:\\n        text (str): The output text to parse.\\n\\n    Returns:\\n        Any: The parsed output.\\n    \"\"\"\\n    reasoning = text.strip()\\n    parsed_scores = _get_score(reasoning)\\n    if parsed_scores is None:\\n        value, score = None, None\\n    else:\\n        value, score = parsed_scores\\n    return {\\n        \"reasoning\": reasoning,\\n        \"value\": value,\\n        \"score\": score,\\n    }', metadata={'source': 'test_repo/libs/langchain/langchain/evaluation/qa/eval_chain.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='class QAEvalChain(LLMChain, StringEvaluator, LLMEvalChain):\\n    \"\"\"LLM Chain for evaluating question answering.\"\"\"\\n\\n    output_key: str = \"results\"  #: :meta private:\\n\\n    class Config:\\n        \"\"\"Configuration for the QAEvalChain.\"\"\"\\n\\n        extra = Extra.ignore\\n\\n    @classmethod\\n    def is_lc_serializable(cls) -> bool:\\n        return False\\n\\n    @property\\n    def evaluation_name(self) -> str:\\n        return \"correctness\"\\n\\n    @property\\n    def requires_reference(self) -> bool:\\n        return True\\n\\n    @property\\n    def requires_input(self) -> bool:\\n        return True\\n\\n    @classmethod\\n    def from_llm(\\n        cls,\\n        llm: BaseLanguageModel,\\n        prompt: Optional[PromptTemplate] = None,\\n        **kwargs: Any,\\n    ) -> QAEvalChain:\\n        \"\"\"Load QA Eval Chain from LLM.\\n\\n        Args:\\n            llm (BaseLanguageModel): the base language model to use.', metadata={'source': 'test_repo/libs/langchain/langchain/evaluation/qa/eval_chain.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='Args:\\n            llm (BaseLanguageModel): the base language model to use.\\n\\n            prompt (PromptTemplate): A prompt template containing the input_variables:\\n            \\'input\\', \\'answer\\' and \\'result\\' that will be used as the prompt\\n            for evaluation.\\n            Defaults to PROMPT.\\n\\n            **kwargs: additional keyword arguments.\\n\\n        Returns:\\n            QAEvalChain: the loaded QA eval chain.\\n        \"\"\"\\n        prompt = prompt or PROMPT\\n        expected_input_vars = {\"query\", \"answer\", \"result\"}\\n        if expected_input_vars != set(prompt.input_variables):\\n            raise ValueError(\\n                f\"Input variables should be {expected_input_vars}, \"\\n                f\"but got {prompt.input_variables}\"\\n            )\\n        return cls(llm=llm, prompt=prompt, **kwargs)', metadata={'source': 'test_repo/libs/langchain/langchain/evaluation/qa/eval_chain.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='def evaluate(\\n        self,\\n        examples: Sequence[dict],\\n        predictions: Sequence[dict],\\n        question_key: str = \"query\",\\n        answer_key: str = \"answer\",\\n        prediction_key: str = \"result\",\\n        *,\\n        callbacks: Callbacks = None,\\n    ) -> List[dict]:\\n        \"\"\"Evaluate question answering examples and predictions.\"\"\"\\n        inputs = [\\n            {\\n                \"query\": example[question_key],\\n                \"answer\": example[answer_key],\\n                \"result\": predictions[i][prediction_key],\\n            }\\n            for i, example in enumerate(examples)\\n        ]\\n\\n        return self.apply(inputs, callbacks=callbacks)\\n\\n    def _prepare_output(self, result: dict) -> dict:\\n        parsed_result = _parse_string_eval_output(result[self.output_key])\\n        if RUN_KEY in result:\\n            parsed_result[RUN_KEY] = result[RUN_KEY]\\n        return parsed_result', metadata={'source': 'test_repo/libs/langchain/langchain/evaluation/qa/eval_chain.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='def _evaluate_strings(\\n        self,\\n        *,\\n        prediction: str,\\n        reference: Optional[str] = None,\\n        input: Optional[str] = None,\\n        callbacks: Callbacks = None,\\n        include_run_info: bool = False,\\n        **kwargs: Any,\\n    ) -> dict:\\n        \"\"\"Evaluate Chain or LLM output, based on optional input and label.', metadata={'source': 'test_repo/libs/langchain/langchain/evaluation/qa/eval_chain.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='Args:\\n            prediction (str): the LLM or chain prediction to evaluate.\\n            reference (Optional[str], optional): the reference label\\n                to evaluate against.\\n            input (Optional[str], optional): the input to consider during evaluation\\n            callbacks (Callbacks, optional): the callbacks to use for tracing.\\n            include_run_info (bool, optional): whether to include run info in the\\n                returned results.\\n            **kwargs: additional keyword arguments, including callbacks, tags, etc.\\n        Returns:\\n            dict: The evaluation results containing the score or value.\\n        \"\"\"\\n        result = self(\\n            {\\n                \"query\": input,\\n                \"answer\": reference,\\n                \"result\": prediction,\\n            },\\n            callbacks=callbacks,\\n            include_run_info=include_run_info,\\n        )\\n        return self._prepare_output(result)', metadata={'source': 'test_repo/libs/langchain/langchain/evaluation/qa/eval_chain.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='async def _aevaluate_strings(\\n        self,\\n        *,\\n        prediction: str,\\n        reference: Optional[str] = None,\\n        input: Optional[str] = None,\\n        callbacks: Callbacks = None,\\n        include_run_info: bool = False,\\n        **kwargs: Any,\\n    ) -> dict:\\n        result = await self.acall(\\n            inputs={\"query\": input, \"answer\": reference, \"result\": prediction},\\n            callbacks=callbacks,\\n            include_run_info=include_run_info,\\n        )\\n        return self._prepare_output(result)', metadata={'source': 'test_repo/libs/langchain/langchain/evaluation/qa/eval_chain.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='class ContextQAEvalChain(LLMChain, StringEvaluator, LLMEvalChain):\\n    \"\"\"LLM Chain for evaluating QA w/o GT based on context\"\"\"\\n\\n    @classmethod\\n    def is_lc_serializable(cls) -> bool:\\n        return False\\n\\n    @property\\n    def requires_reference(self) -> bool:\\n        \"\"\"Whether the chain requires a reference string.\"\"\"\\n        return True\\n\\n    @property\\n    def requires_input(self) -> bool:\\n        \"\"\"Whether the chain requires an input string.\"\"\"\\n        return True\\n\\n    class Config:\\n        \"\"\"Configuration for the QAEvalChain.\"\"\"\\n\\n        extra = Extra.ignore\\n\\n    @classmethod\\n    def _validate_input_vars(cls, prompt: PromptTemplate) -> None:\\n        expected_input_vars = {\"query\", \"context\", \"result\"}\\n        if expected_input_vars != set(prompt.input_variables):\\n            raise ValueError(\\n                f\"Input variables should be {expected_input_vars}, \"\\n                f\"but got {prompt.input_variables}\"\\n            )', metadata={'source': 'test_repo/libs/langchain/langchain/evaluation/qa/eval_chain.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='@property\\n    def evaluation_name(self) -> str:\\n        return \"Contextual Accuracy\"\\n\\n    @classmethod\\n    def from_llm(\\n        cls,\\n        llm: BaseLanguageModel,\\n        prompt: Optional[PromptTemplate] = None,\\n        **kwargs: Any,\\n    ) -> ContextQAEvalChain:\\n        \"\"\"Load QA Eval Chain from LLM.\\n\\n        Args:\\n            llm (BaseLanguageModel): the base language model to use.\\n\\n            prompt (PromptTemplate): A prompt template containing the input_variables:\\n            \\'query\\', \\'context\\' and \\'result\\' that will be used as the prompt\\n            for evaluation.\\n            Defaults to PROMPT.\\n\\n            **kwargs: additional keyword arguments.\\n\\n        Returns:\\n            ContextQAEvalChain: the loaded QA eval chain.\\n        \"\"\"\\n        prompt = prompt or CONTEXT_PROMPT\\n        cls._validate_input_vars(prompt)\\n        return cls(llm=llm, prompt=prompt, **kwargs)', metadata={'source': 'test_repo/libs/langchain/langchain/evaluation/qa/eval_chain.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='def evaluate(\\n        self,\\n        examples: List[dict],\\n        predictions: List[dict],\\n        question_key: str = \"query\",\\n        context_key: str = \"context\",\\n        prediction_key: str = \"result\",\\n        *,\\n        callbacks: Callbacks = None,\\n    ) -> List[dict]:\\n        \"\"\"Evaluate question answering examples and predictions.\"\"\"\\n        inputs = [\\n            {\\n                \"query\": example[question_key],\\n                \"context\": example[context_key],\\n                \"result\": predictions[i][prediction_key],\\n            }\\n            for i, example in enumerate(examples)\\n        ]\\n\\n        return self.apply(inputs, callbacks=callbacks)\\n\\n    def _prepare_output(self, result: dict) -> dict:\\n        parsed_result = _parse_string_eval_output(result[self.output_key])\\n        if RUN_KEY in result:\\n            parsed_result[RUN_KEY] = result[RUN_KEY]\\n        return parsed_result', metadata={'source': 'test_repo/libs/langchain/langchain/evaluation/qa/eval_chain.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='def _evaluate_strings(\\n        self,\\n        *,\\n        prediction: str,\\n        reference: Optional[str] = None,\\n        input: Optional[str] = None,\\n        callbacks: Callbacks = None,\\n        include_run_info: bool = False,\\n        **kwargs: Any,\\n    ) -> dict:\\n        result = self(\\n            {\\n                \"query\": input,\\n                \"context\": reference,\\n                \"result\": prediction,\\n            },\\n            callbacks=callbacks,\\n            include_run_info=include_run_info,\\n        )\\n        return self._prepare_output(result)', metadata={'source': 'test_repo/libs/langchain/langchain/evaluation/qa/eval_chain.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='async def _aevaluate_strings(\\n        self,\\n        *,\\n        prediction: str,\\n        reference: Optional[str] = None,\\n        input: Optional[str] = None,\\n        callbacks: Callbacks = None,\\n        include_run_info: bool = False,\\n        **kwargs: Any,\\n    ) -> dict:\\n        result = await self.acall(\\n            inputs={\"query\": input, \"context\": reference, \"result\": prediction},\\n            callbacks=callbacks,\\n            include_run_info=include_run_info,\\n        )\\n        return self._prepare_output(result)', metadata={'source': 'test_repo/libs/langchain/langchain/evaluation/qa/eval_chain.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='class CotQAEvalChain(ContextQAEvalChain):\\n    \"\"\"LLM Chain for evaluating QA using chain of thought reasoning.\"\"\"\\n\\n    @classmethod\\n    def is_lc_serializable(cls) -> bool:\\n        return False\\n\\n    @property\\n    def evaluation_name(self) -> str:\\n        return \"COT Contextual Accuracy\"\\n\\n    @classmethod\\n    def from_llm(\\n        cls,\\n        llm: BaseLanguageModel,\\n        prompt: Optional[PromptTemplate] = None,\\n        **kwargs: Any,\\n    ) -> CotQAEvalChain:\\n        \"\"\"Load QA Eval Chain from LLM.\"\"\"\\n        prompt = prompt or COT_PROMPT\\n        cls._validate_input_vars(prompt)\\n        return cls(llm=llm, prompt=prompt, **kwargs)', metadata={'source': 'test_repo/libs/langchain/langchain/evaluation/qa/eval_chain.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='# flake8: noqa\\nfrom langchain_core.prompts import PromptTemplate\\n\\ntemplate = \"\"\"You are a teacher grading a quiz.\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student\\'s answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\"\"\"\\nPROMPT = PromptTemplate(\\n    input_variables=[\"query\", \"result\", \"answer\"], template=template\\n)', metadata={'source': 'test_repo/libs/langchain/langchain/evaluation/qa/eval_prompt.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='context_template = \"\"\"You are a teacher grading a quiz.\\nYou are given a question, the context the question is about, and the student\\'s answer. You are asked to score the student\\'s answer as either CORRECT or INCORRECT, based on the context.\\n\\nExample Format:\\nQUESTION: question here\\nCONTEXT: context the question is about here\\nSTUDENT ANSWER: student\\'s answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\n\\nQUESTION: {query}\\nCONTEXT: {context}\\nSTUDENT ANSWER: {result}\\nGRADE:\"\"\"\\nCONTEXT_PROMPT = PromptTemplate(\\n    input_variables=[\"query\", \"context\", \"result\"], template=context_template\\n)', metadata={'source': 'test_repo/libs/langchain/langchain/evaluation/qa/eval_prompt.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='cot_template = \"\"\"You are a teacher grading a quiz.\\nYou are given a question, the context the question is about, and the student\\'s answer. You are asked to score the student\\'s answer as either CORRECT or INCORRECT, based on the context.\\nWrite out in a step by step manner your reasoning to be sure that your conclusion is correct. Avoid simply stating the correct answer at the outset.\\n\\nExample Format:\\nQUESTION: question here\\nCONTEXT: context the question is about here\\nSTUDENT ANSWER: student\\'s answer here\\nEXPLANATION: step by step reasoning here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!', metadata={'source': 'test_repo/libs/langchain/langchain/evaluation/qa/eval_prompt.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='QUESTION: {query}\\nCONTEXT: {context}\\nSTUDENT ANSWER: {result}\\nEXPLANATION:\"\"\"\\nCOT_PROMPT = PromptTemplate(\\n    input_variables=[\"query\", \"context\", \"result\"], template=cot_template\\n)', metadata={'source': 'test_repo/libs/langchain/langchain/evaluation/qa/eval_prompt.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='template = \"\"\"You are comparing a submitted answer to an expert answer on a given SQL coding question. Here is the data:\\n[BEGIN DATA]\\n***\\n[Question]: {query}\\n***\\n[Expert]: {answer}\\n***\\n[Submission]: {result}\\n***\\n[END DATA]\\nCompare the content and correctness of the submitted SQL with the expert answer. Ignore any differences in whitespace, style, or output column names. The submitted answer may either be correct or incorrect. Determine which case applies. First, explain in detail the similarities or differences between the expert answer and the submission, ignoring superficial aspects such as whitespace, style or output column names. Do not state the final answer in your initial explanation. Then, respond with either \"CORRECT\" or \"INCORRECT\" (without quotes or punctuation) on its own line. This should correspond to whether the submitted SQL and the expert answer are semantically the same or different, respectively. Then, repeat your final answer on a new line.\"\"\"', metadata={'source': 'test_repo/libs/langchain/langchain/evaluation/qa/eval_prompt.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='SQL_PROMPT = PromptTemplate(\\n    input_variables=[\"query\", \"answer\", \"result\"], template=template\\n)', metadata={'source': 'test_repo/libs/langchain/langchain/evaluation/qa/eval_prompt.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='# flake8: noqa\\nfrom langchain.output_parsers.regex import RegexParser\\nfrom langchain_core.prompts import PromptTemplate\\n\\ntemplate = \"\"\"You are a teacher coming up with questions to ask on a quiz. \\nGiven the following document, please generate a question and answer based on that document.\\n\\nExample Format:\\n<Begin Document>\\n...\\n<End Document>\\nQUESTION: question here\\nANSWER: answer here\\n\\nThese questions should be detailed and be based explicitly on information in the document. Begin!\\n\\n<Begin Document>\\n{doc}\\n<End Document>\"\"\"\\nPROMPT = PromptTemplate(\\n    input_variables=[\"doc\"],\\n    template=template,\\n)', metadata={'source': 'test_repo/libs/langchain/langchain/evaluation/qa/generate_prompt.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='\"\"\"String distance evaluators.\"\"\"\\nfrom langchain.evaluation.string_distance.base import (\\n    PairwiseStringDistanceEvalChain,\\n    StringDistance,\\n    StringDistanceEvalChain,\\n)\\n\\n__all__ = [\\n    \"PairwiseStringDistanceEvalChain\",\\n    \"StringDistance\",\\n    \"StringDistanceEvalChain\",\\n]', metadata={'source': 'test_repo/libs/langchain/langchain/evaluation/string_distance/__init__.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='\"\"\"String distance evaluators based on the RapidFuzz library.\"\"\"\\n\\nfrom enum import Enum\\nfrom typing import Any, Callable, Dict, List, Optional\\n\\nfrom langchain_core.pydantic_v1 import Field, root_validator\\n\\nfrom langchain.callbacks.manager import (\\n    AsyncCallbackManagerForChainRun,\\n    CallbackManagerForChainRun,\\n    Callbacks,\\n)\\nfrom langchain.chains.base import Chain\\nfrom langchain.evaluation.schema import PairwiseStringEvaluator, StringEvaluator\\nfrom langchain.schema import RUN_KEY\\n\\n\\ndef _load_rapidfuzz() -> Any:\\n    \"\"\"\\n    Load the RapidFuzz library.\\n\\n    Raises:\\n        ImportError: If the rapidfuzz library is not installed.\\n\\n    Returns:\\n        Any: The rapidfuzz.distance module.\\n    \"\"\"\\n    try:\\n        import rapidfuzz\\n    except ImportError:\\n        raise ImportError(\\n            \"Please install the rapidfuzz library to use the FuzzyMatchStringEvaluator.\"\\n            \"Please install it with `pip install rapidfuzz`.\"\\n        )\\n    return rapidfuzz.distance', metadata={'source': 'test_repo/libs/langchain/langchain/evaluation/string_distance/base.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='class StringDistance(str, Enum):\\n    \"\"\"Distance metric to use.\\n\\n    Attributes:\\n        DAMERAU_LEVENSHTEIN: The Damerau-Levenshtein distance.\\n        LEVENSHTEIN: The Levenshtein distance.\\n        JARO: The Jaro distance.\\n        JARO_WINKLER: The Jaro-Winkler distance.\\n        HAMMING: The Hamming distance.\\n        INDEL: The Indel distance.\\n    \"\"\"\\n\\n    DAMERAU_LEVENSHTEIN = \"damerau_levenshtein\"\\n    LEVENSHTEIN = \"levenshtein\"\\n    JARO = \"jaro\"\\n    JARO_WINKLER = \"jaro_winkler\"\\n    HAMMING = \"hamming\"\\n    INDEL = \"indel\"', metadata={'source': 'test_repo/libs/langchain/langchain/evaluation/string_distance/base.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='class _RapidFuzzChainMixin(Chain):\\n    \"\"\"Shared methods for the rapidfuzz string distance evaluators.\"\"\"\\n\\n    distance: StringDistance = Field(default=StringDistance.JARO_WINKLER)\\n    normalize_score: bool = Field(default=True)\\n    \"\"\"Whether to normalize the score to a value between 0 and 1.\\n    Applies only to the Levenshtein and Damerau-Levenshtein distances.\"\"\"\\n\\n    @root_validator\\n    def validate_dependencies(cls, values: Dict[str, Any]) -> Dict[str, Any]:\\n        \"\"\"\\n        Validate that the rapidfuzz library is installed.\\n\\n        Args:\\n            values (Dict[str, Any]): The input values.\\n\\n        Returns:\\n            Dict[str, Any]: The validated values.\\n        \"\"\"\\n        _load_rapidfuzz()\\n        return values\\n\\n    @property\\n    def output_keys(self) -> List[str]:\\n        \"\"\"\\n        Get the output keys.\\n\\n        Returns:\\n            List[str]: The output keys.\\n        \"\"\"\\n        return [\"score\"]', metadata={'source': 'test_repo/libs/langchain/langchain/evaluation/string_distance/base.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='Returns:\\n            List[str]: The output keys.\\n        \"\"\"\\n        return [\"score\"]\\n\\n    def _prepare_output(self, result: Dict[str, Any]) -> Dict[str, Any]:\\n        \"\"\"\\n        Prepare the output dictionary.\\n\\n        Args:\\n            result (Dict[str, Any]): The evaluation results.\\n\\n        Returns:\\n            Dict[str, Any]: The prepared output dictionary.\\n        \"\"\"\\n        result = {\"score\": result[\"score\"]}\\n        if RUN_KEY in result:\\n            result[RUN_KEY] = result[RUN_KEY].dict()\\n        return result\\n\\n    @staticmethod\\n    def _get_metric(distance: str, normalize_score: bool = False) -> Callable:\\n        \"\"\"\\n        Get the distance metric function based on the distance type.\\n\\n        Args:\\n            distance (str): The distance type.\\n\\n        Returns:\\n            Callable: The distance metric function.\\n\\n        Raises:\\n            ValueError: If the distance metric is invalid.\\n        \"\"\"\\n        from rapidfuzz import distance as rf_distance', metadata={'source': 'test_repo/libs/langchain/langchain/evaluation/string_distance/base.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='module_map: Dict[str, Any] = {\\n            StringDistance.DAMERAU_LEVENSHTEIN: rf_distance.DamerauLevenshtein,\\n            StringDistance.LEVENSHTEIN: rf_distance.Levenshtein,\\n            StringDistance.JARO: rf_distance.Jaro,\\n            StringDistance.JARO_WINKLER: rf_distance.JaroWinkler,\\n            StringDistance.HAMMING: rf_distance.Hamming,\\n            StringDistance.INDEL: rf_distance.Indel,\\n        }\\n        if distance not in module_map:\\n            raise ValueError(\\n                f\"Invalid distance metric: {distance}\"\\n                f\"\\\\nMust be one of: {list(StringDistance)}\"\\n            )\\n        module = module_map[distance]\\n        if normalize_score:\\n            return module.normalized_distance\\n        else:\\n            return module.distance\\n\\n    @property\\n    def metric(self) -> Callable:\\n        \"\"\"\\n        Get the distance metric function.', metadata={'source': 'test_repo/libs/langchain/langchain/evaluation/string_distance/base.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='Returns:\\n            Callable: The distance metric function.\\n        \"\"\"\\n        return _RapidFuzzChainMixin._get_metric(\\n            self.distance, normalize_score=self.normalize_score\\n        )\\n\\n    def compute_metric(self, a: str, b: str) -> float:\\n        \"\"\"\\n        Compute the distance between two strings.\\n\\n        Args:\\n            a (str): The first string.\\n            b (str): The second string.\\n\\n        Returns:\\n            float: The distance between the two strings.\\n        \"\"\"\\n        return self.metric(a, b)', metadata={'source': 'test_repo/libs/langchain/langchain/evaluation/string_distance/base.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='class StringDistanceEvalChain(StringEvaluator, _RapidFuzzChainMixin):\\n    \"\"\"Compute string distances between the prediction and the reference.\\n\\n    Examples\\n    ----------\\n\\n    >>> from langchain.evaluation import StringDistanceEvalChain\\n    >>> evaluator = StringDistanceEvalChain()\\n    >>> evaluator.evaluate_strings(\\n            prediction=\"Mindy is the CTO\",\\n            reference=\"Mindy is the CEO\",\\n        )\\n\\n    Using the `load_evaluator` function:\\n\\n    >>> from langchain.evaluation import load_evaluator\\n    >>> evaluator = load_evaluator(\"string_distance\")\\n    >>> evaluator.evaluate_strings(\\n            prediction=\"The answer is three\",\\n            reference=\"three\",\\n        )\\n    \"\"\"\\n\\n    @property\\n    def requires_input(self) -> bool:\\n        \"\"\"\\n        This evaluator does not require input.\\n        \"\"\"\\n        return False\\n\\n    @property\\n    def requires_reference(self) -> bool:\\n        \"\"\"\\n        This evaluator does not require a reference.\\n        \"\"\"\\n        return True', metadata={'source': 'test_repo/libs/langchain/langchain/evaluation/string_distance/base.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='@property\\n    def input_keys(self) -> List[str]:\\n        \"\"\"\\n        Get the input keys.\\n\\n        Returns:\\n            List[str]: The input keys.\\n        \"\"\"\\n        return [\"reference\", \"prediction\"]\\n\\n    @property\\n    def evaluation_name(self) -> str:\\n        \"\"\"\\n        Get the evaluation name.\\n\\n        Returns:\\n            str: The evaluation name.\\n        \"\"\"\\n        return f\"{self.distance.value}_distance\"\\n\\n    def _call(\\n        self,\\n        inputs: Dict[str, Any],\\n        run_manager: Optional[CallbackManagerForChainRun] = None,\\n    ) -> Dict[str, Any]:\\n        \"\"\"\\n        Compute the string distance between the prediction and the reference.\\n\\n        Args:\\n            inputs (Dict[str, Any]): The input values.\\n            run_manager (Optional[CallbackManagerForChainRun]):\\n                The callback manager.', metadata={'source': 'test_repo/libs/langchain/langchain/evaluation/string_distance/base.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='Returns:\\n            Dict[str, Any]: The evaluation results containing the score.\\n        \"\"\"\\n        return {\"score\": self.compute_metric(inputs[\"reference\"], inputs[\"prediction\"])}\\n\\n    async def _acall(\\n        self,\\n        inputs: Dict[str, Any],\\n        run_manager: Optional[AsyncCallbackManagerForChainRun] = None,\\n    ) -> Dict[str, Any]:\\n        \"\"\"\\n        Asynchronously compute the string distance between the prediction\\n            and the reference.\\n\\n        Args:\\n            inputs (Dict[str, Any]): The input values.\\n            run_manager (Optional[AsyncCallbackManagerForChainRun]:\\n                The callback manager.\\n\\n        Returns:\\n            Dict[str, Any]: The evaluation results containing the score.\\n        \"\"\"\\n        return {\"score\": self.compute_metric(inputs[\"reference\"], inputs[\"prediction\"])}', metadata={'source': 'test_repo/libs/langchain/langchain/evaluation/string_distance/base.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='def _evaluate_strings(\\n        self,\\n        *,\\n        prediction: str,\\n        reference: Optional[str] = None,\\n        input: Optional[str] = None,\\n        callbacks: Callbacks = None,\\n        tags: Optional[List[str]] = None,\\n        metadata: Optional[Dict[str, Any]] = None,\\n        include_run_info: bool = False,\\n        **kwargs: Any,\\n    ) -> dict:\\n        \"\"\"\\n        Evaluate the string distance between the prediction and the reference.\\n\\n        Args:\\n            prediction (str): The prediction string.\\n            reference (Optional[str], optional): The reference string.\\n            input (Optional[str], optional): The input string.\\n            callbacks (Callbacks, optional): The callbacks to use.\\n            **kwargs: Additional keyword arguments.', metadata={'source': 'test_repo/libs/langchain/langchain/evaluation/string_distance/base.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='Returns:\\n            dict: The evaluation results containing the score.\\n        \"\"\"\\n        result = self(\\n            inputs={\"prediction\": prediction, \"reference\": reference},\\n            callbacks=callbacks,\\n            tags=tags,\\n            metadata=metadata,\\n            include_run_info=include_run_info,\\n        )\\n\\n        return self._prepare_output(result)\\n\\n    async def _aevaluate_strings(\\n        self,\\n        *,\\n        prediction: str,\\n        reference: Optional[str] = None,\\n        input: Optional[str] = None,\\n        callbacks: Callbacks = None,\\n        tags: Optional[List[str]] = None,\\n        metadata: Optional[Dict[str, Any]] = None,\\n        include_run_info: bool = False,\\n        **kwargs: Any,\\n    ) -> dict:\\n        \"\"\"\\n        Asynchronously evaluate the string distance between the\\n            prediction and the reference.', metadata={'source': 'test_repo/libs/langchain/langchain/evaluation/string_distance/base.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='Args:\\n            prediction (str): The prediction string.\\n            reference (Optional[str], optional): The reference string.\\n            input (Optional[str], optional): The input string.\\n            callbacks (Callbacks, optional): The callbacks to use.\\n            **kwargs: Additional keyword arguments.\\n\\n        Returns:\\n            dict: The evaluation results containing the score.\\n        \"\"\"\\n        result = await self.acall(\\n            inputs={\"prediction\": prediction, \"reference\": reference},\\n            callbacks=callbacks,\\n            tags=tags,\\n            metadata=metadata,\\n            include_run_info=include_run_info,\\n        )\\n        return self._prepare_output(result)', metadata={'source': 'test_repo/libs/langchain/langchain/evaluation/string_distance/base.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='class PairwiseStringDistanceEvalChain(PairwiseStringEvaluator, _RapidFuzzChainMixin):\\n    \"\"\"Compute string edit distances between two predictions.\"\"\"\\n\\n    @property\\n    def input_keys(self) -> List[str]:\\n        \"\"\"\\n        Get the input keys.\\n\\n        Returns:\\n            List[str]: The input keys.\\n        \"\"\"\\n        return [\"prediction\", \"prediction_b\"]\\n\\n    @property\\n    def evaluation_name(self) -> str:\\n        \"\"\"\\n        Get the evaluation name.\\n\\n        Returns:\\n            str: The evaluation name.\\n        \"\"\"\\n        return f\"pairwise_{self.distance.value}_distance\"\\n\\n    def _call(\\n        self,\\n        inputs: Dict[str, Any],\\n        run_manager: Optional[CallbackManagerForChainRun] = None,\\n    ) -> Dict[str, Any]:\\n        \"\"\"\\n        Compute the string distance between two predictions.\\n\\n        Args:\\n            inputs (Dict[str, Any]): The input values.\\n            run_manager (CallbackManagerForChainRun , optional):\\n                The callback manager.', metadata={'source': 'test_repo/libs/langchain/langchain/evaluation/string_distance/base.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='Returns:\\n            Dict[str, Any]: The evaluation results containing the score.\\n        \"\"\"\\n        return {\\n            \"score\": self.compute_metric(inputs[\"prediction\"], inputs[\"prediction_b\"])\\n        }\\n\\n    async def _acall(\\n        self,\\n        inputs: Dict[str, Any],\\n        run_manager: Optional[AsyncCallbackManagerForChainRun] = None,\\n    ) -> Dict[str, Any]:\\n        \"\"\"\\n        Asynchronously compute the string distance between two predictions.\\n\\n        Args:\\n            inputs (Dict[str, Any]): The input values.\\n            run_manager (AsyncCallbackManagerForChainRun , optional):\\n                The callback manager.\\n\\n        Returns:\\n            Dict[str, Any]: The evaluation results containing the score.\\n        \"\"\"\\n        return {\\n            \"score\": self.compute_metric(inputs[\"prediction\"], inputs[\"prediction_b\"])\\n        }', metadata={'source': 'test_repo/libs/langchain/langchain/evaluation/string_distance/base.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='def _evaluate_string_pairs(\\n        self,\\n        *,\\n        prediction: str,\\n        prediction_b: str,\\n        callbacks: Callbacks = None,\\n        tags: Optional[List[str]] = None,\\n        metadata: Optional[Dict[str, Any]] = None,\\n        include_run_info: bool = False,\\n        **kwargs: Any,\\n    ) -> dict:\\n        \"\"\"\\n        Evaluate the string distance between two predictions.\\n\\n        Args:\\n            prediction (str): The first prediction string.\\n            prediction_b (str): The second prediction string.\\n            callbacks (Callbacks, optional): The callbacks to use.\\n            tags (List[str], optional): Tags to apply to traces.\\n            metadata (Dict[str, Any], optional): Metadata to apply to traces.\\n            **kwargs: Additional keyword arguments.', metadata={'source': 'test_repo/libs/langchain/langchain/evaluation/string_distance/base.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='Returns:\\n            dict: The evaluation results containing the score.\\n        \"\"\"\\n        result = self(\\n            inputs={\"prediction\": prediction, \"prediction_b\": prediction_b},\\n            callbacks=callbacks,\\n            tags=tags,\\n            metadata=metadata,\\n            include_run_info=include_run_info,\\n        )\\n        return self._prepare_output(result)\\n\\n    async def _aevaluate_string_pairs(\\n        self,\\n        *,\\n        prediction: str,\\n        prediction_b: str,\\n        callbacks: Callbacks = None,\\n        tags: Optional[List[str]] = None,\\n        metadata: Optional[Dict[str, Any]] = None,\\n        include_run_info: bool = False,\\n        **kwargs: Any,\\n    ) -> dict:\\n        \"\"\"\\n        Asynchronously evaluate the string distance between two predictions.', metadata={'source': 'test_repo/libs/langchain/langchain/evaluation/string_distance/base.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='Args:\\n            prediction (str): The first prediction string.\\n            prediction_b (str): The second prediction string.\\n            callbacks (Callbacks, optional): The callbacks to use.\\n            tags (List[str], optional): Tags to apply to traces.\\n            metadata (Dict[str, Any], optional): Metadata to apply to traces.\\n            **kwargs: Additional keyword arguments.\\n\\n        Returns:\\n            dict: The evaluation results containing the score.\\n        \"\"\"\\n        result = await self.acall(\\n            inputs={\"prediction\": prediction, \"prediction_b\": prediction_b},\\n            callbacks=callbacks,\\n            tags=tags,\\n            metadata=metadata,\\n            include_run_info=include_run_info,\\n        )\\n        return self._prepare_output(result)', metadata={'source': 'test_repo/libs/langchain/langchain/evaluation/string_distance/base.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='\"\"\"Evaluators that measure embedding distances.\"\"\"\\nfrom langchain.evaluation.embedding_distance.base import (\\n    EmbeddingDistance,\\n    EmbeddingDistanceEvalChain,\\n    PairwiseEmbeddingDistanceEvalChain,\\n)\\n\\n__all__ = [\\n    \"EmbeddingDistance\",\\n    \"EmbeddingDistanceEvalChain\",\\n    \"PairwiseEmbeddingDistanceEvalChain\",\\n]', metadata={'source': 'test_repo/libs/langchain/langchain/evaluation/embedding_distance/__init__.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='\"\"\"A chain for comparing the output of two models using embeddings.\"\"\"\\nfrom enum import Enum\\nfrom typing import Any, Dict, List, Optional\\n\\nimport numpy as np\\nfrom langchain_community.embeddings.openai import OpenAIEmbeddings\\nfrom langchain_core.embeddings import Embeddings\\nfrom langchain_core.pydantic_v1 import Field, root_validator\\n\\nfrom langchain.callbacks.manager import (\\n    AsyncCallbackManagerForChainRun,\\n    CallbackManagerForChainRun,\\n    Callbacks,\\n)\\nfrom langchain.chains.base import Chain\\nfrom langchain.evaluation.schema import PairwiseStringEvaluator, StringEvaluator\\nfrom langchain.schema import RUN_KEY\\nfrom langchain.utils.math import cosine_similarity', metadata={'source': 'test_repo/libs/langchain/langchain/evaluation/embedding_distance/base.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='class EmbeddingDistance(str, Enum):\\n    \"\"\"Embedding Distance Metric.\\n\\n    Attributes:\\n        COSINE: Cosine distance metric.\\n        EUCLIDEAN: Euclidean distance metric.\\n        MANHATTAN: Manhattan distance metric.\\n        CHEBYSHEV: Chebyshev distance metric.\\n        HAMMING: Hamming distance metric.\\n    \"\"\"\\n\\n    COSINE = \"cosine\"\\n    EUCLIDEAN = \"euclidean\"\\n    MANHATTAN = \"manhattan\"\\n    CHEBYSHEV = \"chebyshev\"\\n    HAMMING = \"hamming\"', metadata={'source': 'test_repo/libs/langchain/langchain/evaluation/embedding_distance/base.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='class _EmbeddingDistanceChainMixin(Chain):\\n    \"\"\"Shared functionality for embedding distance evaluators.\\n\\n    Attributes:\\n        embeddings (Embeddings): The embedding objects to vectorize the outputs.\\n        distance_metric (EmbeddingDistance): The distance metric to use\\n                                            for comparing the embeddings.\\n    \"\"\"\\n\\n    embeddings: Embeddings = Field(default_factory=OpenAIEmbeddings)\\n    distance_metric: EmbeddingDistance = Field(default=EmbeddingDistance.COSINE)\\n\\n    @root_validator(pre=False)\\n    def _validate_tiktoken_installed(cls, values: Dict[str, Any]) -> Dict[str, Any]:\\n        \"\"\"Validate that the TikTok library is installed.\\n\\n        Args:\\n            values (Dict[str, Any]): The values to validate.', metadata={'source': 'test_repo/libs/langchain/langchain/evaluation/embedding_distance/base.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='Args:\\n            values (Dict[str, Any]): The values to validate.\\n\\n        Returns:\\n            Dict[str, Any]: The validated values.\\n        \"\"\"\\n        embeddings = values.get(\"embeddings\")\\n        if isinstance(embeddings, OpenAIEmbeddings):\\n            try:\\n                import tiktoken  # noqa: F401\\n            except ImportError:\\n                raise ImportError(\\n                    \"The tiktoken library is required to use the default \"\\n                    \"OpenAI embeddings with embedding distance evaluators.\"\\n                    \" Please either manually select a different Embeddings object\"\\n                    \" or install tiktoken using `pip install tiktoken`.\"\\n                )\\n        return values\\n\\n    class Config:\\n        \"\"\"Permit embeddings to go unvalidated.\"\"\"\\n\\n        arbitrary_types_allowed: bool = True\\n\\n    @property\\n    def output_keys(self) -> List[str]:\\n        \"\"\"Return the output keys of the chain.', metadata={'source': 'test_repo/libs/langchain/langchain/evaluation/embedding_distance/base.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='Returns:\\n            List[str]: The output keys.\\n        \"\"\"\\n        return [\"score\"]\\n\\n    def _prepare_output(self, result: dict) -> dict:\\n        parsed = {\"score\": result[\"score\"]}\\n        if RUN_KEY in result:\\n            parsed[RUN_KEY] = result[RUN_KEY]\\n        return parsed\\n\\n    def _get_metric(self, metric: EmbeddingDistance) -> Any:\\n        \"\"\"Get the metric function for the given metric name.\\n\\n        Args:\\n            metric (EmbeddingDistance): The metric name.', metadata={'source': 'test_repo/libs/langchain/langchain/evaluation/embedding_distance/base.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='Args:\\n            metric (EmbeddingDistance): The metric name.\\n\\n        Returns:\\n            Any: The metric function.\\n        \"\"\"\\n        metrics = {\\n            EmbeddingDistance.COSINE: self._cosine_distance,\\n            EmbeddingDistance.EUCLIDEAN: self._euclidean_distance,\\n            EmbeddingDistance.MANHATTAN: self._manhattan_distance,\\n            EmbeddingDistance.CHEBYSHEV: self._chebyshev_distance,\\n            EmbeddingDistance.HAMMING: self._hamming_distance,\\n        }\\n        if metric in metrics:\\n            return metrics[metric]\\n        else:\\n            raise ValueError(f\"Invalid metric: {metric}\")\\n\\n    @staticmethod\\n    def _cosine_distance(a: np.ndarray, b: np.ndarray) -> np.ndarray:\\n        \"\"\"Compute the cosine distance between two vectors.\\n\\n        Args:\\n            a (np.ndarray): The first vector.\\n            b (np.ndarray): The second vector.', metadata={'source': 'test_repo/libs/langchain/langchain/evaluation/embedding_distance/base.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='Returns:\\n            np.ndarray: The cosine distance.\\n        \"\"\"\\n        return 1.0 - cosine_similarity(a, b)\\n\\n    @staticmethod\\n    def _euclidean_distance(a: np.ndarray, b: np.ndarray) -> np.floating:\\n        \"\"\"Compute the Euclidean distance between two vectors.\\n\\n        Args:\\n            a (np.ndarray): The first vector.\\n            b (np.ndarray): The second vector.\\n\\n        Returns:\\n            np.floating: The Euclidean distance.\\n        \"\"\"\\n        return np.linalg.norm(a - b)\\n\\n    @staticmethod\\n    def _manhattan_distance(a: np.ndarray, b: np.ndarray) -> np.floating:\\n        \"\"\"Compute the Manhattan distance between two vectors.\\n\\n        Args:\\n            a (np.ndarray): The first vector.\\n            b (np.ndarray): The second vector.\\n\\n        Returns:\\n            np.floating: The Manhattan distance.\\n        \"\"\"\\n        return np.sum(np.abs(a - b))', metadata={'source': 'test_repo/libs/langchain/langchain/evaluation/embedding_distance/base.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='@staticmethod\\n    def _chebyshev_distance(a: np.ndarray, b: np.ndarray) -> np.floating:\\n        \"\"\"Compute the Chebyshev distance between two vectors.\\n\\n        Args:\\n            a (np.ndarray): The first vector.\\n            b (np.ndarray): The second vector.\\n\\n        Returns:\\n            np.floating: The Chebyshev distance.\\n        \"\"\"\\n        return np.max(np.abs(a - b))\\n\\n    @staticmethod\\n    def _hamming_distance(a: np.ndarray, b: np.ndarray) -> np.floating:\\n        \"\"\"Compute the Hamming distance between two vectors.\\n\\n        Args:\\n            a (np.ndarray): The first vector.\\n            b (np.ndarray): The second vector.\\n\\n        Returns:\\n            np.floating: The Hamming distance.\\n        \"\"\"\\n        return np.mean(a != b)\\n\\n    def _compute_score(self, vectors: np.ndarray) -> float:\\n        \"\"\"Compute the score based on the distance metric.\\n\\n        Args:\\n            vectors (np.ndarray): The input vectors.', metadata={'source': 'test_repo/libs/langchain/langchain/evaluation/embedding_distance/base.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='Args:\\n            vectors (np.ndarray): The input vectors.\\n\\n        Returns:\\n            float: The computed score.\\n        \"\"\"\\n        metric = self._get_metric(self.distance_metric)\\n        score = metric(vectors[0].reshape(1, -1), vectors[1].reshape(1, -1)).item()\\n        return score', metadata={'source': 'test_repo/libs/langchain/langchain/evaluation/embedding_distance/base.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='class EmbeddingDistanceEvalChain(_EmbeddingDistanceChainMixin, StringEvaluator):\\n    \"\"\"Use embedding distances to score semantic difference between\\n    a prediction and reference.\\n\\n    Examples:\\n        >>> chain = EmbeddingDistanceEvalChain()\\n        >>> result = chain.evaluate_strings(prediction=\"Hello\", reference=\"Hi\")\\n        >>> print(result)\\n        {\\'score\\': 0.5}\\n    \"\"\"\\n\\n    @property\\n    def requires_reference(self) -> bool:\\n        \"\"\"Return whether the chain requires a reference.\\n\\n        Returns:\\n            bool: True if a reference is required, False otherwise.\\n        \"\"\"\\n        return True\\n\\n    @property\\n    def evaluation_name(self) -> str:\\n        return f\"embedding_{self.distance_metric.value}_distance\"\\n\\n    @property\\n    def input_keys(self) -> List[str]:\\n        \"\"\"Return the input keys of the chain.\\n\\n        Returns:\\n            List[str]: The input keys.\\n        \"\"\"\\n        return [\"prediction\", \"reference\"]', metadata={'source': 'test_repo/libs/langchain/langchain/evaluation/embedding_distance/base.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='def _call(\\n        self,\\n        inputs: Dict[str, Any],\\n        run_manager: Optional[CallbackManagerForChainRun] = None,\\n    ) -> Dict[str, Any]:\\n        \"\"\"Compute the score for a prediction and reference.\\n\\n        Args:\\n            inputs (Dict[str, Any]): The input data.\\n            run_manager (Optional[CallbackManagerForChainRun], optional):\\n                The callback manager.\\n\\n        Returns:\\n            Dict[str, Any]: The computed score.\\n        \"\"\"\\n        vectors = np.array(\\n            self.embeddings.embed_documents([inputs[\"prediction\"], inputs[\"reference\"]])\\n        )\\n        score = self._compute_score(vectors)\\n        return {\"score\": score}\\n\\n    async def _acall(\\n        self,\\n        inputs: Dict[str, Any],\\n        run_manager: Optional[AsyncCallbackManagerForChainRun] = None,\\n    ) -> Dict[str, Any]:\\n        \"\"\"Asynchronously compute the score for a prediction and reference.', metadata={'source': 'test_repo/libs/langchain/langchain/evaluation/embedding_distance/base.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='Args:\\n            inputs (Dict[str, Any]): The input data.\\n            run_manager (AsyncCallbackManagerForChainRun, optional):\\n                The callback manager.\\n\\n        Returns:\\n            Dict[str, Any]: The computed score.\\n        \"\"\"\\n        embedded = await self.embeddings.aembed_documents(\\n            [inputs[\"prediction\"], inputs[\"reference\"]]\\n        )\\n        vectors = np.array(embedded)\\n        score = self._compute_score(vectors)\\n        return {\"score\": score}\\n\\n    def _evaluate_strings(\\n        self,\\n        *,\\n        prediction: str,\\n        reference: Optional[str] = None,\\n        callbacks: Callbacks = None,\\n        tags: Optional[List[str]] = None,\\n        metadata: Optional[Dict[str, Any]] = None,\\n        include_run_info: bool = False,\\n        **kwargs: Any,\\n    ) -> dict:\\n        \"\"\"Evaluate the embedding distance between a prediction and\\n        reference.', metadata={'source': 'test_repo/libs/langchain/langchain/evaluation/embedding_distance/base.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='Args:\\n            prediction (str): The output string from the first model.\\n            reference (str): The reference string (required)\\n            callbacks (Callbacks, optional): The callbacks to use.\\n            **kwargs (Any): Additional keyword arguments.\\n\\n        Returns:\\n            dict: A dictionary containing:\\n                - score: The embedding distance between the two\\n                    predictions.\\n        \"\"\"\\n        result = self(\\n            inputs={\"prediction\": prediction, \"reference\": reference},\\n            callbacks=callbacks,\\n            tags=tags,\\n            metadata=metadata,\\n            include_run_info=include_run_info,\\n        )\\n        return self._prepare_output(result)', metadata={'source': 'test_repo/libs/langchain/langchain/evaluation/embedding_distance/base.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='async def _aevaluate_strings(\\n        self,\\n        *,\\n        prediction: str,\\n        reference: Optional[str] = None,\\n        callbacks: Callbacks = None,\\n        tags: Optional[List[str]] = None,\\n        metadata: Optional[Dict[str, Any]] = None,\\n        include_run_info: bool = False,\\n        **kwargs: Any,\\n    ) -> dict:\\n        \"\"\"Asynchronously evaluate the embedding distance between\\n        a prediction and reference.\\n\\n        Args:\\n            prediction (str): The output string from the first model.\\n            reference (str): The output string from the second model.\\n            callbacks (Callbacks, optional): The callbacks to use.\\n            **kwargs (Any): Additional keyword arguments.', metadata={'source': 'test_repo/libs/langchain/langchain/evaluation/embedding_distance/base.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='Returns:\\n            dict: A dictionary containing:\\n                - score: The embedding distance between the two\\n                    predictions.\\n        \"\"\"\\n        result = await self.acall(\\n            inputs={\"prediction\": prediction, \"reference\": reference},\\n            callbacks=callbacks,\\n            tags=tags,\\n            metadata=metadata,\\n            include_run_info=include_run_info,\\n        )\\n        return self._prepare_output(result)', metadata={'source': 'test_repo/libs/langchain/langchain/evaluation/embedding_distance/base.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='class PairwiseEmbeddingDistanceEvalChain(\\n    _EmbeddingDistanceChainMixin, PairwiseStringEvaluator\\n):\\n    \"\"\"Use embedding distances to score semantic difference between two predictions.\\n\\n    Examples:\\n    >>> chain = PairwiseEmbeddingDistanceEvalChain()\\n    >>> result = chain.evaluate_string_pairs(prediction=\"Hello\", prediction_b=\"Hi\")\\n    >>> print(result)\\n    {\\'score\\': 0.5}\\n    \"\"\"\\n\\n    @property\\n    def input_keys(self) -> List[str]:\\n        \"\"\"Return the input keys of the chain.\\n\\n        Returns:\\n            List[str]: The input keys.\\n        \"\"\"\\n        return [\"prediction\", \"prediction_b\"]\\n\\n    @property\\n    def evaluation_name(self) -> str:\\n        return f\"pairwise_embedding_{self.distance_metric.value}_distance\"\\n\\n    def _call(\\n        self,\\n        inputs: Dict[str, Any],\\n        run_manager: Optional[CallbackManagerForChainRun] = None,\\n    ) -> Dict[str, Any]:\\n        \"\"\"Compute the score for two predictions.', metadata={'source': 'test_repo/libs/langchain/langchain/evaluation/embedding_distance/base.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='Args:\\n            inputs (Dict[str, Any]): The input data.\\n            run_manager (CallbackManagerForChainRun, optional):\\n                The callback manager.\\n\\n        Returns:\\n            Dict[str, Any]: The computed score.\\n        \"\"\"\\n        vectors = np.array(\\n            self.embeddings.embed_documents(\\n                [inputs[\"prediction\"], inputs[\"prediction_b\"]]\\n            )\\n        )\\n        score = self._compute_score(vectors)\\n        return {\"score\": score}\\n\\n    async def _acall(\\n        self,\\n        inputs: Dict[str, Any],\\n        run_manager: Optional[AsyncCallbackManagerForChainRun] = None,\\n    ) -> Dict[str, Any]:\\n        \"\"\"Asynchronously compute the score for two predictions.\\n\\n        Args:\\n            inputs (Dict[str, Any]): The input data.\\n            run_manager (AsyncCallbackManagerForChainRun, optional):\\n                The callback manager.', metadata={'source': 'test_repo/libs/langchain/langchain/evaluation/embedding_distance/base.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='Returns:\\n            Dict[str, Any]: The computed score.\\n        \"\"\"\\n        embedded = await self.embeddings.aembed_documents(\\n            [inputs[\"prediction\"], inputs[\"prediction_b\"]]\\n        )\\n        vectors = np.array(embedded)\\n        score = self._compute_score(vectors)\\n        return {\"score\": score}\\n\\n    def _evaluate_string_pairs(\\n        self,\\n        *,\\n        prediction: str,\\n        prediction_b: str,\\n        callbacks: Callbacks = None,\\n        tags: Optional[List[str]] = None,\\n        metadata: Optional[Dict[str, Any]] = None,\\n        include_run_info: bool = False,\\n        **kwargs: Any,\\n    ) -> dict:\\n        \"\"\"Evaluate the embedding distance between two predictions.', metadata={'source': 'test_repo/libs/langchain/langchain/evaluation/embedding_distance/base.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='Args:\\n            prediction (str): The output string from the first model.\\n            prediction_b (str): The output string from the second model.\\n            callbacks (Callbacks, optional): The callbacks to use.\\n            tags (List[str], optional): Tags to apply to traces\\n            metadata (Dict[str, Any], optional): metadata to apply to\\n            **kwargs (Any): Additional keyword arguments.\\n\\n        Returns:\\n            dict: A dictionary containing:\\n                - score: The embedding distance between the two\\n                    predictions.\\n        \"\"\"\\n        result = self(\\n            inputs={\"prediction\": prediction, \"prediction_b\": prediction_b},\\n            callbacks=callbacks,\\n            tags=tags,\\n            metadata=metadata,\\n            include_run_info=include_run_info,\\n        )\\n        return self._prepare_output(result)', metadata={'source': 'test_repo/libs/langchain/langchain/evaluation/embedding_distance/base.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='async def _aevaluate_string_pairs(\\n        self,\\n        *,\\n        prediction: str,\\n        prediction_b: str,\\n        callbacks: Callbacks = None,\\n        tags: Optional[List[str]] = None,\\n        metadata: Optional[Dict[str, Any]] = None,\\n        include_run_info: bool = False,\\n        **kwargs: Any,\\n    ) -> dict:\\n        \"\"\"Asynchronously evaluate the embedding distance\\n\\n        between two predictions.\\n\\n        Args:\\n            prediction (str): The output string from the first model.\\n            prediction_b (str): The output string from the second model.\\n            callbacks (Callbacks, optional): The callbacks to use.\\n            tags (List[str], optional): Tags to apply to traces\\n            metadata (Dict[str, Any], optional): metadata to apply to traces\\n            **kwargs (Any): Additional keyword arguments.', metadata={'source': 'test_repo/libs/langchain/langchain/evaluation/embedding_distance/base.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='Returns:\\n            dict: A dictionary containing:\\n                - score: The embedding distance between the two\\n                    predictions.\\n        \"\"\"\\n        result = await self.acall(\\n            inputs={\"prediction\": prediction, \"prediction_b\": prediction_b},\\n            callbacks=callbacks,\\n            tags=tags,\\n            metadata=metadata,\\n            include_run_info=include_run_info,\\n        )\\n        return self._prepare_output(result)', metadata={'source': 'test_repo/libs/langchain/langchain/evaluation/embedding_distance/base.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='\"\"\"Prompt for trajectory evaluation chain.\"\"\"\\n# flake8: noqa\\nfrom langchain_core.messages import HumanMessage, AIMessage, SystemMessage\\n\\nfrom langchain_core.prompts.chat import (\\n    ChatPromptTemplate,\\n    HumanMessagePromptTemplate,\\n)\\n\\n\\nEVAL_TEMPLATE = \"\"\"An AI language model has been given access to the following set of tools to help answer a user\\'s question.\\n\\nThe tools given to the AI model are:\\n[TOOL_DESCRIPTIONS]\\n{tool_descriptions}\\n[END_TOOL_DESCRIPTIONS]\\n\\nThe question the human asked the AI model was:\\n[QUESTION]\\n{question}\\n[END_QUESTION]{reference}\\n\\nThe AI language model decided to use the following set of tools to answer the question:\\n[AGENT_TRAJECTORY]\\n{agent_trajectory}\\n[END_AGENT_TRAJECTORY]\\n\\nThe AI language model\\'s final answer to the question was:\\n[RESPONSE]\\n{answer}\\n[END_RESPONSE]\\n\\nLet\\'s to do a detailed evaluation of the AI language model\\'s answer step by step.\\n\\nWe consider the following criteria before giving a score from 1 to 5:', metadata={'source': 'test_repo/libs/langchain/langchain/evaluation/agents/trajectory_eval_prompt.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='We consider the following criteria before giving a score from 1 to 5:\\n\\ni. Is the final answer helpful?\\nii. Does the AI language use a logical sequence of tools to answer the question?\\niii. Does the AI language model use the tools in a helpful way?\\niv. Does the AI language model use too many steps to answer the question?\\nv. Are the appropriate tools used to answer the question?\"\"\"\\n\\nEXAMPLE_INPUT = \"\"\"An AI language model has been given access to the following set of tools to help answer a user\\'s question.\\n\\nThe tools given to the AI model are:\\n[TOOL_DESCRIPTIONS]\\nTool 1:\\nName: Search\\nDescription: useful for when you need to ask with search\\n\\nTool 2:\\nName: Lookup\\nDescription: useful for when you need to ask with lookup\\n\\nTool 3:\\nName: Calculator\\nDescription: useful for doing calculations\\n\\nTool 4:\\nName: Search the Web (SerpAPI)\\nDescription: useful for when you need to answer questions about current events\\n[END_TOOL_DESCRIPTIONS]', metadata={'source': 'test_repo/libs/langchain/langchain/evaluation/agents/trajectory_eval_prompt.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content=\"The question the human asked the AI model was: If laid the Statue of Liberty end to end, how many times would it stretch across the United States?\\n\\nThe AI language model decided to use the following set of tools to answer the question:\\n[AGENT_TRAJECTORY]\\nStep 1:\\nTool used: Search the Web (SerpAPI)\\nTool input: If laid the Statue of Liberty end to end, how many times would it stretch across the United States?\\nTool output: The Statue of Liberty was given to the United States by France, as a symbol of the two countries' friendship. It was erected atop an American-designed ...\\n[END_AGENT_TRAJECTORY]\", metadata={'source': 'test_repo/libs/langchain/langchain/evaluation/agents/trajectory_eval_prompt.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='[RESPONSE]\\nThe AI language model\\'s final answer to the question was: There are different ways to measure the length of the United States, but if we use the distance between the Statue of Liberty and the westernmost point of the contiguous United States (Cape Alava, Washington), which is approximately 2,857 miles (4,596 km), and assume that the Statue of Liberty is 305 feet (93 meters) tall, then the statue would stretch across the United States approximately 17.5 times if laid end to end.\\n[END_RESPONSE]\\n\\nLet\\'s to do a detailed evaluation of the AI language model\\'s answer step by step.\\n\\nWe consider the following criteria before giving a score from 1 to 5:\\n\\ni. Is the final answer helpful?\\nii. Does the AI language use a logical sequence of tools to answer the question?\\niii. Does the AI language model use the tools in a helpful way?\\niv. Does the AI language model use too many steps to answer the question?\\nv. Are the appropriate tools used to answer the question?\"\"\"', metadata={'source': 'test_repo/libs/langchain/langchain/evaluation/agents/trajectory_eval_prompt.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='EXAMPLE_OUTPUT = \"\"\"First, let\\'s evaluate the final answer. The final uses good reasoning but is wrong. 2,857 divided by 305 is not 17.5.\\\\\\nThe model should have used the calculator to figure this out. Second does the model use a logical sequence of tools to answer the question?\\\\\\nThe way model uses the search is not helpful. The model should have used the search tool to figure the width of the US or the height of the statue.\\\\\\nThe model didn\\'t use the calculator tool and gave an incorrect answer. The search API should be used for current events or specific questions.\\\\\\nThe tools were not used in a helpful way. The model did not use too many steps to answer the question.\\\\\\nThe model did not use the appropriate tools to answer the question.\\\\\\n    \\nJudgment: Given the good reasoning in the final answer but otherwise poor performance, we give the model a score of 2.\\n\\nScore: 2\"\"\"', metadata={'source': 'test_repo/libs/langchain/langchain/evaluation/agents/trajectory_eval_prompt.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='Score: 2\"\"\"\\n\\nEVAL_CHAT_PROMPT = ChatPromptTemplate.from_messages(\\n    messages=[\\n        SystemMessage(\\n            content=\"You are a helpful assistant that evaluates language models.\"\\n        ),\\n        HumanMessage(content=EXAMPLE_INPUT),\\n        AIMessage(content=EXAMPLE_OUTPUT),\\n        HumanMessagePromptTemplate.from_template(EVAL_TEMPLATE),\\n    ]\\n)\\n\\n\\nTOOL_FREE_EVAL_TEMPLATE = \"\"\"An AI language model has been given access to a set of tools to help answer a user\\'s question.\\n\\nThe question the human asked the AI model was:\\n[QUESTION]\\n{question}\\n[END_QUESTION]{reference}\\n\\nThe AI language model decided to use the following set of tools to answer the question:\\n[AGENT_TRAJECTORY]\\n{agent_trajectory}\\n[END_AGENT_TRAJECTORY]\\n\\nThe AI language model\\'s final answer to the question was:\\n[RESPONSE]\\n{answer}\\n[END_RESPONSE]\\n\\nLet\\'s to do a detailed evaluation of the AI language model\\'s answer step by step.\\n\\nWe consider the following criteria before giving a score from 1 to 5:', metadata={'source': 'test_repo/libs/langchain/langchain/evaluation/agents/trajectory_eval_prompt.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='We consider the following criteria before giving a score from 1 to 5:\\n\\ni. Is the final answer helpful?\\nii. Does the AI language use a logical sequence of tools to answer the question?\\niii. Does the AI language model use the tools in a helpful way?\\niv. Does the AI language model use too many steps to answer the question?\\nv. Are the appropriate tools used to answer the question?\"\"\"\\n\\n\\nTOOL_FREE_EVAL_CHAT_PROMPT = ChatPromptTemplate.from_messages(\\n    messages=[\\n        SystemMessage(\\n            content=\"You are a helpful assistant that evaluates language models.\"\\n        ),\\n        HumanMessage(content=EXAMPLE_INPUT),\\n        AIMessage(content=EXAMPLE_OUTPUT),\\n        HumanMessagePromptTemplate.from_template(TOOL_FREE_EVAL_TEMPLATE),\\n    ]\\n)', metadata={'source': 'test_repo/libs/langchain/langchain/evaluation/agents/trajectory_eval_prompt.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='\"\"\"Chains for evaluating ReAct style agents.\"\"\"\\nfrom langchain.evaluation.agents.trajectory_eval_chain import TrajectoryEvalChain\\n\\n__all__ = [\"TrajectoryEvalChain\"]', metadata={'source': 'test_repo/libs/langchain/langchain/evaluation/agents/__init__.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='\"\"\"A chain for evaluating ReAct style agents.\\n\\nThis chain is used to evaluate ReAct style agents by reasoning about\\nthe sequence of actions taken and their outcomes. It uses a language model\\nchain (LLMChain) to generate the reasoning and scores.\\n\"\"\"\\n\\nimport re\\nfrom typing import (\\n    Any,\\n    Dict,\\n    List,\\n    Optional,\\n    Sequence,\\n    Tuple,\\n    TypedDict,\\n    Union,\\n    cast,\\n)\\n\\nfrom langchain_core.agents import AgentAction\\nfrom langchain_core.exceptions import OutputParserException\\nfrom langchain_core.language_models import BaseLanguageModel\\nfrom langchain_core.language_models.chat_models import BaseChatModel\\nfrom langchain_core.output_parsers import BaseOutputParser\\nfrom langchain_core.pydantic_v1 import Extra, Field\\nfrom langchain_core.tools import BaseTool', metadata={'source': 'test_repo/libs/langchain/langchain/evaluation/agents/trajectory_eval_chain.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from langchain.callbacks.manager import (\\n    AsyncCallbackManagerForChainRun,\\n    CallbackManagerForChainRun,\\n    Callbacks,\\n)\\nfrom langchain.chains.llm import LLMChain\\nfrom langchain.evaluation.agents.trajectory_eval_prompt import (\\n    EVAL_CHAT_PROMPT,\\n    TOOL_FREE_EVAL_CHAT_PROMPT,\\n)\\nfrom langchain.evaluation.schema import AgentTrajectoryEvaluator, LLMEvalChain', metadata={'source': 'test_repo/libs/langchain/langchain/evaluation/agents/trajectory_eval_chain.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='class TrajectoryEval(TypedDict):\\n    \"\"\"A named tuple containing the score and reasoning for a trajectory.\"\"\"\\n\\n    score: float\\n    \"\"\"The score for the trajectory, normalized from 0 to 1.\"\"\"\\n    reasoning: str\\n    \"\"\"The reasoning for the score.\"\"\"', metadata={'source': 'test_repo/libs/langchain/langchain/evaluation/agents/trajectory_eval_chain.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='class TrajectoryOutputParser(BaseOutputParser):\\n    \"\"\"Trajectory output parser.\"\"\"\\n\\n    @property\\n    def _type(self) -> str:\\n        return \"agent_trajectory\"\\n\\n    def parse(self, text: str) -> TrajectoryEval:\\n        \"\"\"Parse the output text and extract the score and reasoning.\\n\\n        Args:\\n            text (str): The output text to parse.\\n\\n        Returns:\\n            TrajectoryEval: A named tuple containing the normalized score and reasoning.\\n\\n        Raises:\\n            OutputParserException: If the score is not found in the output text or\\n                if the LLM\\'s score is not a digit in the range 1-5.\\n        \"\"\"\\n        if \"Score:\" not in text:\\n            raise OutputParserException(\\n                f\"Could not find score in model eval output: {text}\"\\n            )\\n\\n        reasoning, score_str = text.split(\"Score: \", maxsplit=1)\\n\\n        reasoning, score_str = reasoning.strip(), score_str.strip()', metadata={'source': 'test_repo/libs/langchain/langchain/evaluation/agents/trajectory_eval_chain.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='# Use regex to extract the score.\\n        # This will get the number in the string, even if it is a float or more than 10.\\n        # E.g. \"Score: 1\" will return 1, \"Score: 3.5\" will return 3.5, and\\n        # \"Score: 10\" will return 10.\\n        # The score should be an integer digit in the range 1-5.\\n        _score = re.search(r\"(\\\\d+(\\\\.\\\\d+)?)\", score_str)\\n        # If the score is not found or is a float, raise an exception.\\n        if _score is None or \".\" in _score.group(1):\\n            raise OutputParserException(\\n                f\"Score is not an integer digit in the range 1-5: {text}\"\\n            )\\n        score = int(_score.group(1))\\n        # If the score is not in the range 1-5, raise an exception.\\n        if not 1 <= score <= 5:\\n            raise OutputParserException(\\n                f\"Score is not a digit in the range 1-5: {text}\"\\n            )\\n        normalized_score = (score - 1) / 4\\n        return TrajectoryEval(score=normalized_score, reasoning=reasoning)', metadata={'source': 'test_repo/libs/langchain/langchain/evaluation/agents/trajectory_eval_chain.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='class TrajectoryEvalChain(AgentTrajectoryEvaluator, LLMEvalChain):\\n    \"\"\"A chain for evaluating ReAct style agents.\\n\\n    This chain is used to evaluate ReAct style agents by reasoning about\\n    the sequence of actions taken and their outcomes.\\n\\n    Example:\\n\\n    .. code-block:: python\\n\\n        from langchain.agents import AgentType, initialize_agent\\n        from langchain_community.chat_models import ChatOpenAI\\n        from langchain.evaluation import TrajectoryEvalChain\\n        from langchain.tools import tool\\n\\n        @tool\\n        def geography_answers(country: str, question: str) -> str:\\n            \\\\\"\\\\\"\\\\\"Very helpful answers to geography questions.\\\\\"\\\\\"\\\\\"\\n            return f\"{country}? IDK - We may never know {question}.\"\\n\\n        llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\\n        agent = initialize_agent(\\n            tools=[geography_answers],\\n            llm=llm,\\n            agent=AgentType.OPENAI_FUNCTIONS,\\n            return_intermediate_steps=True,\\n        )', metadata={'source': 'test_repo/libs/langchain/langchain/evaluation/agents/trajectory_eval_chain.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='question = \"How many dwell in the largest minor region in Argentina?\"\\n        response = agent(question)\\n\\n        eval_chain = TrajectoryEvalChain.from_llm(\\n            llm=llm, agent_tools=[geography_answers], return_reasoning=True\\n        )\\n\\n        result = eval_chain.evaluate_agent_trajectory(\\n            input=question,\\n            agent_trajectory=response[\"intermediate_steps\"],\\n            prediction=response[\"output\"],\\n            reference=\"Paris\",\\n        )\\n        print(result[\"score\"])  # noqa: T201\\n        # 0\\n    \"\"\"  # noqa: E501', metadata={'source': 'test_repo/libs/langchain/langchain/evaluation/agents/trajectory_eval_chain.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='agent_tools: Optional[List[BaseTool]] = None\\n    \"\"\"A list of tools available to the agent.\"\"\"\\n    eval_chain: LLMChain\\n    \"\"\"The language model chain used for evaluation.\"\"\"\\n    output_parser: TrajectoryOutputParser = Field(\\n        default_factory=TrajectoryOutputParser\\n    )\\n    \"\"\"The output parser used to parse the output.\"\"\"\\n    return_reasoning: bool = False  # :meta private:\\n    \"\"\"DEPRECATED. Reasoning always returned.\"\"\"\\n\\n    class Config:\\n        \"\"\"Configuration for the QAEvalChain.\"\"\"\\n\\n        extra = Extra.ignore\\n\\n    @property\\n    def requires_reference(self) -> bool:\\n        \"\"\"Whether this evaluator requires a reference label.\"\"\"\\n        return False\\n\\n    @property\\n    def _tools_description(self) -> str:\\n        \"\"\"Get the description of the agent tools.', metadata={'source': 'test_repo/libs/langchain/langchain/evaluation/agents/trajectory_eval_chain.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='Returns:\\n            str: The description of the agent tools.\\n        \"\"\"\\n        if self.agent_tools is None:\\n            return \"\"\\n        return \"\\\\n\\\\n\".join(\\n            [\\n                f\"\"\"Tool {i}: {tool.name}\\nDescription: {tool.description}\"\"\"\\n                for i, tool in enumerate(self.agent_tools, 1)\\n            ]\\n        )\\n\\n    @staticmethod\\n    def get_agent_trajectory(\\n        steps: Union[str, Sequence[Tuple[AgentAction, str]]],\\n    ) -> str:\\n        \"\"\"Get the agent trajectory as a formatted string.\\n\\n        Args:\\n            steps (Union[str, List[Tuple[AgentAction, str]]]): The agent trajectory.\\n\\n        Returns:\\n            str: The formatted agent trajectory.\\n        \"\"\"\\n        if isinstance(steps, str):\\n            return steps', metadata={'source': 'test_repo/libs/langchain/langchain/evaluation/agents/trajectory_eval_chain.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='return \"\\\\n\\\\n\".join(\\n            [\\n                f\"\"\"Step {i}:\\nTool used: {action.tool}\\nTool input: {action.tool_input}\\nTool output: {output}\"\"\"\\n                for i, (action, output) in enumerate(steps, 1)\\n            ]\\n        )\\n\\n    @staticmethod\\n    def _format_reference(reference: Optional[str]) -> str:\\n        \"\"\"Format the reference text.\\n\\n        Args:\\n            reference (str): The reference text.\\n\\n        Returns:\\n            str: The formatted reference text.\\n        \"\"\"\\n        if not reference:\\n            return \"\"\\n        return f\"\"\"\\n\\nThe following is the expected answer. Use this to measure correctness:\\n[GROUND_TRUTH]\\n{reference}\\n[END_GROUND_TRUTH]\\n\"\"\"', metadata={'source': 'test_repo/libs/langchain/langchain/evaluation/agents/trajectory_eval_chain.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='@classmethod\\n    def from_llm(\\n        cls,\\n        llm: BaseLanguageModel,\\n        agent_tools: Optional[Sequence[BaseTool]] = None,\\n        output_parser: Optional[TrajectoryOutputParser] = None,\\n        **kwargs: Any,\\n    ) -> \"TrajectoryEvalChain\":\\n        \"\"\"Create a TrajectoryEvalChain object from a language model chain.', metadata={'source': 'test_repo/libs/langchain/langchain/evaluation/agents/trajectory_eval_chain.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='Args:\\n            llm (BaseChatModel): The language model chain.\\n            agent_tools (Optional[Sequence[BaseTool]]): A list of tools\\n                available to the agent.\\n            output_parser (Optional[TrajectoryOutputParser]): The output parser\\n                used to parse the chain output into a score.\\n        Returns:\\n            TrajectoryEvalChain: The TrajectoryEvalChain object.\\n        \"\"\"\\n        if not isinstance(llm, BaseChatModel):\\n            raise NotImplementedError(\\n                \"Only chat models supported by the current trajectory eval\"\\n            )\\n        if agent_tools:\\n            prompt = EVAL_CHAT_PROMPT\\n        else:\\n            prompt = TOOL_FREE_EVAL_CHAT_PROMPT\\n        eval_chain = LLMChain(llm=llm, prompt=prompt)\\n        return cls(\\n            agent_tools=agent_tools,\\n            eval_chain=eval_chain,\\n            output_parser=output_parser or TrajectoryOutputParser(),\\n            **kwargs,\\n        )', metadata={'source': 'test_repo/libs/langchain/langchain/evaluation/agents/trajectory_eval_chain.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='@property\\n    def input_keys(self) -> List[str]:\\n        \"\"\"Get the input keys for the chain.\\n\\n        Returns:\\n            List[str]: The input keys.\\n        \"\"\"\\n        return [\"question\", \"agent_trajectory\", \"answer\", \"reference\"]\\n\\n    @property\\n    def output_keys(self) -> List[str]:\\n        \"\"\"Get the output keys for the chain.\\n\\n        Returns:\\n            List[str]: The output keys.\\n        \"\"\"\\n        return [\"score\", \"reasoning\"]\\n\\n    def prep_inputs(self, inputs: Union[Dict[str, Any], Any]) -> Dict[str, str]:\\n        \"\"\"Validate and prep inputs.\"\"\"\\n        if \"reference\" not in inputs:\\n            inputs[\"reference\"] = self._format_reference(inputs.get(\"reference\"))\\n        return super().prep_inputs(inputs)\\n\\n    def _call(\\n        self,\\n        inputs: Dict[str, str],\\n        run_manager: Optional[CallbackManagerForChainRun] = None,\\n    ) -> Dict[str, Any]:\\n        \"\"\"Run the chain and generate the output.', metadata={'source': 'test_repo/libs/langchain/langchain/evaluation/agents/trajectory_eval_chain.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='Args:\\n            inputs (Dict[str, str]): The input values for the chain.\\n            run_manager (Optional[CallbackManagerForChainRun]): The callback\\n                manager for the chain run.\\n\\n        Returns:\\n            Dict[str, Any]: The output values of the chain.\\n        \"\"\"\\n        chain_input = {**inputs}\\n        if self.agent_tools:\\n            chain_input[\"tool_descriptions\"] = self._tools_description\\n        _run_manager = run_manager or CallbackManagerForChainRun.get_noop_manager()\\n        raw_output = self.eval_chain.run(\\n            chain_input, callbacks=_run_manager.get_child()\\n        )\\n        return cast(dict, self.output_parser.parse(raw_output))\\n\\n    async def _acall(\\n        self,\\n        inputs: Dict[str, str],\\n        run_manager: Optional[AsyncCallbackManagerForChainRun] = None,\\n    ) -> Dict[str, Any]:\\n        \"\"\"Run the chain and generate the output.', metadata={'source': 'test_repo/libs/langchain/langchain/evaluation/agents/trajectory_eval_chain.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='Args:\\n            inputs (Dict[str, str]): The input values for the chain.\\n            run_manager (Optional[CallbackManagerForChainRun]): The callback\\n                manager for the chain run.\\n\\n        Returns:\\n            Dict[str, Any]: The output values of the chain.\\n        \"\"\"\\n        chain_input = {**inputs}\\n        if self.agent_tools:\\n            chain_input[\"tool_descriptions\"] = self._tools_description\\n        _run_manager = run_manager or AsyncCallbackManagerForChainRun.get_noop_manager()\\n        raw_output = await self.eval_chain.arun(\\n            chain_input, callbacks=_run_manager.get_child()\\n        )\\n        return cast(dict, self.output_parser.parse(raw_output))', metadata={'source': 'test_repo/libs/langchain/langchain/evaluation/agents/trajectory_eval_chain.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='def _evaluate_agent_trajectory(\\n        self,\\n        *,\\n        prediction: str,\\n        input: str,\\n        agent_trajectory: Sequence[Tuple[AgentAction, str]],\\n        reference: Optional[str] = None,\\n        callbacks: Callbacks = None,\\n        tags: Optional[List[str]] = None,\\n        metadata: Optional[Dict[str, Any]] = None,\\n        include_run_info: bool = False,\\n        **kwargs: Any,\\n    ) -> dict:\\n        \"\"\"Evaluate a trajectory.\\n\\n        Args:\\n            prediction (str): The final predicted response.\\n            input (str): The input to the agent.\\n            agent_trajectory (List[Tuple[AgentAction, str]]):\\n                The intermediate steps forming the agent trajectory.\\n            reference (Optional[str]): The reference answer.\\n            callbacks (Callbacks): Callbacks to use for this chain run.', metadata={'source': 'test_repo/libs/langchain/langchain/evaluation/agents/trajectory_eval_chain.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='Returns:\\n            dict: The evaluation result, which includes the score and optionally\\n                the reasoning for reaching that.\\n        \"\"\"\\n        inputs = {\\n            \"question\": input,\\n            \"agent_trajectory\": self.get_agent_trajectory(agent_trajectory),\\n            \"answer\": prediction,\\n            \"reference\": reference,\\n        }\\n        return self.__call__(\\n            inputs=inputs,\\n            callbacks=callbacks,\\n            tags=tags,\\n            metadata=metadata,\\n            include_run_info=include_run_info,\\n            return_only_outputs=True,\\n        )', metadata={'source': 'test_repo/libs/langchain/langchain/evaluation/agents/trajectory_eval_chain.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='async def _aevaluate_agent_trajectory(\\n        self,\\n        *,\\n        prediction: str,\\n        input: str,\\n        agent_trajectory: Sequence[Tuple[AgentAction, str]],\\n        reference: Optional[str] = None,\\n        callbacks: Callbacks = None,\\n        tags: Optional[List[str]] = None,\\n        metadata: Optional[Dict[str, Any]] = None,\\n        include_run_info: bool = False,\\n        **kwargs: Any,\\n    ) -> dict:\\n        \"\"\"Asynchronously evaluate a trajectory.\\n\\n        Args:\\n            prediction (str): The final predicted response.\\n            input (str): The input to the agent.\\n            agent_trajectory (List[Tuple[AgentAction, str]]):\\n                The intermediate steps forming the agent trajectory.\\n            reference (Optional[str]): The reference answer.\\n            callbacks (Callbacks): Callbacks to use for this chain run.', metadata={'source': 'test_repo/libs/langchain/langchain/evaluation/agents/trajectory_eval_chain.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='Returns:\\n            dict: The evaluation result, which includes the score and optionally\\n                the reasoning for reaching that.\\n        \"\"\"\\n        inputs = {\\n            \"question\": input,\\n            \"agent_trajectory\": self.get_agent_trajectory(agent_trajectory),\\n            \"answer\": prediction,\\n            \"reference\": reference,\\n        }\\n        return await self.acall(\\n            inputs=inputs,\\n            callbacks=callbacks,\\n            tags=tags,\\n            metadata=metadata,\\n            include_run_info=include_run_info,\\n            return_only_outputs=True,\\n        )', metadata={'source': 'test_repo/libs/langchain/langchain/evaluation/agents/trajectory_eval_chain.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from typing import Any, Union\\n\\nfrom langchain.evaluation.schema import StringEvaluator\\nfrom langchain.output_parsers.json import parse_json_markdown', metadata={'source': 'test_repo/libs/langchain/langchain/evaluation/parsing/json_schema.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='class JsonSchemaEvaluator(StringEvaluator):\\n    \"\"\"An evaluator that validates a JSON prediction against a JSON schema reference.\\n\\n    This evaluator checks if a given JSON prediction conforms to the provided JSON schema.\\n    If the prediction is valid, the score is True (no errors). Otherwise, the score is False (error occurred).\\n\\n    Attributes:\\n        requires_input (bool): Whether the evaluator requires input.\\n        requires_reference (bool): Whether the evaluator requires reference.\\n        evaluation_name (str): The name of the evaluation.\\n\\n    Examples:\\n        evaluator = JsonSchemaEvaluator()\\n        result = evaluator.evaluate_strings(\\n            prediction=\\'{\"name\": \"John\", \"age\": 30}\\',\\n            reference={\\n                \"type\": \"object\",\\n                \"properties\": {\\n                    \"name\": {\"type\": \"string\"},\\n                    \"age\": {\"type\": \"integer\"}\\n                }\\n            }\\n        )\\n        assert result[\"score\"] is not None', metadata={'source': 'test_repo/libs/langchain/langchain/evaluation/parsing/json_schema.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='\"\"\"  # noqa: E501\\n\\n    def __init__(self, **kwargs: Any) -> None:\\n        \"\"\"Initializes the JsonSchemaEvaluator.\\n\\n        Args:\\n            **kwargs: Additional keyword arguments.\\n\\n        Raises:\\n            ImportError: If the jsonschema package is not installed.\\n        \"\"\"\\n        super().__init__()\\n        try:\\n            import jsonschema  # noqa: F401\\n        except ImportError:\\n            raise ImportError(\\n                \"The JsonSchemaEvaluator requires the jsonschema package.\"\\n                \" Please install it with `pip install jsonschema`.\"\\n            )\\n\\n    @property\\n    def requires_input(self) -> bool:\\n        \"\"\"Returns whether the evaluator requires input.\"\"\"\\n        return False\\n\\n    @property\\n    def requires_reference(self) -> bool:\\n        \"\"\"Returns whether the evaluator requires reference.\"\"\"\\n        return True', metadata={'source': 'test_repo/libs/langchain/langchain/evaluation/parsing/json_schema.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='@property\\n    def evaluation_name(self) -> str:\\n        \"\"\"Returns the name of the evaluation.\"\"\"\\n        return \"json_schema_validation\"\\n\\n    def _parse_json(self, node: Any) -> Union[dict, list, None, float, bool, int, str]:\\n        if isinstance(node, str):\\n            return parse_json_markdown(node)\\n        elif hasattr(node, \"schema\") and callable(getattr(node, \"schema\")):\\n            # Pydantic model\\n            return getattr(node, \"schema\")()\\n        return node\\n\\n    def _validate(self, prediction: Any, schema: Any) -> dict:\\n        from jsonschema import ValidationError, validate  # noqa: F401\\n\\n        try:\\n            validate(instance=prediction, schema=schema)\\n            return {\\n                \"score\": True,\\n            }\\n        except ValidationError as e:\\n            return {\"score\": False, \"reasoning\": repr(e)}', metadata={'source': 'test_repo/libs/langchain/langchain/evaluation/parsing/json_schema.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='def _evaluate_strings(\\n        self,\\n        prediction: Union[str, Any],\\n        input: Union[str, Any] = None,\\n        reference: Union[str, Any] = None,\\n        **kwargs: Any,\\n    ) -> dict:\\n        parsed_prediction = self._parse_json(prediction)\\n        schema = self._parse_json(reference)\\n        return self._validate(parsed_prediction, schema)', metadata={'source': 'test_repo/libs/langchain/langchain/evaluation/parsing/json_schema.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='import json\\nfrom typing import Any, Callable, Optional, Union\\n\\nfrom langchain.evaluation.schema import StringEvaluator\\nfrom langchain.output_parsers.json import parse_json_markdown', metadata={'source': 'test_repo/libs/langchain/langchain/evaluation/parsing/json_distance.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='class JsonEditDistanceEvaluator(StringEvaluator):\\n    \"\"\"\\n    An evaluator that calculates the edit distance between JSON strings.\\n\\n    This evaluator computes a normalized Damerau-Levenshtein distance between two JSON strings\\n    after parsing them and converting them to a canonical format (i.e., whitespace and key order are normalized).\\n    It can be customized with alternative distance and canonicalization functions.\\n\\n    Args:\\n        string_distance (Optional[Callable[[str, str], float]]): A callable that computes the distance between two strings.\\n            If not provided, a Damerau-Levenshtein distance from the `rapidfuzz` package will be used.\\n        canonicalize (Optional[Callable[[Any], Any]]): A callable that converts a parsed JSON object into its canonical string form.\\n            If not provided, the default behavior is to serialize the JSON with sorted keys and no extra whitespace.\\n        **kwargs (Any): Additional keyword arguments.', metadata={'source': 'test_repo/libs/langchain/langchain/evaluation/parsing/json_distance.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='Attributes:\\n        _string_distance (Callable[[str, str], float]): The internal distance computation function.\\n        _canonicalize (Callable[[Any], Any]): The internal canonicalization function.\\n\\n    Examples:\\n        >>> evaluator = JsonEditDistanceEvaluator()\\n        >>> result = evaluator.evaluate_strings(prediction=\\'{\"a\": 1, \"b\": 2}\\', reference=\\'{\"a\": 1, \"b\": 3}\\')\\n        >>> assert result[\"score\"] is not None\\n\\n    Raises:\\n        ImportError: If `rapidfuzz` is not installed and no alternative `string_distance` function is provided.\\n\\n    \"\"\"  # noqa: E501', metadata={'source': 'test_repo/libs/langchain/langchain/evaluation/parsing/json_distance.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='def __init__(\\n        self,\\n        string_distance: Optional[Callable[[str, str], float]] = None,\\n        canonicalize: Optional[Callable[[Any], Any]] = None,\\n        **kwargs: Any,\\n    ) -> None:\\n        super().__init__()\\n        if string_distance is not None:\\n            self._string_distance = string_distance\\n        else:\\n            try:\\n                from rapidfuzz import distance as rfd  # noqa: F401\\n            except ImportError:\\n                raise ImportError(\\n                    \"The default string_distance operator for the \"\\n                    \" JsonEditDistanceEvaluator requires installation of \"\\n                    \"the rapidfuzz package. \"\\n                    \"Please install it with `pip install rapidfuzz`.\"\\n                )\\n            self._string_distance = rfd.DamerauLevenshtein.normalized_distance\\n        if canonicalize is not None:\\n            self._canonicalize = canonicalize\\n        else:\\n            self._canonicalize = lambda x: json.dumps(', metadata={'source': 'test_repo/libs/langchain/langchain/evaluation/parsing/json_distance.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='else:\\n            self._canonicalize = lambda x: json.dumps(\\n                x,\\n                separators=(\",\", \":\"),\\n                sort_keys=True,  # eliminate whitespace\\n            )', metadata={'source': 'test_repo/libs/langchain/langchain/evaluation/parsing/json_distance.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='@property\\n    def requires_input(self) -> bool:\\n        return False\\n\\n    @property\\n    def requires_reference(self) -> bool:\\n        return True\\n\\n    @property\\n    def evaluation_name(self) -> str:\\n        return \"json_edit_distance\"\\n\\n    def _parse_json(self, node: Any) -> Union[dict, list, None, float, bool, int, str]:\\n        if isinstance(node, str):\\n            return parse_json_markdown(node)\\n        return node\\n\\n    def _evaluate_strings(\\n        self,\\n        prediction: str,\\n        input: Optional[str] = None,\\n        reference: Optional[str] = None,\\n        **kwargs: Any,\\n    ) -> dict:\\n        parsed = self._canonicalize(self._parse_json(prediction))\\n        label = self._canonicalize(self._parse_json(reference))\\n        distance = self._string_distance(parsed, label)\\n        return {\"score\": distance}', metadata={'source': 'test_repo/libs/langchain/langchain/evaluation/parsing/json_distance.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='\"\"\"Evaluators for parsing strings.\"\"\"\\nimport json\\nfrom operator import eq\\nfrom typing import Any, Callable, Optional, Union, cast\\n\\nfrom langchain.evaluation.schema import StringEvaluator\\nfrom langchain.output_parsers.json import parse_json_markdown', metadata={'source': 'test_repo/libs/langchain/langchain/evaluation/parsing/base.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='class JsonValidityEvaluator(StringEvaluator):\\n    \"\"\"Evaluates whether the prediction is valid JSON.\\n\\n    This evaluator checks if the prediction is a valid JSON string. It does not\\n        require any input or reference.\\n\\n    Attributes:\\n        requires_input (bool): Whether this evaluator requires an input\\n            string. Always False.\\n        requires_reference (bool): Whether this evaluator requires a\\n            reference string. Always False.\\n        evaluation_name (str): The name of the evaluation metric.\\n            Always \"json\".\\n\\n    Examples:\\n        >>> evaluator = JsonValidityEvaluator()\\n        >>> prediction = \\'{\"name\": \"John\", \"age\": 30, \"city\": \"New York\"}\\'\\n        >>> evaluator.evaluate(prediction)\\n        {\\'score\\': 1}\\n\\n        >>> prediction = \\'{\"name\": \"John\", \"age\": 30, \"city\": \"New York\",}\\'\\n        >>> evaluator.evaluate(prediction)\\n        {\\'score\\': 0, \\'reasoning\\': \\'Expecting property name enclosed in double quotes\\'}\\n    \"\"\"', metadata={'source': 'test_repo/libs/langchain/langchain/evaluation/parsing/base.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='def __init__(self, **kwargs: Any) -> None:\\n        super().__init__()\\n\\n    @property\\n    def requires_input(self) -> bool:\\n        return False\\n\\n    @property\\n    def requires_reference(self) -> bool:\\n        return False\\n\\n    @property\\n    def evaluation_name(self) -> str:\\n        return \"json_validity\"\\n\\n    def _evaluate_strings(\\n        self,\\n        prediction: str,\\n        input: Optional[str] = None,\\n        reference: Optional[str] = None,\\n        **kwargs: Any,\\n    ) -> dict:\\n        \"\"\"Evaluate the prediction string.\\n\\n        Args:\\n            prediction (str): The prediction string to evaluate.\\n            input (str, optional): Not used in this evaluator. Defaults to None.\\n            reference (str, optional): Not used in this evaluator. Defaults to None.', metadata={'source': 'test_repo/libs/langchain/langchain/evaluation/parsing/base.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='Returns:\\n            dict: A dictionary containing the evaluation score. The score is 1 if\\n            the prediction is valid JSON, and 0 otherwise.\\n                If the prediction is not valid JSON, the dictionary also contains\\n                a \"reasoning\" field with the error message.\\n\\n        \"\"\"\\n        try:\\n            parse_json_markdown(prediction, parser=json.loads)\\n            return {\"score\": 1}\\n        except Exception as e:\\n            return {\"score\": 0, \"reasoning\": str(e)}', metadata={'source': 'test_repo/libs/langchain/langchain/evaluation/parsing/base.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='class JsonEqualityEvaluator(StringEvaluator):\\n    \"\"\"Evaluates whether the prediction is equal to the reference after\\n        parsing both as JSON.\\n\\n    This evaluator checks if the prediction, after parsing as JSON, is equal\\n        to the reference,\\n    which is also parsed as JSON. It does not require an input string.\\n\\n    Attributes:\\n        requires_input (bool): Whether this evaluator requires an\\n            input string. Always False.\\n        requires_reference (bool): Whether this evaluator requires\\n            a reference string. Always True.\\n        evaluation_name (str): The name of the evaluation metric.\\n            Always \"parsed_equality\".\\n\\n    Examples:\\n        >>> evaluator = JsonEqualityEvaluator()\\n        >>> evaluator.evaluate_strings(\\'{\"a\": 1}\\', reference=\\'{\"a\": 1}\\')\\n        {\\'score\\': True}\\n        >>> evaluator.evaluate_strings(\\'{\"a\": 1}\\', reference=\\'{\"a\": 2}\\')\\n        {\\'score\\': False}', metadata={'source': 'test_repo/libs/langchain/langchain/evaluation/parsing/base.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='>>> evaluator = JsonEqualityEvaluator(operator=lambda x, y: x[\\'a\\'] == y[\\'a\\'])\\n        >>> evaluator.evaluate_strings(\\'{\"a\": 1}\\', reference=\\'{\"a\": 1}\\')\\n        {\\'score\\': True}\\n        >>> evaluator.evaluate_strings(\\'{\"a\": 1}\\', reference=\\'{\"a\": 2}\\')\\n        {\\'score\\': False}\\n\\n    \"\"\"\\n\\n    def __init__(self, operator: Optional[Callable] = None, **kwargs: Any) -> None:\\n        super().__init__()\\n        self.operator = operator or eq\\n\\n    @property\\n    def requires_input(self) -> bool:\\n        return False\\n\\n    @property\\n    def requires_reference(self) -> bool:\\n        return True\\n\\n    @property\\n    def evaluation_name(self) -> str:\\n        return \"json_equality\"\\n\\n    def _parse_json(\\n        self,\\n        string: Any,\\n    ) -> Union[dict, list, None, float, bool, int, str]:\\n        if isinstance(string, str):\\n            return parse_json_markdown(string)\\n        return string', metadata={'source': 'test_repo/libs/langchain/langchain/evaluation/parsing/base.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='def _evaluate_strings(\\n        self,\\n        prediction: str,\\n        input: Optional[str] = None,\\n        reference: Optional[str] = None,\\n        **kwargs: Any,\\n    ) -> dict:\\n        \"\"\"Evaluate the prediction string.\\n\\n        Args:\\n            prediction (str): The prediction string to evaluate.\\n            input (str, optional): Not used in this evaluator.\\n            reference (str): The reference string to compare against.\\n\\n        Returns:\\n            dict: A dictionary containing the evaluation score.\\n        \"\"\"\\n        parsed = self._parse_json(prediction)\\n        label = self._parse_json(cast(str, reference))\\n        if isinstance(label, list):\\n            if not isinstance(parsed, list):\\n                return {\"score\": 0}\\n            parsed = sorted(parsed, key=lambda x: str(x))\\n            label = sorted(label, key=lambda x: str(x))\\n        return {\"score\": self.operator(parsed, label)}', metadata={'source': 'test_repo/libs/langchain/langchain/evaluation/parsing/base.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from langchain_community.vectorstores.sklearn import (\\n    BaseSerializer,\\n    BsonSerializer,\\n    JsonSerializer,\\n    ParquetSerializer,\\n    SKLearnVectorStore,\\n    SKLearnVectorStoreException,\\n)\\n\\n__all__ = [\\n    \"BaseSerializer\",\\n    \"JsonSerializer\",\\n    \"BsonSerializer\",\\n    \"ParquetSerializer\",\\n    \"SKLearnVectorStoreException\",\\n    \"SKLearnVectorStore\",\\n]', metadata={'source': 'test_repo/libs/langchain/langchain/vectorstores/sklearn.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from langchain_community.vectorstores.myscale import (\\n    MyScale,\\n    MyScaleSettings,\\n    MyScaleWithoutJSON,\\n)\\n\\n__all__ = [\"MyScaleSettings\", \"MyScale\", \"MyScaleWithoutJSON\"]', metadata={'source': 'test_repo/libs/langchain/langchain/vectorstores/myscale.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from langchain_community.vectorstores.typesense import Typesense\\n\\n__all__ = [\"Typesense\"]', metadata={'source': 'test_repo/libs/langchain/langchain/vectorstores/typesense.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from langchain_community.vectorstores.elasticsearch import (\\n    ApproxRetrievalStrategy,\\n    BaseRetrievalStrategy,\\n    ElasticsearchStore,\\n    ExactRetrievalStrategy,\\n    SparseRetrievalStrategy,\\n)\\n\\n__all__ = [\\n    \"BaseRetrievalStrategy\",\\n    \"ApproxRetrievalStrategy\",\\n    \"ExactRetrievalStrategy\",\\n    \"SparseRetrievalStrategy\",\\n    \"ElasticsearchStore\",\\n]', metadata={'source': 'test_repo/libs/langchain/langchain/vectorstores/elasticsearch.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from langchain_community.vectorstores.cassandra import CVST, Cassandra\\n\\n__all__ = [\"CVST\", \"Cassandra\"]', metadata={'source': 'test_repo/libs/langchain/langchain/vectorstores/cassandra.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from langchain_community.vectorstores.dingo import Dingo\\n\\n__all__ = [\"Dingo\"]', metadata={'source': 'test_repo/libs/langchain/langchain/vectorstores/dingo.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from langchain_community.vectorstores.marqo import Marqo\\n\\n__all__ = [\"Marqo\"]', metadata={'source': 'test_repo/libs/langchain/langchain/vectorstores/marqo.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from langchain_community.vectorstores.semadb import SemaDB\\n\\n__all__ = [\"SemaDB\"]', metadata={'source': 'test_repo/libs/langchain/langchain/vectorstores/semadb.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from langchain_community.vectorstores.vald import Vald\\n\\n__all__ = [\"Vald\"]', metadata={'source': 'test_repo/libs/langchain/langchain/vectorstores/vald.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from langchain_community.vectorstores.annoy import (\\n    Annoy,\\n)\\n\\n__all__ = [\"Annoy\"]', metadata={'source': 'test_repo/libs/langchain/langchain/vectorstores/annoy.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from langchain_community.vectorstores.faiss import (\\n    FAISS,\\n)\\n\\n__all__ = [\"FAISS\"]', metadata={'source': 'test_repo/libs/langchain/langchain/vectorstores/faiss.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from langchain_community.vectorstores.hippo import Hippo\\n\\n__all__ = [\"Hippo\"]', metadata={'source': 'test_repo/libs/langchain/langchain/vectorstores/hippo.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='\"\"\"**Vector store** stores embedded data and performs vector search.\\n\\nOne of the most common ways to store and search over unstructured data is to\\nembed it and store the resulting embedding vectors, and then query the store\\nand retrieve the data that are \\'most similar\\' to the embedded query.\\n\\n**Class hierarchy:**\\n\\n.. code-block::\\n\\n    VectorStore --> <name>  # Examples: Annoy, FAISS, Milvus\\n\\n    BaseRetriever --> VectorStoreRetriever --> <name>Retriever  # Example: VespaRetriever\\n\\n**Main helpers:**\\n\\n.. code-block::\\n\\n    Embeddings, Document\\n\"\"\"  # noqa: E501\\nimport warnings\\nfrom typing import Any\\n\\nfrom langchain_core._api import LangChainDeprecationWarning\\nfrom langchain_core.vectorstores import VectorStore\\n\\nfrom langchain.utils.interactive_env import is_interactive_env', metadata={'source': 'test_repo/libs/langchain/langchain/vectorstores/__init__.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='def __getattr__(name: str) -> Any:\\n    from langchain_community import vectorstores\\n\\n    # If not in interactive env, raise warning.\\n    if not is_interactive_env():\\n        warnings.warn(\\n            \"Importing vector stores from langchain is deprecated. Importing from \"\\n            \"langchain will no longer be supported as of langchain==0.2.0. \"\\n            \"Please import from langchain-community instead:\\\\n\\\\n\"\\n            f\"`from langchain_community.vectorstores import {name}`.\\\\n\\\\n\"\\n            \"To install langchain-community run `pip install -U langchain-community`.\",\\n            category=LangChainDeprecationWarning,\\n        )\\n\\n    return getattr(vectorstores, name)', metadata={'source': 'test_repo/libs/langchain/langchain/vectorstores/__init__.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='__all__ = [\\n    \"AlibabaCloudOpenSearch\",\\n    \"AlibabaCloudOpenSearchSettings\",\\n    \"AnalyticDB\",\\n    \"Annoy\",\\n    \"AtlasDB\",\\n    \"AwaDB\",\\n    \"AzureSearch\",\\n    \"Bagel\",\\n    \"Cassandra\",\\n    \"AstraDB\",\\n    \"Chroma\",\\n    \"Clarifai\",\\n    \"Clickhouse\",\\n    \"ClickhouseSettings\",\\n    \"DashVector\",\\n    \"DatabricksVectorSearch\",\\n    \"DeepLake\",\\n    \"Dingo\",\\n    \"DocArrayHnswSearch\",\\n    \"DocArrayInMemorySearch\",\\n    \"ElasticKnnSearch\",\\n    \"ElasticVectorSearch\",\\n    \"ElasticsearchStore\",\\n    \"Epsilla\",\\n    \"FAISS\",\\n    \"Hologres\",\\n    \"LanceDB\",\\n    \"LLMRails\",\\n    \"Marqo\",\\n    \"MatchingEngine\",\\n    \"Meilisearch\",\\n    \"Milvus\",\\n    \"MomentoVectorIndex\",\\n    \"MongoDBAtlasVectorSearch\",\\n    \"MyScale\",\\n    \"MyScaleSettings\",\\n    \"Neo4jVector\",\\n    \"OpenSearchVectorSearch\",\\n    \"PGEmbedding\",\\n    \"PGVector\",\\n    \"Pinecone\",\\n    \"Qdrant\",\\n    \"Redis\",\\n    \"Rockset\",\\n    \"SKLearnVectorStore\",\\n    \"ScaNN\",\\n    \"SemaDB\",\\n    \"SingleStoreDB\",\\n    \"SQLiteVSS\",\\n    \"StarRocks\",', metadata={'source': 'test_repo/libs/langchain/langchain/vectorstores/__init__.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='\"ScaNN\",\\n    \"SemaDB\",\\n    \"SingleStoreDB\",\\n    \"SQLiteVSS\",\\n    \"StarRocks\",\\n    \"SupabaseVectorStore\",\\n    \"Tair\",\\n    \"TileDB\",\\n    \"Tigris\",\\n    \"TimescaleVector\",\\n    \"Typesense\",\\n    \"USearch\",\\n    \"Vald\",\\n    \"Vearch\",\\n    \"Vectara\",\\n    \"VespaStore\",\\n    \"Weaviate\",\\n    \"Yellowbrick\",\\n    \"ZepVectorStore\",\\n    \"Zilliz\",\\n    \"TencentVectorDB\",\\n    \"AzureCosmosDBVectorSearch\",\\n    \"VectorStore\",\\n]', metadata={'source': 'test_repo/libs/langchain/langchain/vectorstores/__init__.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from langchain_community.vectorstores.sqlitevss import SQLiteVSS\\n\\n__all__ = [\"SQLiteVSS\"]', metadata={'source': 'test_repo/libs/langchain/langchain/vectorstores/sqlitevss.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from langchain_community.vectorstores.pgvecto_rs import PGVecto_rs\\n\\n__all__ = [\"PGVecto_rs\"]', metadata={'source': 'test_repo/libs/langchain/langchain/vectorstores/pgvecto_rs.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from langchain_community.vectorstores.qdrant import (\\n    Qdrant,\\n    QdrantException,\\n)\\n\\n__all__ = [\"QdrantException\", \"Qdrant\"]', metadata={'source': 'test_repo/libs/langchain/langchain/vectorstores/qdrant.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from langchain_community.vectorstores.usearch import USearch\\n\\n__all__ = [\"USearch\"]', metadata={'source': 'test_repo/libs/langchain/langchain/vectorstores/usearch.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from langchain_community.vectorstores.momento_vector_index import (\\n    MomentoVectorIndex,\\n)\\n\\n__all__ = [\"MomentoVectorIndex\"]', metadata={'source': 'test_repo/libs/langchain/langchain/vectorstores/momento_vector_index.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from langchain_community.vectorstores.azure_cosmos_db import (\\n    AzureCosmosDBVectorSearch,\\n    CosmosDBDocumentType,\\n    CosmosDBSimilarityType,\\n)\\n\\n__all__ = [\\n    \"CosmosDBSimilarityType\",\\n    \"CosmosDBDocumentType\",\\n    \"AzureCosmosDBVectorSearch\",\\n]', metadata={'source': 'test_repo/libs/langchain/langchain/vectorstores/azure_cosmos_db.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from langchain_community.vectorstores.bageldb import (\\n    Bagel,\\n)\\n\\n__all__ = [\"Bagel\"]', metadata={'source': 'test_repo/libs/langchain/langchain/vectorstores/bageldb.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from langchain_community.vectorstores.analyticdb import (\\n    AnalyticDB,\\n)\\n\\n__all__ = [\\n    \"AnalyticDB\",\\n]', metadata={'source': 'test_repo/libs/langchain/langchain/vectorstores/analyticdb.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from langchain_community.vectorstores.tair import Tair\\n\\n__all__ = [\"Tair\"]', metadata={'source': 'test_repo/libs/langchain/langchain/vectorstores/tair.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from langchain_community.vectorstores.chroma import (\\n    Chroma,\\n)\\n\\n__all__ = [\"Chroma\"]', metadata={'source': 'test_repo/libs/langchain/langchain/vectorstores/chroma.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from langchain_community.vectorstores.atlas import AtlasDB\\n\\n__all__ = [\"AtlasDB\"]', metadata={'source': 'test_repo/libs/langchain/langchain/vectorstores/atlas.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from langchain_community.vectorstores.scann import (\\n    ScaNN,\\n)\\n\\n__all__ = [\"ScaNN\"]', metadata={'source': 'test_repo/libs/langchain/langchain/vectorstores/scann.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from langchain_community.vectorstores.tencentvectordb import (\\n    ConnectionParams,\\n    IndexParams,\\n    TencentVectorDB,\\n)\\n\\n__all__ = [\"ConnectionParams\", \"IndexParams\", \"TencentVectorDB\"]', metadata={'source': 'test_repo/libs/langchain/langchain/vectorstores/tencentvectordb.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from langchain_community.vectorstores.hologres import (\\n    Hologres,\\n)\\n\\n__all__ = [\"Hologres\"]', metadata={'source': 'test_repo/libs/langchain/langchain/vectorstores/hologres.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from langchain_community.vectorstores.opensearch_vector_search import (\\n    OpenSearchVectorSearch,\\n)\\n\\n__all__ = [\\n    \"OpenSearchVectorSearch\",\\n]', metadata={'source': 'test_repo/libs/langchain/langchain/vectorstores/opensearch_vector_search.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from langchain_community.vectorstores.xata import XataVectorStore\\n\\n__all__ = [\"XataVectorStore\"]', metadata={'source': 'test_repo/libs/langchain/langchain/vectorstores/xata.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from langchain_community.vectorstores.singlestoredb import (\\n    SingleStoreDB,\\n    SingleStoreDBRetriever,\\n)\\n\\n__all__ = [\"SingleStoreDB\", \"SingleStoreDBRetriever\"]', metadata={'source': 'test_repo/libs/langchain/langchain/vectorstores/singlestoredb.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from langchain_community.vectorstores.dashvector import DashVector\\n\\n__all__ = [\"DashVector\"]', metadata={'source': 'test_repo/libs/langchain/langchain/vectorstores/dashvector.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from langchain_community.vectorstores.utils import (\\n    DistanceStrategy,\\n    filter_complex_metadata,\\n    maximal_marginal_relevance,\\n)\\n\\n__all__ = [\"DistanceStrategy\", \"maximal_marginal_relevance\", \"filter_complex_metadata\"]', metadata={'source': 'test_repo/libs/langchain/langchain/vectorstores/utils.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from langchain_community.vectorstores.mongodb_atlas import (\\n    MongoDBAtlasVectorSearch,\\n    MongoDBDocumentType,\\n)\\n\\n__all__ = [\\n    \"MongoDBDocumentType\",\\n    \"MongoDBAtlasVectorSearch\",\\n]', metadata={'source': 'test_repo/libs/langchain/langchain/vectorstores/mongodb_atlas.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from langchain_community.vectorstores.timescalevector import (\\n    TimescaleVector,\\n)\\n\\n__all__ = [\\n    \"TimescaleVector\",\\n]', metadata={'source': 'test_repo/libs/langchain/langchain/vectorstores/timescalevector.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from langchain_community.vectorstores.starrocks import (\\n    StarRocks,\\n    StarRocksSettings,\\n)\\n\\n__all__ = [\\n    \"StarRocksSettings\",\\n    \"StarRocks\",\\n]', metadata={'source': 'test_repo/libs/langchain/langchain/vectorstores/starrocks.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from langchain_community.vectorstores.epsilla import Epsilla\\n\\n__all__ = [\"Epsilla\"]', metadata={'source': 'test_repo/libs/langchain/langchain/vectorstores/epsilla.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from langchain_community.vectorstores.pgembedding import (\\n    CollectionStore,\\n    EmbeddingStore,\\n    PGEmbedding,\\n    QueryResult,\\n)\\n\\n__all__ = [\\n    \"CollectionStore\",\\n    \"EmbeddingStore\",\\n    \"QueryResult\",\\n    \"PGEmbedding\",\\n]', metadata={'source': 'test_repo/libs/langchain/langchain/vectorstores/pgembedding.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from langchain_community.vectorstores.elastic_vector_search import (\\n    ElasticKnnSearch,\\n    ElasticVectorSearch,\\n)\\n\\n__all__ = [\\n    \"ElasticVectorSearch\",\\n    \"ElasticKnnSearch\",\\n]', metadata={'source': 'test_repo/libs/langchain/langchain/vectorstores/elastic_vector_search.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from langchain_community.vectorstores.llm_rails import LLMRails, LLMRailsRetriever\\n\\n__all__ = [\"LLMRails\", \"LLMRailsRetriever\"]', metadata={'source': 'test_repo/libs/langchain/langchain/vectorstores/llm_rails.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from langchain_community.vectorstores.pgvector import (\\n    DistanceStrategy,\\n    PGVector,\\n)\\n\\n__all__ = [\\n    \"DistanceStrategy\",\\n    \"PGVector\",\\n]', metadata={'source': 'test_repo/libs/langchain/langchain/vectorstores/pgvector.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from langchain_community.vectorstores.astradb import (\\n    AstraDB,\\n)\\n\\n__all__ = [\\n    \"AstraDB\",\\n]', metadata={'source': 'test_repo/libs/langchain/langchain/vectorstores/astradb.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from langchain_community.vectorstores.azuresearch import (\\n    AzureSearch,\\n    AzureSearchVectorStoreRetriever,\\n)\\n\\n__all__ = [\\n    \"AzureSearch\",\\n    \"AzureSearchVectorStoreRetriever\",\\n]', metadata={'source': 'test_repo/libs/langchain/langchain/vectorstores/azuresearch.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from langchain_community.vectorstores.yellowbrick import Yellowbrick\\n\\n__all__ = [\"Yellowbrick\"]', metadata={'source': 'test_repo/libs/langchain/langchain/vectorstores/yellowbrick.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from langchain_community.vectorstores.baiducloud_vector_search import BESVectorStore\\n\\n__all__ = [\"BESVectorStore\"]', metadata={'source': 'test_repo/libs/langchain/langchain/vectorstores/baiducloud_vector_search.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from langchain_community.vectorstores.rocksetdb import Rockset\\n\\n__all__ = [\"Rockset\"]', metadata={'source': 'test_repo/libs/langchain/langchain/vectorstores/rocksetdb.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from langchain_community.vectorstores.clickhouse import (\\n    Clickhouse,\\n    ClickhouseSettings,\\n)\\n\\n__all__ = [\"ClickhouseSettings\", \"Clickhouse\"]', metadata={'source': 'test_repo/libs/langchain/langchain/vectorstores/clickhouse.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from langchain_community.vectorstores.vectara import Vectara, VectaraRetriever\\n\\n__all__ = [\"Vectara\", \"VectaraRetriever\"]', metadata={'source': 'test_repo/libs/langchain/langchain/vectorstores/vectara.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from langchain_community.vectorstores.zep import CollectionConfig, ZepVectorStore\\n\\n__all__ = [\"CollectionConfig\", \"ZepVectorStore\"]', metadata={'source': 'test_repo/libs/langchain/langchain/vectorstores/zep.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from langchain_core.vectorstores import VectorStore, VectorStoreRetriever\\n\\n__all__ = [\"VectorStore\", \"VectorStoreRetriever\"]', metadata={'source': 'test_repo/libs/langchain/langchain/vectorstores/base.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from langchain_community.vectorstores.alibabacloud_opensearch import (\\n    AlibabaCloudOpenSearch,\\n    AlibabaCloudOpenSearchSettings,\\n)\\n\\n__all__ = [\\n    \"AlibabaCloudOpenSearchSettings\",\\n    \"AlibabaCloudOpenSearch\",\\n]', metadata={'source': 'test_repo/libs/langchain/langchain/vectorstores/alibabacloud_opensearch.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from langchain_community.vectorstores.tigris import Tigris\\n\\n__all__ = [\"Tigris\"]', metadata={'source': 'test_repo/libs/langchain/langchain/vectorstores/tigris.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from langchain_community.vectorstores.meilisearch import Meilisearch\\n\\n__all__ = [\"Meilisearch\"]', metadata={'source': 'test_repo/libs/langchain/langchain/vectorstores/meilisearch.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from langchain_community.vectorstores.zilliz import Zilliz\\n\\n__all__ = [\"Zilliz\"]', metadata={'source': 'test_repo/libs/langchain/langchain/vectorstores/zilliz.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from langchain_community.vectorstores.weaviate import (\\n    Weaviate,\\n)\\n\\n__all__ = [\\n    \"Weaviate\",\\n]', metadata={'source': 'test_repo/libs/langchain/langchain/vectorstores/weaviate.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from langchain_community.vectorstores.milvus import Milvus\\n\\n__all__ = [\"Milvus\"]', metadata={'source': 'test_repo/libs/langchain/langchain/vectorstores/milvus.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from langchain_community.vectorstores.matching_engine import MatchingEngine\\n\\n__all__ = [\"MatchingEngine\"]', metadata={'source': 'test_repo/libs/langchain/langchain/vectorstores/matching_engine.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from langchain_community.vectorstores.vearch import Vearch\\n\\n__all__ = [\"Vearch\"]', metadata={'source': 'test_repo/libs/langchain/langchain/vectorstores/vearch.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from langchain_community.vectorstores.deeplake import DeepLake\\n\\n__all__ = [\"DeepLake\"]', metadata={'source': 'test_repo/libs/langchain/langchain/vectorstores/deeplake.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from langchain_community.vectorstores.neo4j_vector import (\\n    Neo4jVector,\\n    SearchType,\\n)\\n\\n__all__ = [\\n    \"SearchType\",\\n    \"Neo4jVector\",\\n]', metadata={'source': 'test_repo/libs/langchain/langchain/vectorstores/neo4j_vector.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from langchain_community.vectorstores.vespa import VespaStore\\n\\n__all__ = [\"VespaStore\"]', metadata={'source': 'test_repo/libs/langchain/langchain/vectorstores/vespa.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from langchain_community.vectorstores.awadb import AwaDB\\n\\n__all__ = [\"AwaDB\"]', metadata={'source': 'test_repo/libs/langchain/langchain/vectorstores/awadb.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from langchain_community.vectorstores.lancedb import LanceDB\\n\\n__all__ = [\"LanceDB\"]', metadata={'source': 'test_repo/libs/langchain/langchain/vectorstores/lancedb.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from langchain_community.vectorstores.nucliadb import NucliaDB\\n\\n__all__ = [\"NucliaDB\"]', metadata={'source': 'test_repo/libs/langchain/langchain/vectorstores/nucliadb.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from langchain_community.vectorstores.supabase import SupabaseVectorStore\\n\\n__all__ = [\"SupabaseVectorStore\"]', metadata={'source': 'test_repo/libs/langchain/langchain/vectorstores/supabase.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from langchain_community.vectorstores.pinecone import Pinecone\\n\\n__all__ = [\"Pinecone\"]', metadata={'source': 'test_repo/libs/langchain/langchain/vectorstores/pinecone.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from langchain_community.vectorstores.databricks_vector_search import (\\n    DatabricksVectorSearch,\\n)\\n\\n__all__ = [\"DatabricksVectorSearch\"]', metadata={'source': 'test_repo/libs/langchain/langchain/vectorstores/databricks_vector_search.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from langchain_community.vectorstores.clarifai import Clarifai\\n\\n__all__ = [\"Clarifai\"]', metadata={'source': 'test_repo/libs/langchain/langchain/vectorstores/clarifai.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from langchain_community.vectorstores.tiledb import (\\n    TileDB,\\n)\\n\\n__all__ = [\\n    \"TileDB\",\\n]', metadata={'source': 'test_repo/libs/langchain/langchain/vectorstores/tiledb.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from langchain_community.vectorstores.docarray.hnsw import DocArrayHnswSearch\\nfrom langchain_community.vectorstores.docarray.in_memory import DocArrayInMemorySearch\\n\\n__all__ = [\\n    \"DocArrayHnswSearch\",\\n    \"DocArrayInMemorySearch\",\\n]', metadata={'source': 'test_repo/libs/langchain/langchain/vectorstores/docarray/__init__.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from langchain_community.vectorstores.docarray.base import (\\n    DocArrayIndex,\\n)\\n\\n__all__ = [\"DocArrayIndex\"]', metadata={'source': 'test_repo/libs/langchain/langchain/vectorstores/docarray/base.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from langchain_community.vectorstores.docarray.hnsw import DocArrayHnswSearch\\n\\n__all__ = [\"DocArrayHnswSearch\"]', metadata={'source': 'test_repo/libs/langchain/langchain/vectorstores/docarray/hnsw.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from langchain_community.vectorstores.docarray.in_memory import DocArrayInMemorySearch\\n\\n__all__ = [\"DocArrayInMemorySearch\"]', metadata={'source': 'test_repo/libs/langchain/langchain/vectorstores/docarray/in_memory.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from .base import Redis, RedisVectorStoreRetriever\\nfrom .filters import (\\n    RedisFilter,\\n    RedisNum,\\n    RedisTag,\\n    RedisText,\\n)\\n\\n__all__ = [\\n    \"Redis\",\\n    \"RedisFilter\",\\n    \"RedisTag\",\\n    \"RedisText\",\\n    \"RedisNum\",\\n    \"RedisVectorStoreRetriever\",\\n]', metadata={'source': 'test_repo/libs/langchain/langchain/vectorstores/redis/__init__.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from langchain_community.vectorstores.redis.base import (\\n    Redis,\\n    RedisVectorStoreRetriever,\\n    check_index_exists,\\n)\\n\\n__all__ = [\\n    \"check_index_exists\",\\n    \"Redis\",\\n    \"RedisVectorStoreRetriever\",\\n]', metadata={'source': 'test_repo/libs/langchain/langchain/vectorstores/redis/base.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from langchain_community.vectorstores.redis.filters import (\\n    RedisFilter,\\n    RedisFilterExpression,\\n    RedisFilterField,\\n    RedisFilterOperator,\\n    RedisNum,\\n    RedisTag,\\n    RedisText,\\n    check_operator_misuse,\\n)\\n\\n__all__ = [\\n    \"RedisFilterOperator\",\\n    \"RedisFilter\",\\n    \"RedisFilterField\",\\n    \"check_operator_misuse\",\\n    \"RedisTag\",\\n    \"RedisNum\",\\n    \"RedisText\",\\n    \"RedisFilterExpression\",\\n]', metadata={'source': 'test_repo/libs/langchain/langchain/vectorstores/redis/filters.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from langchain_community.vectorstores.redis.schema import (\\n    FlatVectorField,\\n    HNSWVectorField,\\n    NumericFieldSchema,\\n    RedisDistanceMetric,\\n    RedisField,\\n    RedisModel,\\n    RedisVectorField,\\n    TagFieldSchema,\\n    TextFieldSchema,\\n    read_schema,\\n)\\n\\n__all__ = [\\n    \"RedisDistanceMetric\",\\n    \"RedisField\",\\n    \"TextFieldSchema\",\\n    \"TagFieldSchema\",\\n    \"NumericFieldSchema\",\\n    \"RedisVectorField\",\\n    \"FlatVectorField\",\\n    \"HNSWVectorField\",\\n    \"RedisModel\",\\n    \"read_schema\",\\n]', metadata={'source': 'test_repo/libs/langchain/langchain/vectorstores/redis/schema.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from langchain_community.adapters.openai import (\\n    Chat,\\n    ChatCompletion,\\n    ChatCompletionChunk,\\n    ChatCompletions,\\n    Choice,\\n    ChoiceChunk,\\n    Completions,\\n    IndexableBaseModel,\\n    chat,\\n    convert_dict_to_message,\\n    convert_message_to_dict,\\n    convert_messages_for_finetuning,\\n    convert_openai_messages,\\n)\\n\\n__all__ = [\\n    \"IndexableBaseModel\",\\n    \"Choice\",\\n    \"ChatCompletions\",\\n    \"ChoiceChunk\",\\n    \"ChatCompletionChunk\",\\n    \"convert_dict_to_message\",\\n    \"convert_message_to_dict\",\\n    \"convert_openai_messages\",\\n    \"ChatCompletion\",\\n    \"convert_messages_for_finetuning\",\\n    \"Completions\",\\n    \"Chat\",\\n    \"chat\",\\n]', metadata={'source': 'test_repo/libs/langchain/langchain/adapters/openai.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='\"\"\"Helper functions for managing the LangChain API.\\n\\nThis module is only relevant for LangChain developers, not for users.\\n\\n.. warning::\\n\\n    This module and its submodules are for internal use only.  Do not use them\\n    in your own code.  We may change the API at any time with no warning.\\n\\n\"\"\"\\n\\nfrom .deprecation import (\\n    LangChainDeprecationWarning,\\n    deprecated,\\n    suppress_langchain_deprecation_warning,\\n    surface_langchain_deprecation_warnings,\\n    warn_deprecated,\\n)\\n\\n__all__ = [\\n    \"deprecated\",\\n    \"LangChainDeprecationWarning\",\\n    \"suppress_langchain_deprecation_warning\",\\n    \"surface_langchain_deprecation_warnings\",\\n    \"warn_deprecated\",\\n]', metadata={'source': 'test_repo/libs/langchain/langchain/_api/__init__.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from langchain_core._api.path import as_import_path, get_relative_path\\n\\n__all__ = [\"get_relative_path\", \"as_import_path\"]', metadata={'source': 'test_repo/libs/langchain/langchain/_api/path.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from langchain_core._api.deprecation import (\\n    LangChainDeprecationWarning,\\n    LangChainPendingDeprecationWarning,\\n    deprecated,\\n    suppress_langchain_deprecation_warning,\\n    surface_langchain_deprecation_warnings,\\n    warn_deprecated,\\n)\\n\\n__all__ = [\\n    \"LangChainDeprecationWarning\",\\n    \"LangChainPendingDeprecationWarning\",\\n    \"deprecated\",\\n    \"suppress_langchain_deprecation_warning\",\\n    \"warn_deprecated\",\\n    \"surface_langchain_deprecation_warnings\",\\n]', metadata={'source': 'test_repo/libs/langchain/langchain/_api/deprecation.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='class UpsertionRecord(Base):  # type: ignore[valid-type,misc]\\n    \"\"\"Table used to keep track of when a key was last updated.\"\"\"\\n\\n    # ATTENTION:\\n    # Prior to modifying this table, please determine whether\\n    # we should create migrations for this table to make sure\\n    # users do not experience data loss.\\n    __tablename__ = \"upsertion_record\"\\n\\n    uuid = Column(\\n        String,\\n        index=True,\\n        default=lambda: str(uuid.uuid4()),\\n        primary_key=True,\\n        nullable=False,\\n    )\\n    key = Column(String, index=True)\\n    # Using a non-normalized representation to handle `namespace` attribute.\\n    # If the need arises, this attribute can be pulled into a separate Collection\\n    # table at some time later.\\n    namespace = Column(String, index=True, nullable=False)\\n    group_id = Column(String, index=True, nullable=True)\\n\\n    # The timestamp associated with the last record upsertion.\\n    updated_at = Column(Float, index=True)', metadata={'source': 'test_repo/libs/langchain/langchain/indexes/_sql_record_manager.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='__table_args__ = (\\n        UniqueConstraint(\"key\", \"namespace\", name=\"uix_key_namespace\"),\\n        Index(\"ix_key_namespace\", \"key\", \"namespace\"),\\n    )', metadata={'source': 'test_repo/libs/langchain/langchain/indexes/_sql_record_manager.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='class SQLRecordManager(RecordManager):\\n    \"\"\"A SQL Alchemy based implementation of the record manager.\"\"\"\\n\\n    def __init__(\\n        self,\\n        namespace: str,\\n        *,\\n        engine: Optional[Union[Engine, AsyncEngine]] = None,\\n        db_url: Union[None, str, URL] = None,\\n        engine_kwargs: Optional[Dict[str, Any]] = None,\\n        async_mode: bool = False,\\n    ) -> None:\\n        \"\"\"Initialize the SQLRecordManager.\\n\\n        This class serves as a manager persistence layer that uses an SQL\\n        backend to track upserted records. You should specify either a db_url\\n        to create an engine or provide an existing engine.', metadata={'source': 'test_repo/libs/langchain/langchain/indexes/_sql_record_manager.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='Args:\\n            namespace: The namespace associated with this record manager.\\n            engine: An already existing SQL Alchemy engine.\\n                Default is None.\\n            db_url: A database connection string used to create\\n                an SQL Alchemy engine. Default is None.\\n            engine_kwargs: Additional keyword arguments\\n                to be passed when creating the engine. Default is an empty dictionary.\\n            async_mode: Whether to create an async engine.\\n                Driver should support async operations.\\n                It only applies if db_url is provided.\\n                Default is False.', metadata={'source': 'test_repo/libs/langchain/langchain/indexes/_sql_record_manager.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='Raises:\\n            ValueError: If both db_url and engine are provided or neither.\\n            AssertionError: If something unexpected happens during engine configuration.\\n        \"\"\"\\n        super().__init__(namespace=namespace)\\n        if db_url is None and engine is None:\\n            raise ValueError(\"Must specify either db_url or engine\")\\n\\n        if db_url is not None and engine is not None:\\n            raise ValueError(\"Must specify either db_url or engine, not both\")\\n\\n        _engine: Union[Engine, AsyncEngine]\\n        if db_url:\\n            if async_mode:\\n                _engine = create_async_engine(db_url, **(engine_kwargs or {}))\\n            else:\\n                _engine = create_engine(db_url, **(engine_kwargs or {}))\\n        elif engine:\\n            _engine = engine\\n\\n        else:\\n            raise AssertionError(\"Something went wrong with configuration of engine.\")', metadata={'source': 'test_repo/libs/langchain/langchain/indexes/_sql_record_manager.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='_session_factory: Union[sessionmaker[Session], async_sessionmaker[AsyncSession]]\\n        if isinstance(_engine, AsyncEngine):\\n            _session_factory = async_sessionmaker(bind=_engine)\\n        else:\\n            _session_factory = sessionmaker(bind=_engine)\\n\\n        self.engine = _engine\\n        self.dialect = _engine.dialect.name\\n        self.session_factory = _session_factory\\n\\n    def create_schema(self) -> None:\\n        \"\"\"Create the database schema.\"\"\"\\n        if isinstance(self.engine, AsyncEngine):\\n            raise AssertionError(\"This method is not supported for async engines.\")\\n\\n        Base.metadata.create_all(self.engine)\\n\\n    async def acreate_schema(self) -> None:\\n        \"\"\"Create the database schema.\"\"\"\\n\\n        if not isinstance(self.engine, AsyncEngine):\\n            raise AssertionError(\"This method is not supported for sync engines.\")\\n\\n        async with self.engine.begin() as session:\\n            await session.run_sync(Base.metadata.create_all)', metadata={'source': 'test_repo/libs/langchain/langchain/indexes/_sql_record_manager.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='@contextlib.contextmanager\\n    def _make_session(self) -> Generator[Session, None, None]:\\n        \"\"\"Create a session and close it after use.\"\"\"\\n\\n        if isinstance(self.session_factory, async_sessionmaker):\\n            raise AssertionError(\"This method is not supported for async engines.\")\\n\\n        session = self.session_factory()\\n        try:\\n            yield session\\n        finally:\\n            session.close()\\n\\n    @contextlib.asynccontextmanager\\n    async def _amake_session(self) -> AsyncGenerator[AsyncSession, None]:\\n        \"\"\"Create a session and close it after use.\"\"\"\\n\\n        if not isinstance(self.session_factory, async_sessionmaker):\\n            raise AssertionError(\"This method is not supported for sync engines.\")\\n\\n        async with self.session_factory() as session:\\n            yield session\\n\\n    def get_time(self) -> float:\\n        \"\"\"Get the current server time as a timestamp.', metadata={'source': 'test_repo/libs/langchain/langchain/indexes/_sql_record_manager.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='Please note it\\'s critical that time is obtained from the server since\\n        we want a monotonic clock.\\n        \"\"\"\\n        with self._make_session() as session:\\n            # * SQLite specific implementation, can be changed based on dialect.\\n            # * For SQLite, unlike unixepoch it will work with older versions of SQLite.\\n            # ----\\n            # julianday(\\'now\\'): Julian day number for the current date and time.\\n            # The Julian day is a continuous count of days, starting from a\\n            # reference date (Julian day number 0).\\n            # 2440587.5 - constant represents the Julian day number for January 1, 1970\\n            # 86400.0 - constant represents the number of seconds\\n            # in a day (24 hours * 60 minutes * 60 seconds)\\n            if self.dialect == \"sqlite\":\\n                query = text(\"SELECT (julianday(\\'now\\') - 2440587.5) * 86400.0;\")\\n            elif self.dialect == \"postgresql\":', metadata={'source': 'test_repo/libs/langchain/langchain/indexes/_sql_record_manager.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='elif self.dialect == \"postgresql\":\\n                query = text(\"SELECT EXTRACT (EPOCH FROM CURRENT_TIMESTAMP);\")\\n            else:\\n                raise NotImplementedError(f\"Not implemented for dialect {self.dialect}\")', metadata={'source': 'test_repo/libs/langchain/langchain/indexes/_sql_record_manager.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='dt = session.execute(query).scalar()\\n            if isinstance(dt, decimal.Decimal):\\n                dt = float(dt)\\n            if not isinstance(dt, float):\\n                raise AssertionError(f\"Unexpected type for datetime: {type(dt)}\")\\n            return dt\\n\\n    async def aget_time(self) -> float:\\n        \"\"\"Get the current server time as a timestamp.', metadata={'source': 'test_repo/libs/langchain/langchain/indexes/_sql_record_manager.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='Please note it\\'s critical that time is obtained from the server since\\n        we want a monotonic clock.\\n        \"\"\"\\n        async with self._amake_session() as session:\\n            # * SQLite specific implementation, can be changed based on dialect.\\n            # * For SQLite, unlike unixepoch it will work with older versions of SQLite.\\n            # ----\\n            # julianday(\\'now\\'): Julian day number for the current date and time.\\n            # The Julian day is a continuous count of days, starting from a\\n            # reference date (Julian day number 0).\\n            # 2440587.5 - constant represents the Julian day number for January 1, 1970\\n            # 86400.0 - constant represents the number of seconds\\n            # in a day (24 hours * 60 minutes * 60 seconds)\\n            if self.dialect == \"sqlite\":\\n                query = text(\"SELECT (julianday(\\'now\\') - 2440587.5) * 86400.0;\")\\n            elif self.dialect == \"postgresql\":', metadata={'source': 'test_repo/libs/langchain/langchain/indexes/_sql_record_manager.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='elif self.dialect == \"postgresql\":\\n                query = text(\"SELECT EXTRACT (EPOCH FROM CURRENT_TIMESTAMP);\")\\n            else:\\n                raise NotImplementedError(f\"Not implemented for dialect {self.dialect}\")', metadata={'source': 'test_repo/libs/langchain/langchain/indexes/_sql_record_manager.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='dt = (await session.execute(query)).scalar_one_or_none()\\n\\n            if isinstance(dt, decimal.Decimal):\\n                dt = float(dt)\\n            if not isinstance(dt, float):\\n                raise AssertionError(f\"Unexpected type for datetime: {type(dt)}\")\\n            return dt\\n\\n    def update(\\n        self,\\n        keys: Sequence[str],\\n        *,\\n        group_ids: Optional[Sequence[Optional[str]]] = None,\\n        time_at_least: Optional[float] = None,\\n    ) -> None:\\n        \"\"\"Upsert records into the SQLite database.\"\"\"\\n        if group_ids is None:\\n            group_ids = [None] * len(keys)\\n\\n        if len(keys) != len(group_ids):\\n            raise ValueError(\\n                f\"Number of keys ({len(keys)}) does not match number of \"\\n                f\"group_ids ({len(group_ids)})\"\\n            )', metadata={'source': 'test_repo/libs/langchain/langchain/indexes/_sql_record_manager.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='# Get the current time from the server.\\n        # This makes an extra round trip to the server, should not be a big deal\\n        # if the batch size is large enough.\\n        # Getting the time here helps us compare it against the time_at_least\\n        # and raise an error if there is a time sync issue.\\n        # Here, we\\'re just being extra careful to minimize the chance of\\n        # data loss due to incorrectly deleting records.\\n        update_time = self.get_time()\\n\\n        if time_at_least and update_time < time_at_least:\\n            # Safeguard against time sync issues\\n            raise AssertionError(f\"Time sync issue: {update_time} < {time_at_least}\")\\n\\n        records_to_upsert = [\\n            {\\n                \"key\": key,\\n                \"namespace\": self.namespace,\\n                \"updated_at\": update_time,\\n                \"group_id\": group_id,\\n            }\\n            for key, group_id in zip(keys, group_ids)\\n        ]', metadata={'source': 'test_repo/libs/langchain/langchain/indexes/_sql_record_manager.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='with self._make_session() as session:\\n            if self.dialect == \"sqlite\":\\n                from sqlalchemy.dialects.sqlite import Insert as SqliteInsertType\\n                from sqlalchemy.dialects.sqlite import insert as sqlite_insert', metadata={'source': 'test_repo/libs/langchain/langchain/indexes/_sql_record_manager.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='# Note: uses SQLite insert to make on_conflict_do_update work.\\n                # This code needs to be generalized a bit to work with more dialects.\\n                sqlite_insert_stmt: SqliteInsertType = sqlite_insert(\\n                    UpsertionRecord\\n                ).values(records_to_upsert)\\n                stmt = sqlite_insert_stmt.on_conflict_do_update(\\n                    [UpsertionRecord.key, UpsertionRecord.namespace],\\n                    set_=dict(\\n                        updated_at=sqlite_insert_stmt.excluded.updated_at,\\n                        group_id=sqlite_insert_stmt.excluded.group_id,\\n                    ),\\n                )\\n            elif self.dialect == \"postgresql\":\\n                from sqlalchemy.dialects.postgresql import Insert as PgInsertType\\n                from sqlalchemy.dialects.postgresql import insert as pg_insert', metadata={'source': 'test_repo/libs/langchain/langchain/indexes/_sql_record_manager.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='# Note: uses postgresql insert to make on_conflict_do_update work.\\n                # This code needs to be generalized a bit to work with more dialects.\\n                pg_insert_stmt: PgInsertType = pg_insert(UpsertionRecord).values(\\n                    records_to_upsert\\n                )\\n                stmt = pg_insert_stmt.on_conflict_do_update(\\n                    \"uix_key_namespace\",  # Name of constraint\\n                    set_=dict(\\n                        updated_at=pg_insert_stmt.excluded.updated_at,\\n                        group_id=pg_insert_stmt.excluded.group_id,\\n                    ),\\n                )\\n            else:\\n                raise NotImplementedError(f\"Unsupported dialect {self.dialect}\")\\n\\n            session.execute(stmt)\\n            session.commit()', metadata={'source': 'test_repo/libs/langchain/langchain/indexes/_sql_record_manager.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='session.execute(stmt)\\n            session.commit()\\n\\n    async def aupdate(\\n        self,\\n        keys: Sequence[str],\\n        *,\\n        group_ids: Optional[Sequence[Optional[str]]] = None,\\n        time_at_least: Optional[float] = None,\\n    ) -> None:\\n        \"\"\"Upsert records into the SQLite database.\"\"\"\\n        if group_ids is None:\\n            group_ids = [None] * len(keys)\\n\\n        if len(keys) != len(group_ids):\\n            raise ValueError(\\n                f\"Number of keys ({len(keys)}) does not match number of \"\\n                f\"group_ids ({len(group_ids)})\"\\n            )', metadata={'source': 'test_repo/libs/langchain/langchain/indexes/_sql_record_manager.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='# Get the current time from the server.\\n        # This makes an extra round trip to the server, should not be a big deal\\n        # if the batch size is large enough.\\n        # Getting the time here helps us compare it against the time_at_least\\n        # and raise an error if there is a time sync issue.\\n        # Here, we\\'re just being extra careful to minimize the chance of\\n        # data loss due to incorrectly deleting records.\\n        update_time = await self.aget_time()\\n\\n        if time_at_least and update_time < time_at_least:\\n            # Safeguard against time sync issues\\n            raise AssertionError(f\"Time sync issue: {update_time} < {time_at_least}\")\\n\\n        records_to_upsert = [\\n            {\\n                \"key\": key,\\n                \"namespace\": self.namespace,\\n                \"updated_at\": update_time,\\n                \"group_id\": group_id,\\n            }\\n            for key, group_id in zip(keys, group_ids)\\n        ]', metadata={'source': 'test_repo/libs/langchain/langchain/indexes/_sql_record_manager.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='async with self._amake_session() as session:\\n            if self.dialect == \"sqlite\":\\n                from sqlalchemy.dialects.sqlite import Insert as SqliteInsertType\\n                from sqlalchemy.dialects.sqlite import insert as sqlite_insert', metadata={'source': 'test_repo/libs/langchain/langchain/indexes/_sql_record_manager.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='# Note: uses SQLite insert to make on_conflict_do_update work.\\n                # This code needs to be generalized a bit to work with more dialects.\\n                sqlite_insert_stmt: SqliteInsertType = sqlite_insert(\\n                    UpsertionRecord\\n                ).values(records_to_upsert)\\n                stmt = sqlite_insert_stmt.on_conflict_do_update(\\n                    [UpsertionRecord.key, UpsertionRecord.namespace],\\n                    set_=dict(\\n                        updated_at=sqlite_insert_stmt.excluded.updated_at,\\n                        group_id=sqlite_insert_stmt.excluded.group_id,\\n                    ),\\n                )\\n            elif self.dialect == \"postgresql\":\\n                from sqlalchemy.dialects.postgresql import Insert as PgInsertType\\n                from sqlalchemy.dialects.postgresql import insert as pg_insert', metadata={'source': 'test_repo/libs/langchain/langchain/indexes/_sql_record_manager.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='# Note: uses SQLite insert to make on_conflict_do_update work.\\n                # This code needs to be generalized a bit to work with more dialects.\\n                pg_insert_stmt: PgInsertType = pg_insert(UpsertionRecord).values(\\n                    records_to_upsert\\n                )\\n                stmt = pg_insert_stmt.on_conflict_do_update(\\n                    \"uix_key_namespace\",  # Name of constraint\\n                    set_=dict(\\n                        updated_at=pg_insert_stmt.excluded.updated_at,\\n                        group_id=pg_insert_stmt.excluded.group_id,\\n                    ),\\n                )\\n            else:\\n                raise NotImplementedError(f\"Unsupported dialect {self.dialect}\")\\n\\n            await session.execute(stmt)\\n            await session.commit()', metadata={'source': 'test_repo/libs/langchain/langchain/indexes/_sql_record_manager.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='await session.execute(stmt)\\n            await session.commit()\\n\\n    def exists(self, keys: Sequence[str]) -> List[bool]:\\n        \"\"\"Check if the given keys exist in the SQLite database.\"\"\"\\n        session: Session\\n        with self._make_session() as session:\\n            filtered_query: Query = session.query(UpsertionRecord.key).filter(\\n                and_(\\n                    UpsertionRecord.key.in_(keys),\\n                    UpsertionRecord.namespace == self.namespace,\\n                )\\n            )\\n            records = filtered_query.all()\\n        found_keys = set(r.key for r in records)\\n        return [k in found_keys for k in keys]', metadata={'source': 'test_repo/libs/langchain/langchain/indexes/_sql_record_manager.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='async def aexists(self, keys: Sequence[str]) -> List[bool]:\\n        \"\"\"Check if the given keys exist in the SQLite database.\"\"\"\\n        async with self._amake_session() as session:\\n            records = (\\n                (\\n                    await session.execute(\\n                        select(UpsertionRecord.key).where(\\n                            and_(\\n                                UpsertionRecord.key.in_(keys),\\n                                UpsertionRecord.namespace == self.namespace,\\n                            )\\n                        )\\n                    )\\n                )\\n                .scalars()\\n                .all()\\n            )\\n        found_keys = set(records)\\n        return [k in found_keys for k in keys]', metadata={'source': 'test_repo/libs/langchain/langchain/indexes/_sql_record_manager.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='def list_keys(\\n        self,\\n        *,\\n        before: Optional[float] = None,\\n        after: Optional[float] = None,\\n        group_ids: Optional[Sequence[str]] = None,\\n        limit: Optional[int] = None,\\n    ) -> List[str]:\\n        \"\"\"List records in the SQLite database based on the provided date range.\"\"\"\\n        session: Session\\n        with self._make_session() as session:\\n            query: Query = session.query(UpsertionRecord).filter(\\n                UpsertionRecord.namespace == self.namespace\\n            )\\n\\n            if after:\\n                query = query.filter(UpsertionRecord.updated_at > after)\\n            if before:\\n                query = query.filter(UpsertionRecord.updated_at < before)\\n            if group_ids:\\n                query = query.filter(UpsertionRecord.group_id.in_(group_ids))\\n\\n            if limit:\\n                query = query.limit(limit)\\n            records = query.all()\\n        return [r.key for r in records]', metadata={'source': 'test_repo/libs/langchain/langchain/indexes/_sql_record_manager.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='async def alist_keys(\\n        self,\\n        *,\\n        before: Optional[float] = None,\\n        after: Optional[float] = None,\\n        group_ids: Optional[Sequence[str]] = None,\\n        limit: Optional[int] = None,\\n    ) -> List[str]:\\n        \"\"\"List records in the SQLite database based on the provided date range.\"\"\"\\n        session: AsyncSession\\n        async with self._amake_session() as session:\\n            query: Query = select(UpsertionRecord.key).filter(\\n                UpsertionRecord.namespace == self.namespace\\n            )\\n\\n            # mypy does not recognize .all() or .filter()\\n            if after:\\n                query = query.filter(UpsertionRecord.updated_at > after)\\n            if before:\\n                query = query.filter(UpsertionRecord.updated_at < before)\\n            if group_ids:\\n                query = query.filter(UpsertionRecord.group_id.in_(group_ids))', metadata={'source': 'test_repo/libs/langchain/langchain/indexes/_sql_record_manager.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='if limit:\\n                query = query.limit(limit)\\n            records = (await session.execute(query)).scalars().all()\\n        return list(records)\\n\\n    def delete_keys(self, keys: Sequence[str]) -> None:\\n        \"\"\"Delete records from the SQLite database.\"\"\"\\n        session: Session\\n        with self._make_session() as session:\\n            filtered_query: Query = session.query(UpsertionRecord).filter(\\n                and_(\\n                    UpsertionRecord.key.in_(keys),\\n                    UpsertionRecord.namespace == self.namespace,\\n                )\\n            )\\n\\n            filtered_query.delete()\\n            session.commit()', metadata={'source': 'test_repo/libs/langchain/langchain/indexes/_sql_record_manager.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='filtered_query.delete()\\n            session.commit()\\n\\n    async def adelete_keys(self, keys: Sequence[str]) -> None:\\n        \"\"\"Delete records from the SQLite database.\"\"\"\\n        async with self._amake_session() as session:\\n            await session.execute(\\n                delete(UpsertionRecord).where(\\n                    and_(\\n                        UpsertionRecord.key.in_(keys),\\n                        UpsertionRecord.namespace == self.namespace,\\n                    )\\n                )\\n            )\\n\\n            await session.commit()', metadata={'source': 'test_repo/libs/langchain/langchain/indexes/_sql_record_manager.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='\"\"\"Implementation of a record management layer in SQLAlchemy.\\n\\nThe management layer uses SQLAlchemy to track upserted records.\\n\\nCurrently, this layer only works with SQLite; hopwever, should be adaptable\\nto other SQL implementations with minimal effort.\\n\\nCurrently, includes an implementation that uses SQLAlchemy which should\\nallow it to work with a variety of SQL as a backend.\\n\\n* Each key is associated with an updated_at field.\\n* This filed is updated whenever the key is updated.\\n* Keys can be listed based on the updated at field.\\n* Keys can be deleted.\\n\"\"\"\\nimport contextlib\\nimport decimal\\nimport uuid\\nfrom typing import Any, AsyncGenerator, Dict, Generator, List, Optional, Sequence, Union', metadata={'source': 'test_repo/libs/langchain/langchain/indexes/_sql_record_manager.py', 'content_type': 'simplified_code', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from sqlalchemy import (\\n    URL,\\n    Column,\\n    Engine,\\n    Float,\\n    Index,\\n    String,\\n    UniqueConstraint,\\n    and_,\\n    create_engine,\\n    delete,\\n    select,\\n    text,\\n)\\nfrom sqlalchemy.ext.asyncio import (\\n    AsyncEngine,\\n    AsyncSession,\\n    async_sessionmaker,\\n    create_async_engine,\\n)\\nfrom sqlalchemy.ext.declarative import declarative_base\\nfrom sqlalchemy.orm import Query, Session, sessionmaker\\n\\nfrom langchain.indexes.base import RecordManager\\n\\nBase = declarative_base()\\n\\n\\n# Code for: class UpsertionRecord(Base):  # type: ignore[valid-type,misc]\\n\\n\\n# Code for: class SQLRecordManager(RecordManager):', metadata={'source': 'test_repo/libs/langchain/langchain/indexes/_sql_record_manager.py', 'content_type': 'simplified_code', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='\"\"\"**Index** is used to avoid writing duplicated content\\ninto the vectostore and to avoid over-writing content if it\\'s unchanged.\\n\\nIndexes also :\\n\\n* Create knowledge graphs from data.\\n\\n* Support indexing workflows from LangChain data loaders to vectorstores.\\n\\nImportantly, Index keeps on working even if the content being written is derived\\nvia a set of transformations from some source content (e.g., indexing children\\ndocuments that were derived from parent documents by chunking.)\\n\"\"\"\\nfrom langchain.indexes._api import IndexingResult, aindex, index\\nfrom langchain.indexes._sql_record_manager import SQLRecordManager\\nfrom langchain.indexes.graph import GraphIndexCreator\\nfrom langchain.indexes.vectorstore import VectorstoreIndexCreator\\n\\n__all__ = [\\n    # Keep sorted\\n    \"aindex\",\\n    \"GraphIndexCreator\",\\n    \"index\",\\n    \"IndexingResult\",\\n    \"SQLRecordManager\",\\n    \"VectorstoreIndexCreator\",\\n]', metadata={'source': 'test_repo/libs/langchain/langchain/indexes/__init__.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from typing import Any, Dict, List, Optional, Type\\n\\nfrom langchain_community.document_loaders.base import BaseLoader\\nfrom langchain_community.embeddings.openai import OpenAIEmbeddings\\nfrom langchain_community.llms.openai import OpenAI\\nfrom langchain_community.vectorstores.chroma import Chroma\\nfrom langchain_core.documents import Document\\nfrom langchain_core.embeddings import Embeddings\\nfrom langchain_core.language_models import BaseLanguageModel\\nfrom langchain_core.pydantic_v1 import BaseModel, Extra, Field\\nfrom langchain_core.vectorstores import VectorStore\\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter, TextSplitter\\n\\nfrom langchain.chains.qa_with_sources.retrieval import RetrievalQAWithSourcesChain\\nfrom langchain.chains.retrieval_qa.base import RetrievalQA\\n\\n\\ndef _get_default_text_splitter() -> TextSplitter:\\n    return RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=0)', metadata={'source': 'test_repo/libs/langchain/langchain/indexes/vectorstore.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='class VectorStoreIndexWrapper(BaseModel):\\n    \"\"\"Wrapper around a vectorstore for easy access.\"\"\"\\n\\n    vectorstore: VectorStore\\n\\n    class Config:\\n        \"\"\"Configuration for this pydantic object.\"\"\"\\n\\n        extra = Extra.forbid\\n        arbitrary_types_allowed = True\\n\\n    def query(\\n        self,\\n        question: str,\\n        llm: Optional[BaseLanguageModel] = None,\\n        retriever_kwargs: Optional[Dict[str, Any]] = None,\\n        **kwargs: Any,\\n    ) -> str:\\n        \"\"\"Query the vectorstore.\"\"\"\\n        llm = llm or OpenAI(temperature=0)\\n        retriever_kwargs = retriever_kwargs or {}\\n        chain = RetrievalQA.from_chain_type(\\n            llm, retriever=self.vectorstore.as_retriever(**retriever_kwargs), **kwargs\\n        )\\n        return chain.run(question)', metadata={'source': 'test_repo/libs/langchain/langchain/indexes/vectorstore.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='def query_with_sources(\\n        self,\\n        question: str,\\n        llm: Optional[BaseLanguageModel] = None,\\n        retriever_kwargs: Optional[Dict[str, Any]] = None,\\n        **kwargs: Any,\\n    ) -> dict:\\n        \"\"\"Query the vectorstore and get back sources.\"\"\"\\n        llm = llm or OpenAI(temperature=0)\\n        retriever_kwargs = retriever_kwargs or {}\\n        chain = RetrievalQAWithSourcesChain.from_chain_type(\\n            llm, retriever=self.vectorstore.as_retriever(**retriever_kwargs), **kwargs\\n        )\\n        return chain({chain.question_key: question})', metadata={'source': 'test_repo/libs/langchain/langchain/indexes/vectorstore.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='class VectorstoreIndexCreator(BaseModel):\\n    \"\"\"Logic for creating indexes.\"\"\"\\n\\n    vectorstore_cls: Type[VectorStore] = Chroma\\n    embedding: Embeddings = Field(default_factory=OpenAIEmbeddings)\\n    text_splitter: TextSplitter = Field(default_factory=_get_default_text_splitter)\\n    vectorstore_kwargs: dict = Field(default_factory=dict)\\n\\n    class Config:\\n        \"\"\"Configuration for this pydantic object.\"\"\"\\n\\n        extra = Extra.forbid\\n        arbitrary_types_allowed = True\\n\\n    def from_loaders(self, loaders: List[BaseLoader]) -> VectorStoreIndexWrapper:\\n        \"\"\"Create a vectorstore index from loaders.\"\"\"\\n        docs = []\\n        for loader in loaders:\\n            docs.extend(loader.load())\\n        return self.from_documents(docs)', metadata={'source': 'test_repo/libs/langchain/langchain/indexes/vectorstore.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='def from_documents(self, documents: List[Document]) -> VectorStoreIndexWrapper:\\n        \"\"\"Create a vectorstore index from documents.\"\"\"\\n        sub_docs = self.text_splitter.split_documents(documents)\\n        vectorstore = self.vectorstore_cls.from_documents(\\n            sub_docs, self.embedding, **self.vectorstore_kwargs\\n        )\\n        return VectorStoreIndexWrapper(vectorstore=vectorstore)', metadata={'source': 'test_repo/libs/langchain/langchain/indexes/vectorstore.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='def _hash_string_to_uuid(input_string: str) -> uuid.UUID:\\n    \"\"\"Hashes a string and returns the corresponding UUID.\"\"\"\\n    hash_value = hashlib.sha1(input_string.encode(\"utf-8\")).hexdigest()\\n    return uuid.uuid5(NAMESPACE_UUID, hash_value)', metadata={'source': 'test_repo/libs/langchain/langchain/indexes/_api.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='def _hash_nested_dict_to_uuid(data: dict[Any, Any]) -> uuid.UUID:\\n    \"\"\"Hashes a nested dictionary and returns the corresponding UUID.\"\"\"\\n    serialized_data = json.dumps(data, sort_keys=True)\\n    hash_value = hashlib.sha1(serialized_data.encode(\"utf-8\")).hexdigest()\\n    return uuid.uuid5(NAMESPACE_UUID, hash_value)', metadata={'source': 'test_repo/libs/langchain/langchain/indexes/_api.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='class _HashedDocument(Document):\\n    \"\"\"A hashed document with a unique ID.\"\"\"\\n\\n    uid: str\\n    hash_: str\\n    \"\"\"The hash of the document including content and metadata.\"\"\"\\n    content_hash: str\\n    \"\"\"The hash of the document content.\"\"\"\\n    metadata_hash: str\\n    \"\"\"The hash of the document metadata.\"\"\"\\n\\n    @classmethod\\n    def is_lc_serializable(cls) -> bool:\\n        return False\\n\\n    @root_validator(pre=True)\\n    def calculate_hashes(cls, values: Dict[str, Any]) -> Dict[str, Any]:\\n        \"\"\"Root validator to calculate content and metadata hash.\"\"\"\\n        content = values.get(\"page_content\", \"\")\\n        metadata = values.get(\"metadata\", {})\\n\\n        forbidden_keys = (\"hash_\", \"content_hash\", \"metadata_hash\")\\n\\n        for key in forbidden_keys:\\n            if key in metadata:\\n                raise ValueError(\\n                    f\"Metadata cannot contain key {key} as it \"\\n                    f\"is reserved for internal use.\"\\n                )', metadata={'source': 'test_repo/libs/langchain/langchain/indexes/_api.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='content_hash = str(_hash_string_to_uuid(content))\\n\\n        try:\\n            metadata_hash = str(_hash_nested_dict_to_uuid(metadata))\\n        except Exception as e:\\n            raise ValueError(\\n                f\"Failed to hash metadata: {e}. \"\\n                f\"Please use a dict that can be serialized using json.\"\\n            )\\n\\n        values[\"content_hash\"] = content_hash\\n        values[\"metadata_hash\"] = metadata_hash\\n        values[\"hash_\"] = str(_hash_string_to_uuid(content_hash + metadata_hash))\\n\\n        _uid = values.get(\"uid\", None)\\n\\n        if _uid is None:\\n            values[\"uid\"] = values[\"hash_\"]\\n        return values\\n\\n    def to_document(self) -> Document:\\n        \"\"\"Return a Document object.\"\"\"\\n        return Document(\\n            page_content=self.page_content,\\n            metadata=self.metadata,\\n        )', metadata={'source': 'test_repo/libs/langchain/langchain/indexes/_api.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='@classmethod\\n    def from_document(\\n        cls, document: Document, *, uid: Optional[str] = None\\n    ) -> _HashedDocument:\\n        \"\"\"Create a HashedDocument from a Document.\"\"\"\\n        return cls(\\n            uid=uid,\\n            page_content=document.page_content,\\n            metadata=document.metadata,\\n        )', metadata={'source': 'test_repo/libs/langchain/langchain/indexes/_api.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='def _batch(size: int, iterable: Iterable[T]) -> Iterator[List[T]]:\\n    \"\"\"Utility batching function.\"\"\"\\n    it = iter(iterable)\\n    while True:\\n        chunk = list(islice(it, size))\\n        if not chunk:\\n            return\\n        yield chunk', metadata={'source': 'test_repo/libs/langchain/langchain/indexes/_api.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='async def _abatch(size: int, iterable: AsyncIterable[T]) -> AsyncIterator[List[T]]:\\n    \"\"\"Utility batching function.\"\"\"\\n    batch: List[T] = []\\n    async for element in iterable:\\n        if len(batch) < size:\\n            batch.append(element)\\n\\n        if len(batch) >= size:\\n            yield batch\\n            batch = []\\n\\n    if batch:\\n        yield batch', metadata={'source': 'test_repo/libs/langchain/langchain/indexes/_api.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='def _get_source_id_assigner(\\n    source_id_key: Union[str, Callable[[Document], str], None],\\n) -> Callable[[Document], Union[str, None]]:\\n    \"\"\"Get the source id from the document.\"\"\"\\n    if source_id_key is None:\\n        return lambda doc: None\\n    elif isinstance(source_id_key, str):\\n        return lambda doc: doc.metadata[source_id_key]\\n    elif callable(source_id_key):\\n        return source_id_key\\n    else:\\n        raise ValueError(\\n            f\"source_id_key should be either None, a string or a callable. \"\\n            f\"Got {source_id_key} of type {type(source_id_key)}.\"\\n        )', metadata={'source': 'test_repo/libs/langchain/langchain/indexes/_api.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='def _deduplicate_in_order(\\n    hashed_documents: Iterable[_HashedDocument],\\n) -> Iterator[_HashedDocument]:\\n    \"\"\"Deduplicate a list of hashed documents while preserving order.\"\"\"\\n    seen: Set[str] = set()\\n\\n    for hashed_doc in hashed_documents:\\n        if hashed_doc.hash_ not in seen:\\n            seen.add(hashed_doc.hash_)\\n            yield hashed_doc', metadata={'source': 'test_repo/libs/langchain/langchain/indexes/_api.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='class IndexingResult(TypedDict):\\n    \"\"\"Return a detailed a breakdown of the result of the indexing operation.\"\"\"\\n\\n    num_added: int\\n    \"\"\"Number of added documents.\"\"\"\\n    num_updated: int\\n    \"\"\"Number of updated documents because they were not up to date.\"\"\"\\n    num_deleted: int\\n    \"\"\"Number of deleted documents.\"\"\"\\n    num_skipped: int\\n    \"\"\"Number of skipped documents because they were already up to date.\"\"\"', metadata={'source': 'test_repo/libs/langchain/langchain/indexes/_api.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='def index(\\n    docs_source: Union[BaseLoader, Iterable[Document]],\\n    record_manager: RecordManager,\\n    vector_store: VectorStore,\\n    *,\\n    batch_size: int = 100,\\n    cleanup: Literal[\"incremental\", \"full\", None] = None,\\n    source_id_key: Union[str, Callable[[Document], str], None] = None,\\n    cleanup_batch_size: int = 1_000,\\n    force_update: bool = False,\\n) -> IndexingResult:\\n    \"\"\"Index data from the loader into the vector store.\\n\\n    Indexing functionality uses a manager to keep track of which documents\\n    are in the vector store.\\n\\n    This allows us to keep track of which documents were updated, and which\\n    documents were deleted, which documents should be skipped.\\n\\n    For the time being, documents are indexed using their hashes, and users\\n     are not able to specify the uid of the document.', metadata={'source': 'test_repo/libs/langchain/langchain/indexes/_api.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='IMPORTANT:\\n       if auto_cleanup is set to True, the loader should be returning\\n       the entire dataset, and not just a subset of the dataset.\\n       Otherwise, the auto_cleanup will remove documents that it is not\\n       supposed to.', metadata={'source': 'test_repo/libs/langchain/langchain/indexes/_api.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content=\"Args:\\n        docs_source: Data loader or iterable of documents to index.\\n        record_manager: Timestamped set to keep track of which documents were\\n                         updated.\\n        vector_store: Vector store to index the documents into.\\n        batch_size: Batch size to use when indexing.\\n        cleanup: How to handle clean up of documents.\\n            - Incremental: Cleans up all documents that haven't been updated AND\\n                           that are associated with source ids that were seen\\n                           during indexing.\\n                           Clean up is done continuously during indexing helping\\n                           to minimize the probability of users seeing duplicated\\n                           content.\\n            - Full: Delete all documents that haven to been returned by the loader.\\n                    Clean up runs after all documents have been indexed.\", metadata={'source': 'test_repo/libs/langchain/langchain/indexes/_api.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='Clean up runs after all documents have been indexed.\\n                    This means that users may see duplicated content during indexing.\\n            - None: Do not delete any documents.\\n        source_id_key: Optional key that helps identify the original source\\n            of the document.\\n        cleanup_batch_size: Batch size to use when cleaning up documents.\\n        force_update: Force update documents even if they are present in the\\n            record manager. Useful if you are re-indexing with updated embeddings.', metadata={'source': 'test_repo/libs/langchain/langchain/indexes/_api.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='Returns:\\n        Indexing result which contains information about how many documents\\n        were added, updated, deleted, or skipped.\\n    \"\"\"\\n    if cleanup not in {\"incremental\", \"full\", None}:\\n        raise ValueError(\\n            f\"cleanup should be one of \\'incremental\\', \\'full\\' or None. \"\\n            f\"Got {cleanup}.\"\\n        )\\n\\n    if cleanup == \"incremental\" and source_id_key is None:\\n        raise ValueError(\"Source id key is required when cleanup mode is incremental.\")\\n\\n    # Check that the Vectorstore has required methods implemented\\n    methods = [\"delete\", \"add_documents\"]\\n\\n    for method in methods:\\n        if not hasattr(vector_store, method):\\n            raise ValueError(\\n                f\"Vectorstore {vector_store} does not have required method {method}\"\\n            )', metadata={'source': 'test_repo/libs/langchain/langchain/indexes/_api.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='if type(vector_store).delete == VectorStore.delete:\\n        # Checking if the vectorstore has overridden the default delete method\\n        # implementation which just raises a NotImplementedError\\n        raise ValueError(\"Vectorstore has not implemented the delete method\")\\n\\n    if isinstance(docs_source, BaseLoader):\\n        try:\\n            doc_iterator = docs_source.lazy_load()\\n        except NotImplementedError:\\n            doc_iterator = iter(docs_source.load())\\n    else:\\n        doc_iterator = iter(docs_source)\\n\\n    source_id_assigner = _get_source_id_assigner(source_id_key)\\n\\n    # Mark when the update started.\\n    index_start_dt = record_manager.get_time()\\n    num_added = 0\\n    num_skipped = 0\\n    num_updated = 0\\n    num_deleted = 0\\n\\n    for doc_batch in _batch(batch_size, doc_iterator):\\n        hashed_docs = list(\\n            _deduplicate_in_order(\\n                [_HashedDocument.from_document(doc) for doc in doc_batch]\\n            )\\n        )', metadata={'source': 'test_repo/libs/langchain/langchain/indexes/_api.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='source_ids: Sequence[Optional[str]] = [\\n            source_id_assigner(doc) for doc in hashed_docs\\n        ]\\n\\n        if cleanup == \"incremental\":\\n            # If the cleanup mode is incremental, source ids are required.\\n            for source_id, hashed_doc in zip(source_ids, hashed_docs):\\n                if source_id is None:\\n                    raise ValueError(\\n                        \"Source ids are required when cleanup mode is incremental. \"\\n                        f\"Document that starts with \"\\n                        f\"content: {hashed_doc.page_content[:100]} was not assigned \"\\n                        f\"as source id.\"\\n                    )\\n            # source ids cannot be None after for loop above.\\n            source_ids = cast(Sequence[str], source_ids)  # type: ignore[assignment]\\n\\n        exists_batch = record_manager.exists([doc.uid for doc in hashed_docs])', metadata={'source': 'test_repo/libs/langchain/langchain/indexes/_api.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='exists_batch = record_manager.exists([doc.uid for doc in hashed_docs])\\n\\n        # Filter out documents that already exist in the record store.\\n        uids = []\\n        docs_to_index = []\\n        uids_to_refresh = []\\n        seen_docs: Set[str] = set()\\n        for hashed_doc, doc_exists in zip(hashed_docs, exists_batch):\\n            if doc_exists:\\n                if force_update:\\n                    seen_docs.add(hashed_doc.uid)\\n                else:\\n                    uids_to_refresh.append(hashed_doc.uid)\\n                    continue\\n            uids.append(hashed_doc.uid)\\n            docs_to_index.append(hashed_doc.to_document())\\n\\n        # Update refresh timestamp\\n        if uids_to_refresh:\\n            record_manager.update(uids_to_refresh, time_at_least=index_start_dt)\\n            num_skipped += len(uids_to_refresh)', metadata={'source': 'test_repo/libs/langchain/langchain/indexes/_api.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='# Be pessimistic and assume that all vector store write will fail.\\n        # First write to vector store\\n        if docs_to_index:\\n            vector_store.add_documents(docs_to_index, ids=uids)\\n            num_added += len(docs_to_index) - len(seen_docs)\\n            num_updated += len(seen_docs)\\n\\n        # And only then update the record store.\\n        # Update ALL records, even if they already exist since we want to refresh\\n        # their timestamp.\\n        record_manager.update(\\n            [doc.uid for doc in hashed_docs],\\n            group_ids=source_ids,\\n            time_at_least=index_start_dt,\\n        )\\n\\n        # If source IDs are provided, we can do the deletion incrementally!\\n        if cleanup == \"incremental\":\\n            # Get the uids of the documents that were not returned by the loader.', metadata={'source': 'test_repo/libs/langchain/langchain/indexes/_api.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='# mypy isn\\'t good enough to determine that source ids cannot be None\\n            # here due to a check that\\'s happening above, so we check again.\\n            for source_id in source_ids:\\n                if source_id is None:\\n                    raise AssertionError(\"Source ids cannot be None here.\")\\n\\n            _source_ids = cast(Sequence[str], source_ids)\\n\\n            uids_to_delete = record_manager.list_keys(\\n                group_ids=_source_ids, before=index_start_dt\\n            )\\n            if uids_to_delete:\\n                # Then delete from vector store.\\n                vector_store.delete(uids_to_delete)\\n                # First delete from record store.\\n                record_manager.delete_keys(uids_to_delete)\\n                num_deleted += len(uids_to_delete)', metadata={'source': 'test_repo/libs/langchain/langchain/indexes/_api.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='if cleanup == \"full\":\\n        while uids_to_delete := record_manager.list_keys(\\n            before=index_start_dt, limit=cleanup_batch_size\\n        ):\\n            # First delete from record store.\\n            vector_store.delete(uids_to_delete)\\n            # Then delete from record manager.\\n            record_manager.delete_keys(uids_to_delete)\\n            num_deleted += len(uids_to_delete)\\n\\n    return {\\n        \"num_added\": num_added,\\n        \"num_updated\": num_updated,\\n        \"num_skipped\": num_skipped,\\n        \"num_deleted\": num_deleted,\\n    }', metadata={'source': 'test_repo/libs/langchain/langchain/indexes/_api.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='async def _to_async_iterator(iterator: Iterable[T]) -> AsyncIterator[T]:\\n    \"\"\"Convert an iterable to an async iterator.\"\"\"\\n    for item in iterator:\\n        yield item', metadata={'source': 'test_repo/libs/langchain/langchain/indexes/_api.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='async def aindex(\\n    docs_source: Union[BaseLoader, Iterable[Document], AsyncIterator[Document]],\\n    record_manager: RecordManager,\\n    vector_store: VectorStore,\\n    *,\\n    batch_size: int = 100,\\n    cleanup: Literal[\"incremental\", \"full\", None] = None,\\n    source_id_key: Union[str, Callable[[Document], str], None] = None,\\n    cleanup_batch_size: int = 1_000,\\n    force_update: bool = False,\\n) -> IndexingResult:\\n    \"\"\"Index data from the loader into the vector store.\\n\\n    Indexing functionality uses a manager to keep track of which documents\\n    are in the vector store.\\n\\n    This allows us to keep track of which documents were updated, and which\\n    documents were deleted, which documents should be skipped.\\n\\n    For the time being, documents are indexed using their hashes, and users\\n     are not able to specify the uid of the document.', metadata={'source': 'test_repo/libs/langchain/langchain/indexes/_api.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='IMPORTANT:\\n       if auto_cleanup is set to True, the loader should be returning\\n       the entire dataset, and not just a subset of the dataset.\\n       Otherwise, the auto_cleanup will remove documents that it is not\\n       supposed to.', metadata={'source': 'test_repo/libs/langchain/langchain/indexes/_api.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content=\"Args:\\n        docs_source: Data loader or iterable of documents to index.\\n        record_manager: Timestamped set to keep track of which documents were\\n                         updated.\\n        vector_store: Vector store to index the documents into.\\n        batch_size: Batch size to use when indexing.\\n        cleanup: How to handle clean up of documents.\\n            - Incremental: Cleans up all documents that haven't been updated AND\\n                           that are associated with source ids that were seen\\n                           during indexing.\\n                           Clean up is done continuously during indexing helping\\n                           to minimize the probability of users seeing duplicated\\n                           content.\\n            - Full: Delete all documents that haven to been returned by the loader.\\n                    Clean up runs after all documents have been indexed.\", metadata={'source': 'test_repo/libs/langchain/langchain/indexes/_api.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='Clean up runs after all documents have been indexed.\\n                    This means that users may see duplicated content during indexing.\\n            - None: Do not delete any documents.\\n        source_id_key: Optional key that helps identify the original source\\n            of the document.\\n        cleanup_batch_size: Batch size to use when cleaning up documents.\\n        force_update: Force update documents even if they are present in the\\n            record manager. Useful if you are re-indexing with updated embeddings.', metadata={'source': 'test_repo/libs/langchain/langchain/indexes/_api.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='Returns:\\n        Indexing result which contains information about how many documents\\n        were added, updated, deleted, or skipped.\\n    \"\"\"\\n\\n    if cleanup not in {\"incremental\", \"full\", None}:\\n        raise ValueError(\\n            f\"cleanup should be one of \\'incremental\\', \\'full\\' or None. \"\\n            f\"Got {cleanup}.\"\\n        )\\n\\n    if cleanup == \"incremental\" and source_id_key is None:\\n        raise ValueError(\"Source id key is required when cleanup mode is incremental.\")\\n\\n    # Check that the Vectorstore has required methods implemented\\n    methods = [\"adelete\", \"aadd_documents\"]\\n\\n    for method in methods:\\n        if not hasattr(vector_store, method):\\n            raise ValueError(\\n                f\"Vectorstore {vector_store} does not have required method {method}\"\\n            )', metadata={'source': 'test_repo/libs/langchain/langchain/indexes/_api.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='if type(vector_store).adelete == VectorStore.adelete:\\n        # Checking if the vectorstore has overridden the default delete method\\n        # implementation which just raises a NotImplementedError\\n        raise ValueError(\"Vectorstore has not implemented the delete method\")', metadata={'source': 'test_repo/libs/langchain/langchain/indexes/_api.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='async_doc_iterator: AsyncIterator[Document]\\n    if isinstance(docs_source, BaseLoader):\\n        try:\\n            async_doc_iterator = docs_source.alazy_load()\\n        except NotImplementedError:\\n            # Exception triggered when neither lazy_load nor alazy_load are implemented.\\n            # * The default implementation of alazy_load uses lazy_load.\\n            # * The default implementation of lazy_load raises NotImplementedError.\\n            # In such a case, we use the load method and convert it to an async\\n            # iterator.\\n            async_doc_iterator = _to_async_iterator(docs_source.load())\\n    else:\\n        if hasattr(docs_source, \"__aiter__\"):\\n            async_doc_iterator = docs_source  # type: ignore[assignment]\\n        else:\\n            async_doc_iterator = _to_async_iterator(docs_source)\\n\\n    source_id_assigner = _get_source_id_assigner(source_id_key)', metadata={'source': 'test_repo/libs/langchain/langchain/indexes/_api.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='source_id_assigner = _get_source_id_assigner(source_id_key)\\n\\n    # Mark when the update started.\\n    index_start_dt = await record_manager.aget_time()\\n    num_added = 0\\n    num_skipped = 0\\n    num_updated = 0\\n    num_deleted = 0\\n\\n    async for doc_batch in _abatch(batch_size, async_doc_iterator):\\n        hashed_docs = list(\\n            _deduplicate_in_order(\\n                [_HashedDocument.from_document(doc) for doc in doc_batch]\\n            )\\n        )\\n\\n        source_ids: Sequence[Optional[str]] = [\\n            source_id_assigner(doc) for doc in hashed_docs\\n        ]', metadata={'source': 'test_repo/libs/langchain/langchain/indexes/_api.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='if cleanup == \"incremental\":\\n            # If the cleanup mode is incremental, source ids are required.\\n            for source_id, hashed_doc in zip(source_ids, hashed_docs):\\n                if source_id is None:\\n                    raise ValueError(\\n                        \"Source ids are required when cleanup mode is incremental. \"\\n                        f\"Document that starts with \"\\n                        f\"content: {hashed_doc.page_content[:100]} was not assigned \"\\n                        f\"as source id.\"\\n                    )\\n            # source ids cannot be None after for loop above.\\n            source_ids = cast(Sequence[str], source_ids)\\n\\n        exists_batch = await record_manager.aexists([doc.uid for doc in hashed_docs])', metadata={'source': 'test_repo/libs/langchain/langchain/indexes/_api.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='exists_batch = await record_manager.aexists([doc.uid for doc in hashed_docs])\\n\\n        # Filter out documents that already exist in the record store.\\n        uids: list[str] = []\\n        docs_to_index: list[Document] = []\\n        uids_to_refresh = []\\n        seen_docs: Set[str] = set()\\n        for hashed_doc, doc_exists in zip(hashed_docs, exists_batch):\\n            if doc_exists:\\n                if force_update:\\n                    seen_docs.add(hashed_doc.uid)\\n                else:\\n                    uids_to_refresh.append(hashed_doc.uid)\\n                    continue\\n            uids.append(hashed_doc.uid)\\n            docs_to_index.append(hashed_doc.to_document())\\n\\n        if uids_to_refresh:\\n            # Must be updated to refresh timestamp.\\n            await record_manager.aupdate(uids_to_refresh, time_at_least=index_start_dt)\\n            num_skipped += len(uids_to_refresh)', metadata={'source': 'test_repo/libs/langchain/langchain/indexes/_api.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='# Be pessimistic and assume that all vector store write will fail.\\n        # First write to vector store\\n        if docs_to_index:\\n            await vector_store.aadd_documents(docs_to_index, ids=uids)\\n            num_added += len(docs_to_index) - len(seen_docs)\\n            num_updated += len(seen_docs)\\n\\n        # And only then update the record store.\\n        # Update ALL records, even if they already exist since we want to refresh\\n        # their timestamp.\\n        await record_manager.aupdate(\\n            [doc.uid for doc in hashed_docs],\\n            group_ids=source_ids,\\n            time_at_least=index_start_dt,\\n        )\\n\\n        # If source IDs are provided, we can do the deletion incrementally!\\n\\n        if cleanup == \"incremental\":\\n            # Get the uids of the documents that were not returned by the loader.', metadata={'source': 'test_repo/libs/langchain/langchain/indexes/_api.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='# mypy isn\\'t good enough to determine that source ids cannot be None\\n            # here due to a check that\\'s happening above, so we check again.\\n            for source_id in source_ids:\\n                if source_id is None:\\n                    raise AssertionError(\"Source ids cannot be None here.\")\\n\\n            _source_ids = cast(Sequence[str], source_ids)\\n\\n            uids_to_delete = await record_manager.alist_keys(\\n                group_ids=_source_ids, before=index_start_dt\\n            )\\n            if uids_to_delete:\\n                # Then delete from vector store.\\n                await vector_store.adelete(uids_to_delete)\\n                # First delete from record store.\\n                await record_manager.adelete_keys(uids_to_delete)\\n                num_deleted += len(uids_to_delete)', metadata={'source': 'test_repo/libs/langchain/langchain/indexes/_api.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='if cleanup == \"full\":\\n        while uids_to_delete := await record_manager.alist_keys(\\n            before=index_start_dt, limit=cleanup_batch_size\\n        ):\\n            # First delete from record store.\\n            await vector_store.adelete(uids_to_delete)\\n            # Then delete from record manager.\\n            await record_manager.adelete_keys(uids_to_delete)\\n            num_deleted += len(uids_to_delete)\\n\\n    return {\\n        \"num_added\": num_added,\\n        \"num_updated\": num_updated,\\n        \"num_skipped\": num_skipped,\\n        \"num_deleted\": num_deleted,\\n    }', metadata={'source': 'test_repo/libs/langchain/langchain/indexes/_api.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='\"\"\"Module contains logic for indexing documents into vector stores.\"\"\"\\nfrom __future__ import annotations\\n\\nimport hashlib\\nimport json\\nimport uuid\\nfrom itertools import islice\\nfrom typing import (\\n    Any,\\n    AsyncIterable,\\n    AsyncIterator,\\n    Callable,\\n    Dict,\\n    Iterable,\\n    Iterator,\\n    List,\\n    Literal,\\n    Optional,\\n    Sequence,\\n    Set,\\n    TypedDict,\\n    TypeVar,\\n    Union,\\n    cast,\\n)\\n\\nfrom langchain_community.document_loaders.base import BaseLoader\\nfrom langchain_core.documents import Document\\nfrom langchain_core.pydantic_v1 import root_validator\\nfrom langchain_core.vectorstores import VectorStore\\n\\nfrom langchain.indexes.base import NAMESPACE_UUID, RecordManager\\n\\nT = TypeVar(\"T\")\\n\\n\\n# Code for: def _hash_string_to_uuid(input_string: str) -> uuid.UUID:\\n\\n\\n# Code for: def _hash_nested_dict_to_uuid(data: dict[Any, Any]) -> uuid.UUID:\\n\\n\\n# Code for: class _HashedDocument(Document):\\n\\n\\n# Code for: def _batch(size: int, iterable: Iterable[T]) -> Iterator[List[T]]:', metadata={'source': 'test_repo/libs/langchain/langchain/indexes/_api.py', 'content_type': 'simplified_code', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='# Code for: def _batch(size: int, iterable: Iterable[T]) -> Iterator[List[T]]:\\n\\n\\n# Code for: async def _abatch(size: int, iterable: AsyncIterable[T]) -> AsyncIterator[List[T]]:\\n\\n\\n# Code for: def _get_source_id_assigner(\\n\\n\\n# Code for: def _deduplicate_in_order(\\n\\n\\n# PUBLIC API\\n\\n\\n# Code for: class IndexingResult(TypedDict):\\n\\n\\n# Code for: def index(\\n\\n\\n# Define an asynchronous generator function\\n# Code for: async def _to_async_iterator(iterator: Iterable[T]) -> AsyncIterator[T]:\\n\\n\\n# Code for: async def aindex(', metadata={'source': 'test_repo/libs/langchain/langchain/indexes/_api.py', 'content_type': 'simplified_code', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='from __future__ import annotations\\n\\nimport uuid\\nfrom abc import ABC, abstractmethod\\nfrom typing import List, Optional, Sequence\\n\\nNAMESPACE_UUID = uuid.UUID(int=1984)', metadata={'source': 'test_repo/libs/langchain/langchain/indexes/base.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='class RecordManager(ABC):\\n    \"\"\"An abstract base class representing the interface for a record manager.\"\"\"\\n\\n    def __init__(\\n        self,\\n        namespace: str,\\n    ) -> None:\\n        \"\"\"Initialize the record manager.\\n\\n        Args:\\n            namespace (str): The namespace for the record manager.\\n        \"\"\"\\n        self.namespace = namespace\\n\\n    @abstractmethod\\n    def create_schema(self) -> None:\\n        \"\"\"Create the database schema for the record manager.\"\"\"\\n\\n    @abstractmethod\\n    async def acreate_schema(self) -> None:\\n        \"\"\"Create the database schema for the record manager.\"\"\"\\n\\n    @abstractmethod\\n    def get_time(self) -> float:\\n        \"\"\"Get the current server time as a high resolution timestamp!\\n\\n        It\\'s important to get this from the server to ensure a monotonic clock,\\n        otherwise there may be data loss when cleaning up old documents!\\n\\n        Returns:\\n            The current server time as a float timestamp.\\n        \"\"\"', metadata={'source': 'test_repo/libs/langchain/langchain/indexes/base.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='Returns:\\n            The current server time as a float timestamp.\\n        \"\"\"\\n\\n    @abstractmethod\\n    async def aget_time(self) -> float:\\n        \"\"\"Get the current server time as a high resolution timestamp!\\n\\n        It\\'s important to get this from the server to ensure a monotonic clock,\\n        otherwise there may be data loss when cleaning up old documents!\\n\\n        Returns:\\n            The current server time as a float timestamp.\\n        \"\"\"\\n\\n    @abstractmethod\\n    def update(\\n        self,\\n        keys: Sequence[str],\\n        *,\\n        group_ids: Optional[Sequence[Optional[str]]] = None,\\n        time_at_least: Optional[float] = None,\\n    ) -> None:\\n        \"\"\"Upsert records into the database.\\n\\n        Args:\\n            keys: A list of record keys to upsert.\\n            group_ids: A list of group IDs corresponding to the keys.\\n            time_at_least: if provided, updates should only happen if the\\n              updated_at field is at least this time.', metadata={'source': 'test_repo/libs/langchain/langchain/indexes/base.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='Raises:\\n            ValueError: If the length of keys doesn\\'t match the length of group_ids.\\n        \"\"\"\\n\\n    @abstractmethod\\n    async def aupdate(\\n        self,\\n        keys: Sequence[str],\\n        *,\\n        group_ids: Optional[Sequence[Optional[str]]] = None,\\n        time_at_least: Optional[float] = None,\\n    ) -> None:\\n        \"\"\"Upsert records into the database.\\n\\n        Args:\\n            keys: A list of record keys to upsert.\\n            group_ids: A list of group IDs corresponding to the keys.\\n            time_at_least: if provided, updates should only happen if the\\n              updated_at field is at least this time.\\n\\n        Raises:\\n            ValueError: If the length of keys doesn\\'t match the length of group_ids.\\n        \"\"\"\\n\\n    @abstractmethod\\n    def exists(self, keys: Sequence[str]) -> List[bool]:\\n        \"\"\"Check if the provided keys exist in the database.\\n\\n        Args:\\n            keys: A list of keys to check.', metadata={'source': 'test_repo/libs/langchain/langchain/indexes/base.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='Args:\\n            keys: A list of keys to check.\\n\\n        Returns:\\n            A list of boolean values indicating the existence of each key.\\n        \"\"\"\\n\\n    @abstractmethod\\n    async def aexists(self, keys: Sequence[str]) -> List[bool]:\\n        \"\"\"Check if the provided keys exist in the database.\\n\\n        Args:\\n            keys: A list of keys to check.\\n\\n        Returns:\\n            A list of boolean values indicating the existence of each key.\\n        \"\"\"\\n\\n    @abstractmethod\\n    def list_keys(\\n        self,\\n        *,\\n        before: Optional[float] = None,\\n        after: Optional[float] = None,\\n        group_ids: Optional[Sequence[str]] = None,\\n        limit: Optional[int] = None,\\n    ) -> List[str]:\\n        \"\"\"List records in the database based on the provided filters.', metadata={'source': 'test_repo/libs/langchain/langchain/indexes/base.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='Args:\\n            before: Filter to list records updated before this time.\\n            after: Filter to list records updated after this time.\\n            group_ids: Filter to list records with specific group IDs.\\n            limit: optional limit on the number of records to return.\\n\\n        Returns:\\n            A list of keys for the matching records.\\n        \"\"\"\\n\\n    @abstractmethod\\n    async def alist_keys(\\n        self,\\n        *,\\n        before: Optional[float] = None,\\n        after: Optional[float] = None,\\n        group_ids: Optional[Sequence[str]] = None,\\n        limit: Optional[int] = None,\\n    ) -> List[str]:\\n        \"\"\"List records in the database based on the provided filters.\\n\\n        Args:\\n            before: Filter to list records updated before this time.\\n            after: Filter to list records updated after this time.\\n            group_ids: Filter to list records with specific group IDs.\\n            limit: optional limit on the number of records to return.', metadata={'source': 'test_repo/libs/langchain/langchain/indexes/base.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='Returns:\\n            A list of keys for the matching records.\\n        \"\"\"\\n\\n    @abstractmethod\\n    def delete_keys(self, keys: Sequence[str]) -> None:\\n        \"\"\"Delete specified records from the database.\\n\\n        Args:\\n            keys: A list of keys to delete.\\n        \"\"\"\\n\\n    @abstractmethod\\n    async def adelete_keys(self, keys: Sequence[str]) -> None:\\n        \"\"\"Delete specified records from the database.\\n\\n        Args:\\n            keys: A list of keys to delete.\\n        \"\"\"', metadata={'source': 'test_repo/libs/langchain/langchain/indexes/base.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='\"\"\"Graph Index Creator.\"\"\"\\nfrom typing import Optional, Type\\n\\nfrom langchain_community.graphs.networkx_graph import NetworkxEntityGraph, parse_triples\\nfrom langchain_core.language_models import BaseLanguageModel\\nfrom langchain_core.prompts import BasePromptTemplate\\nfrom langchain_core.pydantic_v1 import BaseModel\\n\\nfrom langchain.chains.llm import LLMChain\\nfrom langchain.indexes.prompts.knowledge_triplet_extraction import (\\n    KNOWLEDGE_TRIPLE_EXTRACTION_PROMPT,\\n)', metadata={'source': 'test_repo/libs/langchain/langchain/indexes/graph.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='class GraphIndexCreator(BaseModel):\\n    \"\"\"Functionality to create graph index.\"\"\"\\n\\n    llm: Optional[BaseLanguageModel] = None\\n    graph_type: Type[NetworkxEntityGraph] = NetworkxEntityGraph\\n\\n    def from_text(\\n        self, text: str, prompt: BasePromptTemplate = KNOWLEDGE_TRIPLE_EXTRACTION_PROMPT\\n    ) -> NetworkxEntityGraph:\\n        \"\"\"Create graph index from text.\"\"\"\\n        if self.llm is None:\\n            raise ValueError(\"llm should not be None\")\\n        graph = self.graph_type()\\n        chain = LLMChain(llm=self.llm, prompt=prompt)\\n        output = chain.predict(text=text)\\n        knowledge = parse_triples(output)\\n        for triple in knowledge:\\n            graph.add_triple(triple)\\n        return graph', metadata={'source': 'test_repo/libs/langchain/langchain/indexes/graph.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='async def afrom_text(\\n        self, text: str, prompt: BasePromptTemplate = KNOWLEDGE_TRIPLE_EXTRACTION_PROMPT\\n    ) -> NetworkxEntityGraph:\\n        \"\"\"Create graph index from text asynchronously.\"\"\"\\n        if self.llm is None:\\n            raise ValueError(\"llm should not be None\")\\n        graph = self.graph_type()\\n        chain = LLMChain(llm=self.llm, prompt=prompt)\\n        output = await chain.apredict(text=text)\\n        knowledge = parse_triples(output)\\n        for triple in knowledge:\\n            graph.add_triple(triple)\\n        return graph', metadata={'source': 'test_repo/libs/langchain/langchain/indexes/graph.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='# flake8: noqa\\nfrom langchain_core.prompts.prompt import PromptTemplate\\n\\n_DEFAULT_ENTITY_SUMMARIZATION_TEMPLATE = \"\"\"You are an AI assistant helping a human keep track of facts about relevant people, places, and concepts in their life. Update the summary of the provided entity in the \"Entity\" section based on the last line of your conversation with the human. If you are writing the summary for the first time, return a single sentence.\\nThe update should only include facts that are relayed in the last line of conversation about the provided entity, and should only contain facts about the provided entity.\\n\\nIf there is no new information about the provided entity or the information is not worth noting (not an important or relevant fact to remember long-term), return the existing summary unchanged.\\n\\nFull conversation history (for context):\\n{history}\\n\\nEntity to summarize:\\n{entity}\\n\\nExisting summary of {entity}:\\n{summary}\\n\\nLast line of conversation:\\nHuman: {input}\\nUpdated summary:\"\"\"', metadata={'source': 'test_repo/libs/langchain/langchain/indexes/prompts/entity_summarization.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='Last line of conversation:\\nHuman: {input}\\nUpdated summary:\"\"\"\\n\\nENTITY_SUMMARIZATION_PROMPT = PromptTemplate(\\n    input_variables=[\"entity\", \"summary\", \"history\", \"input\"],\\n    template=_DEFAULT_ENTITY_SUMMARIZATION_TEMPLATE,\\n)', metadata={'source': 'test_repo/libs/langchain/langchain/indexes/prompts/entity_summarization.py', 'language': <Language.PYTHON: 'python'>}),\n"," Document(page_content='\"\"\"Relevant prompts for constructing indexes.\"\"\"', metadata={'source': 'test_repo/libs/langchain/langchain/indexes/prompts/__init__.py', 'language': <Language.PYTHON: 'python'>}),\n"," ...]"]},"metadata":{},"execution_count":21}],"source":["texts[:1500]"]},{"cell_type":"markdown","metadata":{"id":"0m1gpiir_PVD"},"source":["**Exercice**\n","\n","Créer des embeddings de votre code source splitté et stocké les dans une VectorDB de votre choix"]},{"cell_type":"code","source":["from langchain.embeddings.openai import OpenAIEmbeddings\n","from langchain.vectorstores import Chroma"],"metadata":{"id":"yXe3huFwCXx-","executionInfo":{"status":"ok","timestamp":1710253277870,"user_tz":-60,"elapsed":376,"user":{"displayName":"Tom TEA","userId":"00599091751588244729"}}},"execution_count":18,"outputs":[]},{"cell_type":"code","execution_count":22,"metadata":{"id":"Sw4IKQ52_PVD","executionInfo":{"status":"ok","timestamp":1710253479603,"user_tz":-60,"elapsed":120148,"user":{"displayName":"Tom TEA","userId":"00599091751588244729"}}},"outputs":[],"source":["embeddings = OpenAIEmbeddings(disallowed_special=())\n","\n","db =  Chroma.from_documents(texts, embeddings)\n","retriever = db.as_retriever(\n","    search_type=\"mmr\",  # Also test \"similarity\"\n","    search_kwargs={\"k\": 8},\n",")"]},{"cell_type":"markdown","metadata":{"id":"XBoJtBwV_PVD"},"source":["### Chat"]},{"cell_type":"markdown","metadata":{"id":"VZ-oO5ji_PVD"},"source":["**Exercice**\n","\n","Créer un model de chat avec Mémoire et Une chain de RAG à partir du retriever appelant la VectorDB\n","tester votre code en demandant des questions au LLM sur Lngchain"]},{"cell_type":"code","execution_count":25,"metadata":{"id":"9YlXRD9i_PVD","executionInfo":{"status":"ok","timestamp":1710253583549,"user_tz":-60,"elapsed":317,"user":{"displayName":"Tom TEA","userId":"00599091751588244729"}}},"outputs":[],"source":["from langchain.chains import ConversationalRetrievalChain\n","from langchain.memory import ConversationSummaryMemory\n","from langchain_openai import ChatOpenAI\n","\n","llm = ChatOpenAI(temperature=0)\n","qr_model = ConversationalRetrievalChain.from_llm(llm, retriever=retriever)"]},{"cell_type":"code","source":["chat_history = []\n"],"metadata":{"id":"7W_5eVp0DubC","executionInfo":{"status":"ok","timestamp":1710253630083,"user_tz":-60,"elapsed":4,"user":{"displayName":"Tom TEA","userId":"00599091751588244729"}}},"execution_count":27,"outputs":[]},{"cell_type":"code","execution_count":31,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":53},"id":"5sElugrW_PVD","executionInfo":{"status":"ok","timestamp":1710254570394,"user_tz":-60,"elapsed":23040,"user":{"displayName":"Tom TEA","userId":"00599091751588244729"}},"outputId":"e219882a-d412-46b6-af30-45e7422be28e"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["\"LangSmith est une composante de LangChain qui fournit des utilitaires pour se connecter à LangSmith, permettant ainsi d'évaluer les chaînes et d'autres composants d'application de modèle de langage à l'aide d'évaluateurs LangChain.\""],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":31}],"source":["question = \"Expliques moi à quoi sert LangSmith une composante de LangChain?\"\n","result = qr_model({\"question\": question, \"chat_history\": chat_history})\n","chat_history.append((question, result[\"answer\"]))\n","result[\"answer\"]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZoXfJXzZ_PVE"},"outputs":[],"source":["#qr_model.memory.clear()"]},{"cell_type":"markdown","metadata":{"id":"0ZPHDVDb_PVE"},"source":[]}],"metadata":{"kernelspec":{"display_name":"base","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.10"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}