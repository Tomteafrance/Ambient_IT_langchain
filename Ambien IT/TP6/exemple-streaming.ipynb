{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"_-o9qC2zot1i"},"outputs":[],"source":["#!pip install -U langchain-openai langchain langchainhub tavily-python\n","OPENAI_API_KEY= \"\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WYPm39Wvot1i"},"outputs":[],"source":["import os\n","os.environ['OPENAI_API_KEY']=OPENAI_API_KEY"]},{"cell_type":"markdown","metadata":{"id":"R0FYGAK3ot1j"},"source":["# Streaming"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8T8WdT3_ot1j","outputId":"4235fcef-26ce-4611-93c2-b3505a548ed6"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","\n","Verse 1:\n","Dans un verger ensoleillé\n","Poussent des fruits colorés\n","Des oranges juteuses et sucrées\n","Qui nous font saliver\n","\n","Refrain:\n","Oh les oranges, si belles et si rondes\n","On en raffole, elles sont si bonnes\n","On les croque, on les presse en jus\n","Les oranges, c'est un vrai délice pour nous\n","\n","Verse 2:\n","Leur peau est si douce et lisse\n","Et leur parfum nous envoûte\n","On les cueille avec délice\n","Et on en fait des compotes\n","\n","Refrain:\n","Oh les oranges, si belles et si rondes\n","On en raffole, elles sont si bonnes\n","On les croque, on les presse en jus\n","Les oranges, c'est un vrai délice pour nous\n","\n","Pont:\n","Et quand vient l'hiver\n","Les oranges sont là pour nous réchauffer\n","Dans un bon vin chaud\n","Ou en marmelade sur du pain chaud\n","\n","Refrain:\n","Oh les oranges, si belles et si rondes\n","On en raffole, elles sont si bonnes\n","On les croque,"]}],"source":["from langchain_openai import OpenAI\n","\n","llm = OpenAI(model=\"gpt-3.5-turbo-instruct\", temperature=0,max_tokens=256)\n","async for chunk in llm.astream(\"Écris-moi une chansons sur les oranges.\"):\n","    print(chunk, end=\"\", flush=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vJVTB-kQot1j"},"outputs":[],"source":["from langchain_openai import OpenAI\n","\n","llm = OpenAI(model=\"gpt-3.5-turbo-instruct\", temperature=0,max_tokens=256,streaming=True)\n","for chunk in llm.stream(\"Écris-moi une chansons sur les oranges.\"):\n","    print(chunk, end=\"\", flush=True)"]},{"cell_type":"markdown","metadata":{"id":"a0yt5v0Mot1k"},"source":["## Streaming d'agent"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fIlSf4Loot1k"},"outputs":[],"source":["from langchain import hub\n","from langchain.agents import AgentExecutor, create_openai_functions_agent\n","# Get the prompt to use - you can modify this!\n","prompt = hub.pull(\"hwchase17/openai-tools-agent\")\n","prompt.messages[0].prompt.template=\"Tu es un assistant IA serviable.\"\n","prompt.messages"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EAHDyCM-ot1k"},"outputs":[],"source":["import random\n","from langchain.tools import tool\n","\n","#define tools\n","@tool\n","async def where_cat_is_hiding() -> str:\n","    \"\"\"Où le chat se cache-t-il?\"\"\"\n","    return random.choice([\"sous le lit\", \"sur l'étagère\"])\n","\n","\n","@tool\n","async def get_items(place: str) -> str:\n","    \"\"\"Utilisez cet outil pour rechercher les éléments qui se trouvent à l'endroit donné.\"\"\"\n","    if \"bed\" in place:  # For under the bed\n","        return \"chaussettes, chaussures et poussière\"\n","    if \"shelf\" in place:  # For 'shelf'\n","        return \"livres, crayons et photos\"\n","    else:  # if the agent decides to ask about a different place\n","        return \"nourriture pour chat\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NRo4MNK3ot1k"},"outputs":[],"source":["from langchain_openai import ChatOpenAI\n","# Choose the LLM that will drive the agent\n","chat_model = ChatOpenAI(temperature=0.9,streaming=True)\n","tools = [get_items, where_cat_is_hiding]\n","# Construct the OpenAI Functions agent\n","agent = create_openai_functions_agent(chat_model.with_config({\"tags\": [\"agent_llm\"]}), tools, prompt)\n","agent_executor = AgentExecutor(agent=agent, tools=tools).with_config(\n","    {\"run_name\": \"Agent\"}\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7HzQ4ZcNot1k","outputId":"5a9a15cc-0572-4dec-def4-478fca007a42"},"outputs":[{"name":"stdout","output_type":"stream","text":["------\n","{'actions': [...], 'messages': [...]}\n","------\n","{'messages': [...], 'steps': [...]}\n","------\n","{'actions': [...], 'messages': [...]}\n","------\n","{'messages': [...], 'steps': [...]}\n","------\n","{'messages': [...],\n"," 'output': \"Les objets qui se trouvent à l'endroit où le chat se cache sont de \"\n","           'la nourriture pour chat.'}\n"]}],"source":["# Note: We use `pprint` to print only to depth 1, it makes it easier to see the output from a high level, before digging in.\n","import pprint\n","\n","chunks = []\n","\n","async for chunk in agent_executor.astream(\n","    {\"input\": \"Quels sont les objets qui se trouvent à l'endroit où le chat se cache?\"}\n","):\n","    chunks.append(chunk)\n","    print(\"------\")\n","    pprint.pprint(chunk, depth=1)"]},{"cell_type":"markdown","metadata":{"id":"qbs-W2MWot1k"},"source":["## Streming avec du RAG"]},{"cell_type":"markdown","metadata":{"id":"zw4Q114Hot1k"},"source":["Voici l'application Q&A avec les sources que nous avons construites à partir de l'article de Lilian Weng sur le blog [LLM Powered Autonomous Agents](https://lilianweng.github.io/posts/2023-06-23-agent/) dans le guide Returning sources :"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_4FcxTotot1l"},"outputs":[],"source":["!pip install chromadb"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JpQTFzjRot1l"},"outputs":[],"source":["import bs4\n","from langchain import hub\n","from langchain_community.document_loaders import WebBaseLoader\n","from langchain_community.vectorstores import Chroma\n","from langchain_core.output_parsers import StrOutputParser\n","from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n","from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n","from langchain_text_splitters import RecursiveCharacterTextSplitter"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3j6T8F2oot1l"},"outputs":[],"source":["# Load, chunk and index the contents of the blog.\n","bs_strainer = bs4.SoupStrainer(class_=(\"post-content\", \"post-title\", \"post-header\"))\n","loader = WebBaseLoader(\n","    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n","    bs_kwargs={\"parse_only\": bs_strainer},\n",")\n","docs = loader.load()\n","\n","text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n","splits = text_splitter.split_documents(docs)\n","vectorstore = Chroma.from_documents(documents=splits, embedding=OpenAIEmbeddings())\n","\n","# Retrieve and generate using the relevant snippets of the blog.\n","retriever = vectorstore.as_retriever()\n","prompt = hub.pull(\"rlm/rag-prompt\")\n","llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n","\n","\n","def format_docs(docs):\n","    return \"\\n\\n\".join(doc.page_content for doc in docs)\n","\n","\n","rag_chain_from_docs = (\n","    RunnablePassthrough.assign(context=(lambda x: format_docs(x[\"context\"])))\n","    | prompt\n","    | llm\n","    | StrOutputParser()\n",")\n","\n","rag_chain_with_source = RunnableParallel(\n","    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",").assign(answer=rag_chain_from_docs)"]},{"cell_type":"markdown","metadata":{"id":"NGU46_e9ot1l"},"source":["### Stream la sortie finale"]},{"cell_type":"markdown","metadata":{"id":"Jn2OzqDQot1l"},"source":["avec LCEL c'est plus facile il suffit juste de boucler dessus"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Hej-_J7qot1l","outputId":"67703a38-a870-4312-a7e6-88dc1b6915e6"},"outputs":[{"name":"stdout","output_type":"stream","text":["{'question': \"Qu'est-ce que la 'Self-Reflexion?\"}\n","{'context': [Document(page_content='Fig. 3. Illustration of the Reflexion framework. (Image source: Shinn & Labash, 2023)\\nThe heuristic function determines when the trajectory is inefficient or contains hallucination and should be stopped. Inefficient planning refers to trajectories that take too long without success. Hallucination is defined as encountering a sequence of consecutive identical actions that lead to the same observation in the environment.\\nSelf-reflection is created by showing two-shot examples to LLM and each example is a pair of (failed trajectory, ideal reflection for guiding future changes in the plan). Then reflections are added into the agent’s working memory, up to three, to be used as context for querying LLM.', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}), Document(page_content='Another quite distinct approach, LLM+P (Liu et al. 2023), involves relying on an external classical planner to do long-horizon planning. This approach utilizes the Planning Domain Definition Language (PDDL) as an intermediate interface to describe the planning problem. In this process, LLM (1) translates the problem into “Problem PDDL”, then (2) requests a classical planner to generate a PDDL plan based on an existing “Domain PDDL”, and finally (3) translates the PDDL plan back into natural language. Essentially, the planning step is outsourced to an external tool, assuming the availability of domain-specific PDDL and a suitable planner which is common in certain robotic setups but not in many other domains.\\nSelf-Reflection#\\nSelf-reflection is a vital aspect that allows autonomous agents to improve iteratively by refining past action decisions and correcting previous mistakes. It plays a crucial role in real-world tasks where trial and error are inevitable.', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}), Document(page_content='Fig. 2.  Examples of reasoning trajectories for knowledge-intensive tasks (e.g. HotpotQA, FEVER) and decision-making tasks (e.g. AlfWorld Env, WebShop). (Image source: Yao et al. 2023).\\nIn both experiments on knowledge-intensive tasks and decision-making tasks, ReAct works better than the Act-only baseline where Thought: … step is removed.\\nReflexion (Shinn & Labash 2023) is a framework to equips agents with dynamic memory and self-reflection capabilities to improve reasoning skills. Reflexion has a standard RL setup, in which the reward model provides a simple binary reward and the action space follows the setup in ReAct where the task-specific action space is augmented with language to enable complex reasoning steps. After each action $a_t$, the agent computes a heuristic $h_t$ and optionally may decide to reset the environment to start a new trial depending on the self-reflection results.', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}), Document(page_content='Memory stream: is a long-term memory module (external database) that records a comprehensive list of agents’ experience in natural language.\\n\\nEach element is an observation, an event directly provided by the agent.\\n- Inter-agent communication can trigger new natural language statements.\\n\\n\\nRetrieval model: surfaces the context to inform the agent’s behavior, according to relevance, recency and importance.\\n\\nRecency: recent events have higher scores\\nImportance: distinguish mundane from core memories. Ask LM directly.\\nRelevance: based on how related it is to the current situation / query.\\n\\n\\nReflection mechanism: synthesizes memories into higher level inferences over time and guides the agent’s future behavior. They are higher-level summaries of past events (<- note that this is a bit different from self-reflection above)', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'})]}\n","{'answer': ''}\n","{'answer': 'La'}\n","{'answer': \" '\"}\n","{'answer': 'Self'}\n","{'answer': '-'}\n","{'answer': 'Ref'}\n","{'answer': 'lex'}\n","{'answer': 'ion'}\n","{'answer': \"'\"}\n","{'answer': ' est'}\n","{'answer': ' un'}\n","{'answer': ' aspect'}\n","{'answer': ' vital'}\n","{'answer': ' qui'}\n","{'answer': ' permet'}\n","{'answer': ' aux'}\n","{'answer': ' agents'}\n","{'answer': ' autonom'}\n","{'answer': 'es'}\n","{'answer': ' d'}\n","{'answer': \"'am\"}\n","{'answer': 'éli'}\n","{'answer': 'orer'}\n","{'answer': ' de'}\n","{'answer': ' manière'}\n","{'answer': ' it'}\n","{'answer': 'ér'}\n","{'answer': 'ative'}\n","{'answer': ' en'}\n","{'answer': ' aff'}\n","{'answer': 'inant'}\n","{'answer': ' les'}\n","{'answer': ' déc'}\n","{'answer': 'isions'}\n","{'answer': ' d'}\n","{'answer': \"'action\"}\n","{'answer': ' pass'}\n","{'answer': 'ées'}\n","{'answer': ' et'}\n","{'answer': ' en'}\n","{'answer': ' corr'}\n","{'answer': 'ige'}\n","{'answer': 'ant'}\n","{'answer': ' les'}\n","{'answer': ' erre'}\n","{'answer': 'urs'}\n","{'answer': ' préc'}\n","{'answer': 'éd'}\n","{'answer': 'entes'}\n","{'answer': '.'}\n","{'answer': ' Elle'}\n","{'answer': ' jou'}\n","{'answer': 'e'}\n","{'answer': ' un'}\n","{'answer': ' r'}\n","{'answer': 'ôle'}\n","{'answer': ' crucial'}\n","{'answer': ' dans'}\n","{'answer': ' les'}\n","{'answer': ' t'}\n","{'answer': 'â'}\n","{'answer': 'ches'}\n","{'answer': ' du'}\n","{'answer': ' monde'}\n","{'answer': ' ré'}\n","{'answer': 'el'}\n","{'answer': ' où'}\n","{'answer': ' l'}\n","{'answer': \"'\"}\n","{'answer': 'ess'}\n","{'answer': 'ai'}\n","{'answer': ' et'}\n","{'answer': ' l'}\n","{'answer': \"'\"}\n","{'answer': 'erreur'}\n","{'answer': ' sont'}\n","{'answer': ' in'}\n","{'answer': 'é'}\n","{'answer': 'vit'}\n","{'answer': 'ables'}\n","{'answer': '.'}\n","{'answer': ' Le'}\n","{'answer': ' cadre'}\n","{'answer': ' Reflex'}\n","{'answer': 'ion'}\n","{'answer': ' é'}\n","{'answer': 'quipe'}\n","{'answer': ' les'}\n","{'answer': ' agents'}\n","{'answer': ' de'}\n","{'answer': ' capac'}\n","{'answer': 'ités'}\n","{'answer': ' de'}\n","{'answer': ' mé'}\n","{'answer': 'moire'}\n","{'answer': ' dynam'}\n","{'answer': 'ique'}\n","{'answer': ' et'}\n","{'answer': ' de'}\n","{'answer': ' self'}\n","{'answer': '-ref'}\n","{'answer': 'lex'}\n","{'answer': 'ion'}\n","{'answer': ' pour'}\n","{'answer': ' am'}\n","{'answer': 'éli'}\n","{'answer': 'orer'}\n","{'answer': ' leurs'}\n","{'answer': ' compét'}\n","{'answer': 'ences'}\n","{'answer': ' en'}\n","{'answer': ' mat'}\n","{'answer': 'ière'}\n","{'answer': ' de'}\n","{'answer': ' rais'}\n","{'answer': 'onnement'}\n","{'answer': '.'}\n","{'answer': ''}\n"]}],"source":["for chunk in rag_chain_with_source.stream(\"Qu'est-ce que la 'Self-Reflexion?\"):\n","    print(chunk)"]},{"cell_type":"markdown","metadata":{"id":"pGN0tG4yot1l"},"source":["On peut print la sortie par token"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nfeFHacjot1l","outputId":"989df146-bd5a-4ef1-bda5-690a2a403efe"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","\n","question: Qu'est-ce que la 'Self-Reflexion?\n","\n","context: [Document(page_content='Fig. 3. Illustration of the Reflexion framework. (Image source: Shinn & Labash, 2023)\\nThe heuristic function determines when the trajectory is inefficient or contains hallucination and should be stopped. Inefficient planning refers to trajectories that take too long without success. Hallucination is defined as encountering a sequence of consecutive identical actions that lead to the same observation in the environment.\\nSelf-reflection is created by showing two-shot examples to LLM and each example is a pair of (failed trajectory, ideal reflection for guiding future changes in the plan). Then reflections are added into the agent’s working memory, up to three, to be used as context for querying LLM.', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}), Document(page_content='Another quite distinct approach, LLM+P (Liu et al. 2023), involves relying on an external classical planner to do long-horizon planning. This approach utilizes the Planning Domain Definition Language (PDDL) as an intermediate interface to describe the planning problem. In this process, LLM (1) translates the problem into “Problem PDDL”, then (2) requests a classical planner to generate a PDDL plan based on an existing “Domain PDDL”, and finally (3) translates the PDDL plan back into natural language. Essentially, the planning step is outsourced to an external tool, assuming the availability of domain-specific PDDL and a suitable planner which is common in certain robotic setups but not in many other domains.\\nSelf-Reflection#\\nSelf-reflection is a vital aspect that allows autonomous agents to improve iteratively by refining past action decisions and correcting previous mistakes. It plays a crucial role in real-world tasks where trial and error are inevitable.', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}), Document(page_content='Fig. 2.  Examples of reasoning trajectories for knowledge-intensive tasks (e.g. HotpotQA, FEVER) and decision-making tasks (e.g. AlfWorld Env, WebShop). (Image source: Yao et al. 2023).\\nIn both experiments on knowledge-intensive tasks and decision-making tasks, ReAct works better than the Act-only baseline where Thought: … step is removed.\\nReflexion (Shinn & Labash 2023) is a framework to equips agents with dynamic memory and self-reflection capabilities to improve reasoning skills. Reflexion has a standard RL setup, in which the reward model provides a simple binary reward and the action space follows the setup in ReAct where the task-specific action space is augmented with language to enable complex reasoning steps. After each action $a_t$, the agent computes a heuristic $h_t$ and optionally may decide to reset the environment to start a new trial depending on the self-reflection results.', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}), Document(page_content='Memory stream: is a long-term memory module (external database) that records a comprehensive list of agents’ experience in natural language.\\n\\nEach element is an observation, an event directly provided by the agent.\\n- Inter-agent communication can trigger new natural language statements.\\n\\n\\nRetrieval model: surfaces the context to inform the agent’s behavior, according to relevance, recency and importance.\\n\\nRecency: recent events have higher scores\\nImportance: distinguish mundane from core memories. Ask LM directly.\\nRelevance: based on how related it is to the current situation / query.\\n\\n\\nReflection mechanism: synthesizes memories into higher level inferences over time and guides the agent’s future behavior. They are higher-level summaries of past events (<- note that this is a bit different from self-reflection above)', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'})]\n","\n","answer: La 'Self-Reflexion' est un aspect vital qui permet aux agents autonomes d'améliorer de manière itérative en affinant les décisions d'action passées et en corrigeant les erreurs précédentes. Elle joue un rôle crucial dans les tâches du monde réel où l'essai et l'erreur sont inévitables."]},{"data":{"text/plain":["{'question': \"Qu'est-ce que la 'Self-Reflexion?\",\n"," 'context': [Document(page_content='Fig. 3. Illustration of the Reflexion framework. (Image source: Shinn & Labash, 2023)\\nThe heuristic function determines when the trajectory is inefficient or contains hallucination and should be stopped. Inefficient planning refers to trajectories that take too long without success. Hallucination is defined as encountering a sequence of consecutive identical actions that lead to the same observation in the environment.\\nSelf-reflection is created by showing two-shot examples to LLM and each example is a pair of (failed trajectory, ideal reflection for guiding future changes in the plan). Then reflections are added into the agent’s working memory, up to three, to be used as context for querying LLM.', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}),\n","  Document(page_content='Another quite distinct approach, LLM+P (Liu et al. 2023), involves relying on an external classical planner to do long-horizon planning. This approach utilizes the Planning Domain Definition Language (PDDL) as an intermediate interface to describe the planning problem. In this process, LLM (1) translates the problem into “Problem PDDL”, then (2) requests a classical planner to generate a PDDL plan based on an existing “Domain PDDL”, and finally (3) translates the PDDL plan back into natural language. Essentially, the planning step is outsourced to an external tool, assuming the availability of domain-specific PDDL and a suitable planner which is common in certain robotic setups but not in many other domains.\\nSelf-Reflection#\\nSelf-reflection is a vital aspect that allows autonomous agents to improve iteratively by refining past action decisions and correcting previous mistakes. It plays a crucial role in real-world tasks where trial and error are inevitable.', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}),\n","  Document(page_content='Fig. 2.  Examples of reasoning trajectories for knowledge-intensive tasks (e.g. HotpotQA, FEVER) and decision-making tasks (e.g. AlfWorld Env, WebShop). (Image source: Yao et al. 2023).\\nIn both experiments on knowledge-intensive tasks and decision-making tasks, ReAct works better than the Act-only baseline where Thought: … step is removed.\\nReflexion (Shinn & Labash 2023) is a framework to equips agents with dynamic memory and self-reflection capabilities to improve reasoning skills. Reflexion has a standard RL setup, in which the reward model provides a simple binary reward and the action space follows the setup in ReAct where the task-specific action space is augmented with language to enable complex reasoning steps. After each action $a_t$, the agent computes a heuristic $h_t$ and optionally may decide to reset the environment to start a new trial depending on the self-reflection results.', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}),\n","  Document(page_content='Memory stream: is a long-term memory module (external database) that records a comprehensive list of agents’ experience in natural language.\\n\\nEach element is an observation, an event directly provided by the agent.\\n- Inter-agent communication can trigger new natural language statements.\\n\\n\\nRetrieval model: surfaces the context to inform the agent’s behavior, according to relevance, recency and importance.\\n\\nRecency: recent events have higher scores\\nImportance: distinguish mundane from core memories. Ask LM directly.\\nRelevance: based on how related it is to the current situation / query.\\n\\n\\nReflection mechanism: synthesizes memories into higher level inferences over time and guides the agent’s future behavior. They are higher-level summaries of past events (<- note that this is a bit different from self-reflection above)', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'})],\n"," 'answer': \"La 'Self-Reflexion' est un aspect vital qui permet aux agents autonomes d'améliorer de manière itérative en affinant les décisions d'action passées et en corrigeant les erreurs précédentes. Elle joue un rôle crucial dans les tâches du monde réel où l'essai et l'erreur sont inévitables.\"}"]},"execution_count":60,"metadata":{},"output_type":"execute_result"}],"source":["output = {}\n","curr_key = None\n","for chunk in rag_chain_with_source.stream(\"Qu'est-ce que la 'Self-Reflexion?\"):\n","    for key in chunk:\n","        if key not in output:\n","            output[key] = chunk[key]\n","        else:\n","            output[key] += chunk[key]\n","        if key != curr_key:\n","            print(f\"\\n\\n{key}: {chunk[key]}\", end=\"\", flush=True)\n","        else:\n","            print(chunk[key], end=\"\", flush=True)\n","        curr_key = key\n","output"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gt4mLOHgot1l"},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"base","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.10"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}